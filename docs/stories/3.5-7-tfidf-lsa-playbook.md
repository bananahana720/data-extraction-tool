# Story 3.5.7: TF-IDF/LSA Implementation Playbook

Status: todo - Ready for implementation (Epic 3.5 - Bridge Epic)

## Story

As a **developer implementing Epic 4 semantic analysis with TF-IDF and LSA**,
I want **a comprehensive playbook with code examples, best practices, and gotchas**,
so that **I can implement semantic features consistently without trial-and-error learning**.

## Context Summary

**Epic Context:** Story 3.5.7 is the final story in Epic 3.5 (Bridge Epic - Tooling & Semantic Prep), which prepares the project for Epic 4 (Foundational Semantic Analysis). This story addresses a preparation task from the Epic 3 retrospective: "TF-IDF/LSA playbook (Owner: Charlie + Elena, Est: 4h)."

**Business Value:**
- Accelerates Epic 4 implementation (developers have working examples to adapt)
- Prevents common TF-IDF/LSA mistakes (document frequency bugs, dimensionality issues)
- Ensures consistent semantic analysis patterns across Epic 4 stories
- Reduces Epic 4 code review cycles (playbook documents best practices upfront)
- Creates knowledge base for future semantic features (Word2Vec, topic modeling)

**Dependencies:**
- **Story 3.5.4 (Semantic Dependencies)** - scikit-learn, joblib, gensim installed and smoke tested
- **Story 3.5.5 (Model/Cache ADR)** - Caching strategy documented (joblib, user home cache)
- **Story 3.5.6 (Semantic QA Fixtures)** - Test corpus and ground truth available
- **scikit-learn documentation** - Official docs for TfidfVectorizer, TruncatedSVD
- **Enterprise constraint:** Classical NLP only (no transformer models)

**Technical Foundation:**
- **TF-IDF (Term Frequency-Inverse Document Frequency)** - Vectorization for text documents
- **LSA (Latent Semantic Analysis)** - Dimensionality reduction using Truncated SVD
- **Cosine Similarity** - Similarity metric for document vectors
- **scikit-learn API** - fit(), transform(), fit_transform() patterns

**Key Requirements:**
1. **Playbook Document:** Comprehensive guide for TF-IDF/LSA implementation
2. **Code Examples:** Working examples with audit domain data
3. **Best Practices:** Configuration recommendations, performance optimization
4. **Common Gotchas:** Edge cases, debugging tips, what to avoid
5. **Integration Patterns:** How to integrate with Epic 3 chunks and Epic 4 pipeline

## Acceptance Criteria

**AC-3.5.7-1: Playbook Document Created (P0 - Critical)**
- Playbook file created: `docs/tfidf-lsa-playbook.md`
- Document includes sections:
  - **Overview:** What TF-IDF/LSA are, when to use them
  - **Quick Start:** Minimal working example (10-20 lines)
  - **TF-IDF Deep Dive:** Vectorization, configuration, best practices
  - **LSA Deep Dive:** Dimensionality reduction, n_components selection, interpretation
  - **Cosine Similarity:** Similarity calculations, ranking, thresholds
  - **Caching:** Model persistence with joblib (reference ADR-012)
  - **Testing:** How to use fixtures from Story 3.5.6
  - **Gotchas:** Common mistakes and how to avoid them
  - **References:** Links to scikit-learn docs, papers, tutorials
- Document length: 400-600 lines (comprehensive but not overwhelming)
- **Validation:** Manual review by Charlie and Elena for completeness
- **UAT Required:** Yes - Team reviews playbook, confirms clarity and usability

**AC-3.5.7-2: TF-IDF Code Examples Included (P0 - Critical)**
- Playbook includes working TF-IDF examples:
  - **Basic vectorization:** Simple TfidfVectorizer usage
  - **Configuration:** max_features, min_df, max_df, ngram_range, use_idf
  - **Custom preprocessing:** lowercase, stop word removal, stemming
  - **Incremental updates:** partial_fit() for streaming data
  - **Audit domain example:** Vectorize risk findings corpus from Story 3.5.6
- Each example includes:
  - Code snippet (runnable Python)
  - Input data (audit documents or fixture references)
  - Expected output (TF-IDF matrix shape, vocabulary sample)
  - Explanation of why configuration was chosen
- **Validation:** Run all code examples, verify they execute without errors
- **UAT Required:** Yes - Developers validate examples are helpful for Epic 4

**AC-3.5.7-3: LSA Code Examples Included (P0)**
- Playbook includes working LSA examples:
  - **Basic dimensionality reduction:** TruncatedSVD usage
  - **n_components selection:** How to choose number of components (5-50 typical)
  - **Explained variance:** Interpreting explained_variance_ratio_
  - **Component interpretation:** What do LSA components represent?
  - **Audit domain example:** Reduce audit corpus from 1000 features → 10 components
- Each example includes:
  - Code snippet (runnable Python)
  - Input data (TF-IDF matrix from previous example)
  - Expected output (LSA matrix shape, variance explained)
  - Explanation of component selection rationale
- **Validation:** Run all code examples, verify they execute without errors
- **UAT Required:** Yes - Developers validate examples are helpful for Epic 4

**AC-3.5.7-4: Cosine Similarity Code Examples Included (P1)**
- Playbook includes cosine similarity examples:
  - **Pairwise similarity:** Calculate similarity between all document pairs
  - **Top-K similar documents:** Find K most similar documents to query
  - **Similarity thresholds:** When to consider documents similar (>0.7 typical)
  - **Audit domain example:** Find similar risk findings, rank by similarity
- Each example includes:
  - Code snippet (runnable Python)
  - Input data (TF-IDF or LSA vectors)
  - Expected output (similarity matrix, top-K results)
  - Explanation of threshold choices
- **Validation:** Run all code examples, verify they execute without errors
- **UAT Required:** No - Similarity is well-understood, examples are reference

**AC-3.5.7-5: Best Practices Documented (P0)**
- Playbook includes best practices section:
  - **TF-IDF Configuration:**
    - max_features: 500-2000 for typical corpus (balance coverage and sparsity)
    - min_df: 2-5 (filter rare terms, reduce noise)
    - max_df: 0.8-0.95 (filter common terms like "the", "and")
    - ngram_range: (1,2) for bi-grams (captures phrases like "data breach")
    - use_idf: True (default, essential for TF-IDF weighting)
  - **LSA Configuration:**
    - n_components: 5-50 typical (more components = more detail, slower computation)
    - Explained variance target: >80% (retain most information)
    - Random state: Set for reproducibility (random_state=42)
  - **Performance:**
    - Cache trained models with joblib (avoid re-training)
    - Use sparse matrices (scipy.sparse) for memory efficiency
    - Profile with pytest-benchmark before optimizing
  - **Audit Domain:**
    - Include domain stop words (e.g., "finding", "control", "recommendation")
    - Preserve capitalized entity IDs (RISK-001, CTRL-042)
- **Validation:** Cross-reference with scikit-learn documentation best practices
- **UAT Required:** Yes - Team validates recommendations align with project needs

**AC-3.5.7-6: Common Gotchas Documented (P0)**
- Playbook includes gotchas section:
  - **TF-IDF Gotchas:**
    - Empty documents: TfidfVectorizer raises error on empty strings (filter upfront)
    - Vocabulary mismatch: transform() fails if features not in fit() vocabulary (use ignore)
    - Document frequency bugs: min_df/max_df calculated on fit() corpus only
    - Memory explosion: Default settings can create huge matrices (use max_features)
  - **LSA Gotchas:**
    - n_components > n_features: TruncatedSVD raises error (cap components)
    - Random initialization: Different runs produce different results (set random_state)
    - Negative values: LSA components can be negative (cosine similarity still works)
  - **Caching Gotchas:**
    - Corpus changes: Cache invalidation needed (check corpus hash)
    - Version changes: scikit-learn 1.x → 2.x may break cached models (re-train)
  - **Testing Gotchas:**
    - Numerical precision: Similarity scores vary slightly across platforms (use tolerance)
- **Validation:** Review gotchas with Epic 4 lead (Charlie), add from experience
- **UAT Required:** Yes - Team confirms gotchas are relevant and preventable

**AC-3.5.7-7: Integration Patterns Documented (P1)**
- Playbook includes integration section:
  - **Epic 3 Chunks → TF-IDF:**
    - Extract text from Chunk.text field
    - Use Chunk.metadata.source_file for provenance
    - Respect Chunk.metadata.entity_tags for entity preservation
  - **Caching Pattern:**
    - Reference ADR-012 model caching strategy
    - Use joblib to save/load trained vectorizers
    - Cache location: ~/.data-extract/models/tfidf/
  - **Pipeline Integration:**
    - Semantic analysis as ProcessingResult → SemanticResult transform
    - TF-IDF/LSA in src/data_extract/semantic/ module (Epic 4)
  - **Testing Pattern:**
    - Use fixtures from Story 3.5.6 (audit_corpus, similarity_ground_truth)
    - Validate with pytest, compare to ground truth within tolerance
- **Validation:** Review integration patterns with Epic 4 lead
- **UAT Required:** No - Epic 4 will validate integration

## Acceptance Criteria Trade-offs and Deferrals

**AC-3.5.7-4 Reduced Priority (P1):**
- **Issue:** Cosine similarity is well-understood, may not need extensive examples
- **Resolution:** Include examples but reduce priority to P1 (helpful but not critical)
- **Rationale:** Developers can reference scikit-learn docs if needed; TF-IDF/LSA more complex
- **Documented In:** AC priority levels

**AC-3.5.7-7 Reduced Priority (P1):**
- **Issue:** Integration patterns are Epic 4 implementation details (may change)
- **Resolution:** Include patterns but reduce priority to P1 (guidance, not mandate)
- **Rationale:** Epic 4 stories will finalize integration; playbook provides starting point
- **Documented In:** AC priority levels

## Tasks / Subtasks

### Task 1: Research TF-IDF/LSA Best Practices (AC: All)
- [ ] Review scikit-learn TfidfVectorizer documentation and examples
- [ ] Review scikit-learn TruncatedSVD (LSA) documentation and examples
- [ ] Research TF-IDF best practices (Stack Overflow, blog posts, papers)
- [ ] Research LSA dimensionality selection heuristics
- [ ] Review Story 3.5.6 fixtures for audit domain examples
- [ ] Review ADR-012 for caching patterns

### Task 2: Create Playbook Document Structure (AC: #3.5.7-1)
- [ ] Create `docs/tfidf-lsa-playbook.md`
- [ ] Add header and table of contents
- [ ] Add sections:
  - Overview
  - Quick Start
  - TF-IDF Deep Dive
  - LSA Deep Dive
  - Cosine Similarity
  - Caching with joblib
  - Testing with Fixtures
  - Best Practices
  - Common Gotchas
  - Integration Patterns
  - References

### Task 3: Write Overview Section (AC: #3.5.7-1)
- [ ] Explain what TF-IDF is (term frequency × inverse document frequency)
- [ ] Explain what LSA is (dimensionality reduction via SVD)
- [ ] Explain when to use TF-IDF/LSA (document similarity, topic modeling prep)
- [ ] Explain enterprise constraint (classical NLP only, no transformers)
- [ ] Reference Epic 4 use cases (risk finding similarity, control clustering)

### Task 4: Write Quick Start Section (AC: #3.5.7-1, #3.5.7-2)
- [ ] Create minimal working example (10-20 lines):
  ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.metrics.pairwise import cosine_similarity

  # Sample corpus
  docs = ["Data breach from weak passwords", "Password policy failure", "Vendor contract management"]

  # Vectorize and compute similarity
  vectorizer = TfidfVectorizer()
  tfidf_matrix = vectorizer.fit_transform(docs)
  similarities = cosine_similarity(tfidf_matrix)
  print(similarities)  # Expect docs 0 and 1 to be similar
  ```
- [ ] Explain each line
- [ ] Show expected output

### Task 5: Write TF-IDF Deep Dive Section (AC: #3.5.7-2, #3.5.7-5)
- [ ] Explain TF-IDF weighting formula
- [ ] Document TfidfVectorizer configuration parameters:
  - max_features, min_df, max_df, ngram_range, use_idf, smooth_idf, sublinear_tf
- [ ] Provide basic vectorization example
- [ ] Provide configuration example (max_features=1000, ngram_range=(1,2))
- [ ] Provide custom preprocessing example (lowercase, stop words, stemming)
- [ ] Provide audit domain example using Story 3.5.6 fixtures
- [ ] Document best practices (configuration recommendations)

### Task 6: Write LSA Deep Dive Section (AC: #3.5.7-3, #3.5.7-5)
- [ ] Explain LSA dimensionality reduction concept
- [ ] Document TruncatedSVD configuration parameters:
  - n_components, algorithm, n_iter, random_state
- [ ] Provide basic LSA example (reduce 1000 features → 10 components)
- [ ] Provide n_components selection example (plot explained variance)
- [ ] Provide component interpretation example (top terms per component)
- [ ] Provide audit domain example (reduce audit corpus dimensionality)
- [ ] Document best practices (n_components selection, variance targets)

### Task 7: Write Cosine Similarity Section (AC: #3.5.7-4)
- [ ] Explain cosine similarity metric (dot product / magnitude)
- [ ] Provide pairwise similarity example (cosine_similarity on matrix)
- [ ] Provide top-K similar documents example (argsort, ranking)
- [ ] Provide similarity threshold example (filter by >0.7)
- [ ] Provide audit domain example (find similar risk findings)
- [ ] Document similarity interpretation (0.0=dissimilar, 1.0=identical)

### Task 8: Write Caching Section (AC: #3.5.7-1, #3.5.7-7)
- [ ] Reference ADR-012 model caching strategy
- [ ] Provide joblib save example (joblib.dump with compress=3)
- [ ] Provide joblib load example (joblib.load with cache validation)
- [ ] Document cache location (~/.data-extract/models/tfidf/)
- [ ] Document cache invalidation triggers (corpus hash, version changes)
- [ ] Provide corpus hash calculation example

### Task 9: Write Testing Section (AC: #3.5.7-1, #3.5.7-7)
- [ ] Reference Story 3.5.6 semantic QA fixtures
- [ ] Provide pytest test example using audit_corpus fixture
- [ ] Provide ground truth validation example using similarity_ground_truth
- [ ] Document tolerance for numerical comparisons (±0.15 typical)
- [ ] Reference smoke tests from Story 3.5.4

### Task 10: Write Best Practices Section (AC: #3.5.7-5)
- [ ] Document TF-IDF configuration best practices (see AC-3.5.7-5)
- [ ] Document LSA configuration best practices (see AC-3.5.7-5)
- [ ] Document performance best practices (caching, sparse matrices, profiling)
- [ ] Document audit domain best practices (domain stop words, entity preservation)
- [ ] Cross-reference scikit-learn documentation

### Task 11: Write Common Gotchas Section (AC: #3.5.7-6)
- [ ] Document TF-IDF gotchas (see AC-3.5.7-6)
- [ ] Document LSA gotchas (see AC-3.5.7-6)
- [ ] Document caching gotchas (see AC-3.5.7-6)
- [ ] Document testing gotchas (see AC-3.5.7-6)
- [ ] Include code examples showing how to avoid each gotcha

### Task 12: Write Integration Patterns Section (AC: #3.5.7-7)
- [ ] Document Epic 3 Chunks → TF-IDF integration
- [ ] Document caching pattern (reference ADR-012)
- [ ] Document pipeline integration (semantic module in Epic 4)
- [ ] Document testing pattern (use Story 3.5.6 fixtures)
- [ ] Provide code examples for each pattern

### Task 13: Write References Section (AC: #3.5.7-1)
- [ ] Link to scikit-learn TfidfVectorizer documentation
- [ ] Link to scikit-learn TruncatedSVD documentation
- [ ] Link to scikit-learn cosine_similarity documentation
- [ ] Link to joblib documentation
- [ ] Link to Story 3.5.4 (semantic dependencies smoke tests)
- [ ] Link to Story 3.5.5 (ADR-012 model caching)
- [ ] Link to Story 3.5.6 (semantic QA fixtures)
- [ ] Link to relevant papers/tutorials (optional)

### Task 14: Validate Code Examples
- [ ] Extract all code examples from playbook
- [ ] Create test script to run all examples
- [ ] Verify all examples execute without errors
- [ ] Verify expected outputs match documentation
- [ ] Fix any broken examples

### Task 15: Quality Gates and UAT
- [ ] Manual review of playbook completeness (all sections present)
- [ ] Spell check and grammar review
- [ ] Verify all code examples are runnable
- [ ] Verify all links are valid (documentation, story references)
- [ ] UAT: Team reviews playbook for clarity and usability (AC-3.5.7-1)
- [ ] UAT: Developers validate TF-IDF examples helpful (AC-3.5.7-2)
- [ ] UAT: Developers validate LSA examples helpful (AC-3.5.7-3)
- [ ] UAT: Team validates best practices align with needs (AC-3.5.7-5)
- [ ] UAT: Team confirms gotchas are relevant (AC-3.5.7-6)

## Dev Notes

**Provenance Tracking:**
- tfidf-lsa-playbook.md is a developer guide (no source_hash needed)
- Document includes "Last Updated: 2025-11-16" timestamp
- Reference Epic 3.5, Story 3.5.7 in document header

**Structured Logging:**
- No structured logging needed (documentation, not code)
- Git commit message should reference Story 3.5.7 and Epic 3.5

**Pipeline Wiring:**
- Playbook is developer reference (not part of pipeline)
- Epic 4 stories will implement patterns from playbook
- Integration point: Developers read playbook → implement semantic features

**Quality Gates:**
- No code quality gates (markdown documentation)
- Validation gates:
  - All code examples execute without errors
  - All links are valid (documentation, story references)
  - Team approval of clarity and usability

**Document Structure:**
```markdown
# TF-IDF/LSA Implementation Playbook

## Overview
[What TF-IDF/LSA are, when to use, Epic 4 context]

## Quick Start
[Minimal 10-20 line working example]

## TF-IDF Deep Dive
[Vectorization, configuration, examples, best practices]

## LSA Deep Dive
[Dimensionality reduction, n_components, examples, best practices]

## Cosine Similarity
[Similarity calculations, ranking, thresholds, examples]

## Caching with joblib
[Model persistence, cache location, invalidation, examples]

## Testing with Fixtures
[Using Story 3.5.6 fixtures, ground truth validation, examples]

## Best Practices
[Configuration recommendations, performance tips, audit domain tips]

## Common Gotchas
[TF-IDF gotchas, LSA gotchas, caching gotchas, testing gotchas]

## Integration Patterns
[Epic 3 chunks, pipeline integration, testing patterns]

## References
[Links to scikit-learn docs, stories, papers]
```

**TF-IDF Configuration Recommendations:**
```python
# Recommended TF-IDF configuration for audit domain
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=1000,      # Limit vocabulary size (balance coverage and sparsity)
    min_df=2,               # Filter rare terms (appear in <2 documents)
    max_df=0.9,             # Filter common terms (appear in >90% of documents)
    ngram_range=(1, 2),     # Include unigrams and bigrams (capture phrases)
    use_idf=True,           # Use IDF weighting (essential for TF-IDF)
    smooth_idf=True,        # Smooth IDF weights (prevent division by zero)
    sublinear_tf=False,     # Use raw term frequency (not log-scaled)
    lowercase=True,         # Convert to lowercase (default)
    stop_words="english",   # Remove common English words (the, and, or, etc.)
    strip_accents="unicode" # Remove accents (café → cafe)
)
```

**LSA Configuration Recommendations:**
```python
# Recommended LSA configuration for audit domain
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(
    n_components=10,       # Reduce to 10 dimensions (5-50 typical)
    algorithm="randomized", # Faster than "arpack" for large matrices
    n_iter=5,              # Number of iterations (more = more accurate)
    random_state=42        # Set for reproducibility (essential for testing)
)
```

**Audit Domain Stop Words Example:**
```python
# Extend default stop words with audit domain terms
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

audit_stop_words = ENGLISH_STOP_WORDS.union([
    "finding", "control", "recommendation", "assessment",
    "risk", "compliance", "audit", "process", "requirement"
])

vectorizer = TfidfVectorizer(stop_words=audit_stop_words)
```

**Entity Preservation Example:**
```python
# Preserve capitalized entity IDs (RISK-001, CTRL-042) in vocabulary
from sklearn.feature_extraction.text import TfidfVectorizer
import re

def preprocess_text(text):
    """Custom preprocessing that preserves entity IDs."""
    # Extract entity IDs (RISK-001, CTRL-042, etc.)
    entity_pattern = r'\b[A-Z]{3,5}-\d{3}\b'
    entities = re.findall(entity_pattern, text)

    # Lowercase everything else
    text = text.lower()

    # Re-insert entities in original case
    for entity in entities:
        text = text.replace(entity.lower(), entity)

    return text

vectorizer = TfidfVectorizer(preprocessor=preprocess_text)
```

**Common Gotchas - Empty Documents:**
```python
# WRONG: Empty documents cause TfidfVectorizer to fail
docs = ["Data breach", "", "Password policy"]  # Empty string
vectorizer.fit(docs)  # Raises error!

# RIGHT: Filter empty documents before vectorization
docs = ["Data breach", "", "Password policy"]
docs_filtered = [doc for doc in docs if doc.strip()]  # Remove empty
vectorizer.fit(docs_filtered)  # Success
```

**Common Gotchas - Vocabulary Mismatch:**
```python
# WRONG: transform() on new vocabulary fails
train_docs = ["Data breach", "Password policy"]
test_docs = ["Vendor management", "Contract review"]  # Different vocabulary

vectorizer = TfidfVectorizer()
vectorizer.fit(train_docs)
vectorizer.transform(test_docs)  # May produce all-zero vectors (no matching terms)

# RIGHT: Expect vocabulary mismatch, handle gracefully
# Option 1: Fit on combined corpus
all_docs = train_docs + test_docs
vectorizer.fit(all_docs)

# Option 2: Accept zero vectors for new vocabulary
# (valid for some use cases like anomaly detection)
```

**Retrospective Learnings Applied:**
- This story directly addresses "TF-IDF/LSA playbook (Owner: Charlie + Elena, Est: 4h)" (Epic 3 retro prep task)
- Prevents Epic 4 trial-and-error learning (developers have working examples upfront)
- Documents best practices and gotchas (reduces code review cycles)
- Creates knowledge base for future semantic features (reusable patterns)

**Next Story Dependencies:**
- Epic 4 stories will reference playbook for TF-IDF/LSA implementation
- Playbook provides starting point for semantic analysis features
- Developers adapt examples from playbook to specific Epic 4 requirements
