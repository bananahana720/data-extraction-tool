# Story 3.5.3: Test-Dependency Audit Documentation

Status: todo - Ready for implementation (Epic 3.5 - Bridge Epic)

## Story

As a **developer adding new dependencies to data-extraction-tool**,
I want **documented process for auditing test dependencies and their impacts**,
so that **I can validate new dependencies don't break existing tests or introduce performance regressions**.

## Context Summary

**Epic Context:** Story 3.5.3 is part of Epic 3.5 (Bridge Epic - Tooling & Semantic Prep), which prepares the project for Epic 4 (Foundational Semantic Analysis). This story addresses a significant discovery from the Epic 3 retrospective: "Dependency audit process still undocumented."

**Business Value:**
- Prevents test breakage from new dependencies (e.g., version conflicts, changed APIs)
- Catches performance regressions early (e.g., slow imports, increased memory usage)
- Ensures CI/CD pipeline stability across dependency upgrades
- Reduces debugging time when dependency issues arise (documented process to follow)
- Enables confident dependency updates for security patches or new features

**Dependencies:**
- **Epic 3 Complete (Chunk & Output)** - All dependencies identified (spaCy, textstat, pandas, jq)
- **Story 2.5.3 (Quality Gate Automation)** - Pre-commit hooks and CI/CD baseline for comparison
- **Story 2.5.4 (CI/CD Enhancement)** - CI/CD caching, performance monitoring infrastructure
- **Epic 3 Retrospective (2025-11-16)** - Action item: "Test-dependency audit doc (Owner: Winston)"
- **Epic 4 Prep:** Upcoming semantic dependencies (scikit-learn, joblib, gensim) need audit process

**Technical Foundation:**
- **pytest with markers** - Test organization for targeted execution (unit, integration, performance)
- **pip-audit** - Security vulnerability scanning for dependencies
- **pytest-benchmark** - Performance regression testing for dependency impacts
- **Coverage.py** - Code coverage analysis to detect test breakage
- **pip freeze** - Dependency version pinning and comparison

**Key Requirements:**
1. **Audit Process:** Step-by-step documentation for dependency audits
2. **Test Categories:** Document impact assessment per test category (unit, integration, performance)
3. **Performance Baseline:** Document how to establish and compare performance baselines
4. **Troubleshooting:** Common dependency issues and resolution patterns
5. **CI/CD Integration:** How audits fit into CI/CD pipeline and pre-commit hooks

## Acceptance Criteria

**AC-3.5.3-1: Dependency Audit Process Documented (P0 - Critical)**
- Documentation file created at `docs/dependency-audit-process.md`
- Process includes step-by-step instructions for:
  1. **Pre-Audit:** Record current state (pip freeze, test baseline, coverage baseline)
  2. **Install Dependency:** Add to pyproject.toml, install in venv, update lock file
  3. **Security Audit:** Run pip-audit to check for vulnerabilities
  4. **Test Execution:** Run unit/integration/performance tests with new dependency
  5. **Coverage Check:** Verify coverage hasn't dropped (may increase if new code)
  6. **Performance Comparison:** Compare test execution time against baseline
  7. **CI/CD Validation:** Push to feature branch, verify CI passes
  8. **Documentation Update:** Update CLAUDE.md, tech specs, dependency lists
- Each step includes example commands and expected outputs
- **Validation:** Manual review by team for completeness and clarity
- **UAT Required:** Yes - Test process by auditing scikit-learn for Epic 4

**AC-3.5.3-2: Test Impact Categories Documented (P0)**
- Document how dependencies affect different test categories:
  - **Unit Tests:** Direct imports, API compatibility, mocking impact
  - **Integration Tests:** Component interactions, data format changes, pipeline flow
  - **Performance Tests:** Import overhead, memory usage, execution time
  - **Fixtures:** Shared test data compatibility, serialization changes
- Include examples from Epic 3 dependencies:
  - spaCy: Heavy import overhead (1.2s first load), model caching required (Story 2.5.2)
  - pandas: CSV/JSON validation impact (Story 3.4, 3.6)
  - textstat: Quality metrics performance (Story 3.3)
  - jq: External binary dependency, PATH requirements (Story 3.4)
- **Validation:** Cross-reference with actual dependency additions from Epic 2.5-3
- **UAT Required:** No - Examples verify accuracy

**AC-3.5.3-3: Performance Baseline Process Documented (P1)**
- Document how to establish and compare performance baselines:
  - **Baseline Creation:** Run pytest-benchmark before adding dependency
  - **Baseline Storage:** Store in docs/performance-baselines-epic-*.md
  - **Comparison Method:** Run pytest-benchmark after adding dependency, diff results
  - **Thresholds:** Define acceptable regression (<10% for non-critical, <5% for critical path)
  - **Regression Handling:** Document escalation process if thresholds exceeded
- Include example from Story 2.5.2.1 (spaCy performance impact analysis)
- **Validation:** Compare with existing performance baseline docs
- **UAT Required:** No - Process validation via Epic 4 prep

**AC-3.5.3-4: Troubleshooting Guide Documented (P1)**
- Document common dependency issues and resolutions:
  - **Version Conflicts:** How to resolve with dependency resolution tools
  - **Import Errors:** Module not found, circular imports, namespace conflicts
  - **Test Failures:** API changes, deprecated features, type mismatches
  - **Performance Regressions:** Identify bottlenecks, optimize imports (lazy loading)
  - **CI/CD Failures:** Cache invalidation, platform-specific issues (Windows vs Linux)
- Include examples from Epic 2.5-3 dependency work:
  - spaCy model caching in CI (Story 2.5.4)
  - jq binary installation on different platforms (Story 3.4)
  - pandas version compatibility (Story 3.6)
- **Validation:** Review against actual issues encountered in Epic 2.5-3
- **UAT Required:** Yes - Team validates troubleshooting guide covers common cases

**AC-3.5.3-5: CI/CD Integration Documented (P0)**
- Document how dependency audits integrate with CI/CD:
  - **Pre-commit Hooks:** What checks run automatically (black, ruff, mypy)
  - **CI Pipeline Stages:** Dependency installation, test execution, coverage, performance
  - **Caching Strategy:** How dependencies are cached (pip cache, spaCy models)
  - **Failure Handling:** What to do when CI fails on dependency update
  - **Security Scanning:** pip-audit in CI, vulnerability reporting
- Include references to .github/workflows/ and .pre-commit-config.yaml
- **Validation:** Cross-reference with actual CI/CD configuration files
- **UAT Required:** No - CI/CD configuration validates accuracy

## Acceptance Criteria Trade-offs and Deferrals

**AC-3.5.3-3 Trade-off (Performance Thresholds):**
- **Issue:** Acceptable performance regression thresholds are context-dependent
- **Resolution:** Document thresholds as guidelines, not hard rules (5-10% regression range)
- **Rationale:** Critical path (extraction/chunking) stricter than non-critical (CLI startup)
- **Documented In:** Performance Baseline Process section, Dev Notes

**No Deferrals:** All ACs are needed before Epic 4 semantic dependencies.

## Tasks / Subtasks

### Task 1: Create Dependency Audit Process Documentation (AC: #3.5.3-1)
- [ ] Create `docs/dependency-audit-process.md`
- [ ] Write introduction explaining purpose and when to use audit process
- [ ] Document Pre-Audit step (pip freeze, test baseline, coverage baseline)
  - Example commands: `pip freeze > pre-audit-deps.txt`, `pytest --benchmark-only --benchmark-save=pre`
- [ ] Document Install Dependency step (pyproject.toml, venv, lock file)
  - Example: `pip install scikit-learn`, `pip freeze > requirements.txt`
- [ ] Document Security Audit step (pip-audit)
  - Example: `pip-audit --desc`, expected output with vulnerability report
- [ ] Document Test Execution step (unit/integration/performance)
  - Example: `pytest -m unit -v`, `pytest -m integration -v`, `pytest -m performance -v`
- [ ] Document Coverage Check step (coverage comparison)
  - Example: `pytest --cov=src --cov-report=term`, compare to baseline
- [ ] Document Performance Comparison step (pytest-benchmark)
  - Example: `pytest --benchmark-only --benchmark-compare=pre`, interpret diff
- [ ] Document CI/CD Validation step (feature branch push, verify CI)
  - Example: `git push origin feature/add-sklearn`, check GitHub Actions
- [ ] Document Documentation Update step (CLAUDE.md, tech specs)
  - Example: Update Technology Stack table, add to Dependencies section

### Task 2: Document Test Impact Categories (AC: #3.5.3-2)
- [ ] Add "## Test Impact Categories" section to dependency-audit-process.md
- [ ] Document Unit Tests impact (imports, API, mocking)
  - Example: spaCy import overhead requires lazy loading pattern
- [ ] Document Integration Tests impact (components, data, pipeline)
  - Example: pandas CSV validation affects output formatter tests
- [ ] Document Performance Tests impact (import time, memory, execution)
  - Example: spaCy 1.2s first load, 4000+ words/sec processing (Story 2.5.2)
- [ ] Document Fixtures impact (data compatibility, serialization)
  - Example: JSON schema changes require fixture updates (Story 3.4)
- [ ] Add cross-references to actual Epic 3 dependency stories

### Task 3: Document Performance Baseline Process (AC: #3.5.3-3)
- [ ] Add "## Performance Baseline Process" section to dependency-audit-process.md
- [ ] Document baseline creation (pytest-benchmark before dependency)
  - Example: `pytest tests/performance/ --benchmark-save=epic3-baseline`
- [ ] Document baseline storage (docs/performance-baselines-epic-*.md)
  - Example: Story 2.5.2.1 performance baseline document structure
- [ ] Document comparison method (pytest-benchmark after dependency)
  - Example: `pytest tests/performance/ --benchmark-compare=epic3-baseline`
- [ ] Document acceptable regression thresholds
  - Critical path (<5%): extraction, chunking, core pipeline
  - Non-critical (<10%): CLI startup, formatting, organization
- [ ] Document regression handling process
  - If exceeded: Profile with cProfile, optimize, or escalate to team

### Task 4: Document Troubleshooting Guide (AC: #3.5.3-4)
- [ ] Add "## Troubleshooting Common Issues" section to dependency-audit-process.md
- [ ] Document version conflict resolution
  - Example: `pip install --upgrade pip`, use pip-tools for resolution
- [ ] Document import error debugging
  - Example: `python -c "import module"`, check PYTHONPATH, verify installation
- [ ] Document test failure debugging
  - Example: API changes require test updates, use `pytest -vv --showlocals`
- [ ] Document performance regression debugging
  - Example: `python -m cProfile -o profile.stats script.py`, analyze with snakeviz
- [ ] Document CI/CD failure debugging
  - Example: Cache invalidation with `@actions/cache`, platform-specific issues
- [ ] Add examples from Epic 2.5-3 actual issues (spaCy caching, jq binary, pandas)

### Task 5: Document CI/CD Integration (AC: #3.5.3-5)
- [ ] Add "## CI/CD Integration" section to dependency-audit-process.md
- [ ] Document pre-commit hooks (black, ruff, mypy from .pre-commit-config.yaml)
- [ ] Document CI pipeline stages (from .github/workflows/)
  - Dependency installation, test execution, coverage reporting, performance monitoring
- [ ] Document caching strategy (pip cache, spaCy models)
  - Example: `@actions/cache` for pip dependencies, spaCy model cache
- [ ] Document failure handling process
  - Example: CI fails → check logs → local reproduction → fix → push
- [ ] Document security scanning (pip-audit in CI)
  - Example: Weekly vulnerability scans, critical CVE handling

### Task 6: Add Examples from Epic 3 Dependencies
- [ ] Extract spaCy audit example from Story 2.5.2
  - Import overhead, model caching, CI/CD integration
- [ ] Extract pandas audit example from Story 3.6
  - CSV validation, version compatibility
- [ ] Extract textstat audit example from Story 3.3
  - Quality metrics performance impact
- [ ] Extract jq audit example from Story 3.4
  - External binary dependency, PATH requirements, platform differences

### Task 7: Quality Gates and UAT
- [ ] Manual review of dependency-audit-process.md for completeness
- [ ] Spell check and grammar review
- [ ] Verify all cross-references are valid (story IDs, file paths)
- [ ] UAT: Test process by auditing scikit-learn for Epic 4 (AC-3.5.3-1)
- [ ] UAT: Team reviews troubleshooting guide (AC-3.5.3-4)
- [ ] Update CLAUDE.md to reference dependency-audit-process.md

## Dev Notes

**Provenance Tracking:**
- dependency-audit-process.md is a process document (no source_hash needed)
- Document should include "Last Updated: YYYY-MM-DD" timestamp
- Reference Epic 3.5, Story 3.5.3 in document header

**Structured Logging:**
- No structured logging needed (process documentation, not code)
- Git commit message should reference Story 3.5.3 and Epic 3.5

**Pipeline Wiring:**
- Not applicable (documentation, not code)
- Document how new dependencies integrate into pipeline (import patterns, lazy loading)

**Quality Gates:**
- No code quality gates (markdown documentation)
- Manual review for:
  - Markdown formatting correctness
  - Link validity (file paths, story references)
  - Command example accuracy (test commands locally)
  - Spelling and grammar

**Testing Strategy:**
- UAT: Test process by auditing scikit-learn for Epic 4 (validates process completeness)
- UAT: Team reviews and validates troubleshooting guide covers common cases
- Cross-reference examples with actual Epic 2.5-3 dependency work

**Document Structure:**
```markdown
# Dependency Audit Process

## Overview
[Purpose, when to use, who uses]

## Audit Process
[8-step process with commands and examples]

## Test Impact Categories
[Unit, Integration, Performance, Fixtures with examples]

## Performance Baseline Process
[Baseline creation, storage, comparison, thresholds]

## Troubleshooting Common Issues
[Version conflicts, imports, tests, performance, CI/CD]

## CI/CD Integration
[Pre-commit, pipeline stages, caching, failures, security]

## Examples from Epic 3
[spaCy, pandas, textstat, jq audit examples]

## References
[Links to CI/CD config, pre-commit config, performance baselines]
```

**Example Commands to Document:**
```bash
# Pre-Audit Baseline
pip freeze > pre-audit-deps.txt
pytest --benchmark-only --benchmark-save=pre-audit
pytest --cov=src --cov-report=term > pre-audit-coverage.txt

# Install and Audit
pip install scikit-learn
pip-audit --desc
pytest -m unit -v
pytest -m integration -v
pytest -m performance --benchmark-compare=pre-audit

# CI/CD Validation
git checkout -b feature/add-sklearn
git add pyproject.toml
git commit -m "Add scikit-learn for Epic 4 semantic analysis"
git push origin feature/add-sklearn
# Check GitHub Actions for CI results
```

**Performance Regression Thresholds:**
- **Critical Path (<5% acceptable):**
  - PDF extraction (PyMuPDF operations)
  - Text chunking (spaCy sentence segmentation)
  - Core pipeline flow (ProcessingResult → Chunk)
- **Non-Critical (<10% acceptable):**
  - CLI startup time
  - Output formatting (JSON/TXT/CSV generation)
  - Organization strategies (file writing)
- **Investigate if exceeded:** Profile with cProfile, analyze bottlenecks, optimize or escalate

**Epic 3 Dependency Examples to Include:**
1. **spaCy (Story 2.5.2):**
   - Import overhead: 1.2s first load
   - Model download: 43MB en_core_web_md
   - CI/CD caching required
   - Lazy loading pattern for performance
2. **pandas (Story 3.6):**
   - CSV validation impact
   - Version compatibility (1.x vs 2.x)
   - Memory usage for large datasets
3. **textstat (Story 3.3):**
   - Quality metrics calculation overhead
   - Performance acceptable (<50ms per chunk)
4. **jq (Story 3.4):**
   - External binary dependency
   - Platform-specific installation (Linux vs macOS vs Windows)
   - PATH configuration requirements

**Retrospective Learnings Applied:**
- This story directly addresses "Dependency audit process still undocumented" (Epic 3 retro)
- Prepares for Epic 4 semantic dependencies (scikit-learn, joblib, gensim)
- Prevents repeating Epic 3 dependency challenges (spaCy caching, jq installation)

**Next Story Dependencies:**
- Story 3.5.4 (Semantic dependencies + smoke test) will use this audit process
- Epic 4 stories adding scikit-learn/joblib/gensim will reference this document
