# Story 2.5.3.1: UAT Workflow Framework

Status: drafted

## Story

As a QA Engineer / Developer,
I want automated UAT workflows for test case creation, context-building, AI-assisted test execution, and senior QA review,
so that acceptance criteria can be systematically validated with AI assistance and human oversight.

## Acceptance Criteria

1. **AC-2.5.3.1-1**: UAT workflow `create-test-cases` designed and documented
   - Input: Story markdown files with acceptance criteria
   - Output: Test case specifications with scenarios and expected outcomes
   - Workflow stub created in `bmad/bmm/workflows/4-implementation/create-test-cases/`

2. **AC-2.5.3.1-2**: UAT workflow `build-test-context` designed and documented
   - Input: Story file + test cases from create-test-cases
   - Output: Test context XML with relevant code, fixtures, and configuration
   - Similar to story-context workflow but focused on testing needs

3. **AC-2.5.3.1-3**: UAT workflow `execute-tests` designed and documented
   - Input: Test cases + test context
   - Output: Test execution results with pass/fail/blocked status
   - Integration with tmux-cli for CLI testing scenarios
   - AI-driven test execution with structured result capture

4. **AC-2.5.3.1-4**: UAT workflow `review-uat-results` designed and documented
   - Input: Test execution results
   - Output: Senior QA review with approval/changes-requested status
   - AI-assisted review identifying gaps, edge cases, and quality issues
   - Human-readable summary for stakeholder review

5. **AC-2.5.3.1-5**: Workflow integration documented in tech-spec-epic-2.5.md
   - End-to-end UAT process flow diagram
   - Workflow orchestration and handoff points
   - Integration with existing story-based development workflows

6. **AC-2.5.3.1-6**: Example UAT execution documented for Story 2.5.3
   - Demonstrate all 4 workflows on parent story
   - Generate test cases, context, execute, and review
   - Document lessons learned and refinements needed

## Tasks / Subtasks

**Task 1: Design create-test-cases Workflow** (AC: 1)
- [ ] 1.1: Create `bmad/bmm/workflows/4-implementation/create-test-cases/` directory structure
- [ ] 1.2: Create `workflow.yaml` with input/output specifications
  - [ ] 1.2.1: Define input: story file path, epic file path (for context)
  - [ ] 1.2.2: Define output: test-cases.md with scenarios and expected outcomes
  - [ ] 1.2.3: Define variables: story_key, acceptance_criteria_list, test_coverage_level
- [ ] 1.3: Create `instructions.md` with step-by-step workflow logic
  - [ ] 1.3.1: Step 1: Parse story file and extract acceptance criteria
  - [ ] 1.3.2: Step 2: For each AC, generate test scenarios (happy path, edge cases, error cases)
  - [ ] 1.3.3: Step 3: Map test scenarios to test types (unit, integration, manual)
  - [ ] 1.3.4: Step 4: Output structured test cases with preconditions, steps, expected results
- [ ] 1.4: Create `template.md` for test case output format
- [ ] 1.5: Create `checklist.md` for workflow validation
- [ ] 1.6: Document workflow purpose and usage in README.md

**Task 2: Design build-test-context Workflow** (AC: 2)
- [ ] 2.1: Create `bmad/bmm/workflows/4-implementation/build-test-context/` directory structure
- [ ] 2.2: Create `workflow.yaml` with input/output specifications
  - [ ] 2.2.1: Define input: story file, test cases file, story context XML (optional)
  - [ ] 2.2.2: Define output: test-context.xml with fixtures, test helpers, config
  - [ ] 2.2.3: Define variables: test_fixture_paths, helper_module_paths, config_files
- [ ] 2.3: Create `instructions.md` with context gathering logic
  - [ ] 2.3.1: Step 1: Load test cases and identify required fixtures/helpers
  - [ ] 2.3.2: Step 2: Discover relevant test fixtures (from tests/fixtures/)
  - [ ] 2.3.3: Step 3: Identify test helper modules (conftest.py, custom fixtures)
  - [ ] 2.3.4: Step 4: Gather pytest configuration and markers
  - [ ] 2.3.5: Step 5: Generate test-context.xml with all testing infrastructure
- [ ] 2.4: Create `template.xml` for test context output format
- [ ] 2.5: Create `checklist.md` for workflow validation
- [ ] 2.6: Document relationship to story-context workflow in README.md

**Task 3: Design execute-tests Workflow** (AC: 3)
- [ ] 3.1: Create `bmad/bmm/workflows/4-implementation/execute-tests/` directory structure
- [ ] 3.2: Create `workflow.yaml` with input/output specifications
  - [ ] 3.2.1: Define input: test cases, test context, story file
  - [ ] 3.2.2: Define output: test-execution-results.md with pass/fail/blocked status
  - [ ] 3.2.3: Define variables: test_execution_mode (automated/manual/hybrid), tmux_session_name
- [ ] 3.3: Create `instructions.md` with test execution logic
  - [ ] 3.3.1: Step 1: Load test cases and categorize by execution type
  - [ ] 3.3.2: Step 2: For pytest tests, execute via tmux-cli and capture output
  - [ ] 3.3.3: Step 3: For CLI tests, use tmux-cli to interact with application
  - [ ] 3.3.4: Step 4: For manual tests, guide tester through steps and capture results
  - [ ] 3.3.5: Step 5: Aggregate results with evidence (screenshots, logs, output)
- [ ] 3.4: Create `template.md` for test execution results format
- [ ] 3.5: Document tmux-cli integration patterns and examples
- [ ] 3.6: Create `checklist.md` for workflow validation
- [ ] 3.7: Document workflow purpose and tmux-cli requirements in README.md

**Task 4: Design review-uat-results Workflow** (AC: 4)
- [ ] 4.1: Create `bmad/bmm/workflows/4-implementation/review-uat-results/` directory structure
- [ ] 4.2: Create `workflow.yaml` with input/output specifications
  - [ ] 4.2.1: Define input: test execution results, test cases, story file
  - [ ] 4.2.2: Define output: uat-review.md with approval/changes-requested/blocked
  - [ ] 4.2.3: Define variables: reviewer_name, review_date, quality_gate_level
- [ ] 4.3: Create `instructions.md` with AI-assisted review logic
  - [ ] 4.3.1: Step 1: Load test results and analyze pass/fail/blocked ratio
  - [ ] 4.3.2: Step 2: Identify gaps in test coverage vs acceptance criteria
  - [ ] 4.3.3: Step 3: Analyze edge cases and error scenarios coverage
  - [ ] 4.3.4: Step 4: Check evidence quality (logs, screenshots, assertions)
  - [ ] 4.3.5: Step 5: Generate review findings with severity (critical/major/minor)
  - [ ] 4.3.6: Step 6: Provide approval decision or required changes
- [ ] 4.4: Create `template.md` for UAT review report format
- [ ] 4.5: Create `checklist.md` for review quality validation
- [ ] 4.6: Document senior QA role and decision criteria in README.md

**Task 5: Document Workflow Integration** (AC: 5)
- [ ] 5.1: Update `docs/tech-spec-epic-2.5.md` with UAT workflow section
- [ ] 5.2: Create end-to-end UAT process flow diagram (mermaid or ASCII)
  - [ ] 5.2.1: Show workflow sequence: create-test-cases → build-test-context → execute-tests → review-uat-results
  - [ ] 5.2.2: Show handoff points and required inputs/outputs
  - [ ] 5.2.3: Show integration with story development workflow (story-context, dev-story, code-review)
- [ ] 5.3: Document workflow orchestration options
  - [ ] 5.3.1: Manual sequential execution
  - [ ] 5.3.2: Automated orchestration workflow (future enhancement)
  - [ ] 5.3.3: Selective execution (skip already-completed steps)
- [ ] 5.4: Add UAT workflow usage to CLAUDE.md under "Common Tasks"
- [ ] 5.5: Document when to use UAT workflows vs standard testing

**Task 6: Execute Example UAT for Story 2.5.3** (AC: 6)
- [ ] 6.1: Run create-test-cases on `docs/stories/2.5-3-quality-gate-automation-and-documentation.md`
- [ ] 6.2: Review generated test cases and validate coverage of all 8 ACs
- [ ] 6.3: Run build-test-context to gather test fixtures and helpers
- [ ] 6.4: Run execute-tests (manual/hybrid mode for fixture creation story)
- [ ] 6.5: Run review-uat-results and generate review report
- [ ] 6.6: Document lessons learned and workflow refinements needed
- [ ] 6.7: Update workflow templates based on real execution feedback

**Task 7: Testing and Validation**
- [ ] 7.1: Validate all 4 workflow.yaml files parse correctly
- [ ] 7.2: Validate all workflow instructions are complete and unambiguous
- [ ] 7.3: Validate all templates provide clear structure and guidance
- [ ] 7.4: Execute example UAT end-to-end and capture metrics (time, quality)
- [ ] 7.5: Update CLAUDE.md with UAT workflow patterns and best practices

## Dev Notes

### Epic 2.5 Context

**Parent Story**: Story 2.5.3 (Large Document Fixtures & Testing Infrastructure)

**Deferred Scope**: This story implements AC-2.5.3.7 which was deferred from the parent story:
- Original AC: "UAT workflow structure designed for `bmad:bmm:workflows:create-test-cases` with story markdown input"
- **Expanded Scope**: Full UAT framework with 4 workflows covering the complete testing lifecycle

**Rationale for Expansion**: During story creation, user requested UAT workflow be broken out to a separate story. This allows for proper design of the complete UAT testing lifecycle, not just test case creation.

### UAT Workflow Framework Architecture

**Four-Workflow Pipeline**:

1. **create-test-cases**: Story ACs → Test case specifications
   - Generates test scenarios (happy path, edge cases, error cases)
   - Maps to test types (unit, integration, manual, CLI)
   - Output: Structured test cases with preconditions, steps, expected results

2. **build-test-context**: Test cases → Test context XML
   - Discovers relevant fixtures, helpers, configuration
   - Similar to story-context but focused on testing infrastructure
   - Output: XML with all testing dependencies and references

3. **execute-tests**: Test cases + context → Test results
   - AI-driven test execution with tmux-cli integration
   - Automated (pytest), CLI (tmux-cli), and manual test support
   - Output: Pass/fail/blocked results with evidence

4. **review-uat-results**: Test results → QA review report
   - AI-assisted gap analysis and edge case identification
   - Senior QA approval/changes-requested decision
   - Output: Review report with findings and approval status

**Integration Points**:
- **Input**: Story markdown files from story development workflow
- **Parallel to**: story-context workflow (can reuse story context XML)
- **Output to**: code-review workflow (UAT results inform review)
- **Hand-off**: UAT approval gates story completion (DoD)

### tmux-cli Integration Strategy

**Purpose**: Enable AI-driven CLI testing without human interaction

**Use Cases**:
1. **CLI Command Testing**: Test `data-extract` CLI with various flags and inputs
2. **Interactive Application Testing**: Test prompts, confirmations, error handling
3. **Performance Testing**: Monitor real-time output during batch processing
4. **Integration Testing**: Launch application, interact, capture results

**Pattern** (from CLAUDE.md global instructions):
```bash
# Launch tmux-cli session
tmux-cli --session test-session --command "data-extract process tests/fixtures/"

# Send input to CLI
tmux-cli --session test-session --send "y\n"

# Capture output
tmux-cli --session test-session --capture
```

**Workflow Integration**:
- execute-tests workflow uses tmux-cli for CLI test scenarios
- Automated capture of stdout/stderr for evidence
- Screenshot capture for visual validation (if supported)

[Source: CLAUDE.md#tmux-cli, execute-tests workflow design]

### Relationship to Existing Testing Infrastructure

**Existing Test Structure** (from CLAUDE.md):
```
tests/
├── unit/              # Fast, isolated tests
├── integration/       # Multi-component tests
├── performance/       # Benchmarks and stress tests
└── fixtures/          # Shared test data
```

**UAT Workflows Complement (Not Replace)**:
- **Unit/Integration/Performance Tests**: Continue as-is (pytest-driven, automated)
- **UAT Workflows**: Add systematic AC validation layer above existing tests
- **Coverage**: UAT validates "did we build the right thing?" vs tests validate "did we build it right?"

**Workflow Output Locations**:
```
docs/uat/
├── test-cases/        # Generated from create-test-cases
│   └── {story-key}-test-cases.md
├── test-context/      # Generated from build-test-context
│   └── {story-key}-test-context.xml
├── test-results/      # Generated from execute-tests
│   └── {story-key}-test-results.md
└── reviews/           # Generated from review-uat-results
    └── {story-key}-uat-review.md
```

### Design Principles for UAT Workflows

**Modularity**: Each workflow independent, can be run standalone or orchestrated

**Repeatability**: Same inputs produce same outputs (deterministic test case generation)

**Evidence-Based**: All test results include evidence (logs, screenshots, assertions)

**AI-Assisted, Human-Approved**: AI generates test cases and executes tests, but senior QA reviews and approves

**Integration-First**: Designed to integrate with existing BMAD workflows (story-context, dev-story, code-review)

**Graceful Degradation**: Workflows continue if optional inputs missing (e.g., test-context.xml can be skipped if story-context.xml exists)

### Learnings from Previous Story (2.5.3)

**From Story 2.5-3 (Status: drafted)**

**Story Dependencies**:
- This story (2.5.3.1) is a sub-story of 2.5.3
- Parent story creates large document fixtures and integration tests
- This story creates UAT framework to systematically validate those tests

**Testing Patterns to Follow**:
- Integration test structure from `tests/integration/test_large_files.py`
- Fixture documentation pattern from `tests/fixtures/README.md`
- Memory monitoring pattern from Story 2.5.2.1

**Anti-Patterns to Avoid**:
- Don't defer workflow design to implementation (design workflows NOW)
- Don't create workflows without example execution (Task 6 validates design)
- Don't ignore tmux-cli integration (critical for CLI testing)

**Scope Management**:
- Parent story deferred UAT to this sub-story (correct decision - too large)
- This story focuses ONLY on workflow design and documentation
- Actual UAT execution infrastructure (agents, complex orchestration) deferred to future stories if needed

### Project Structure Notes

**New BMAD Workflow Directories**:
```
bmad/bmm/workflows/4-implementation/
├── create-test-cases/
│   ├── workflow.yaml
│   ├── instructions.md
│   ├── template.md
│   ├── checklist.md
│   └── README.md
├── build-test-context/
│   ├── workflow.yaml
│   ├── instructions.md
│   ├── template.xml
│   ├── checklist.md
│   └── README.md
├── execute-tests/
│   ├── workflow.yaml
│   ├── instructions.md
│   ├── template.md
│   ├── checklist.md
│   └── README.md
└── review-uat-results/
    ├── workflow.yaml
    ├── instructions.md
    ├── template.md
    ├── checklist.md
    └── README.md
```

**Documentation Updates**:
- `docs/tech-spec-epic-2.5.md` - Add UAT workflow section with process flow
- `CLAUDE.md` - Add UAT workflow usage under "Common Tasks"

**No Code Changes Required**:
- This story is workflow design only (markdown/YAML documentation)
- No Python code, no test code modifications
- Future stories may implement UAT orchestration agents if needed

### References

**Technical Specifications**:
- [Source: docs/tech-spec-epic-2.5.md#Story-2.5.3] - Parent story specification
- [Source: docs/stories/2.5-3-quality-gate-automation-and-documentation.md] - Parent story detailed requirements
- [Source: CLAUDE.md#tmux-cli] - tmux-cli usage patterns and examples

**BMAD Workflow Framework**:
- [Source: bmad/core/tasks/workflow.xml] - Workflow execution engine
- [Source: bmad/bmm/workflows/4-implementation/create-story/] - Example workflow structure to follow
- [Source: bmad/bmm/workflows/4-implementation/story-context/] - Similar context-building workflow pattern

**Testing Infrastructure**:
- [Source: CLAUDE.md#Testing] - Test organization, markers, and execution
- [Source: docs/testing-strategy.md] - Testing standards and coverage requirements
- [Source: tests/conftest.py] - Existing test fixtures and helpers

**Related Stories**:
- [Source: stories/2.5-3-quality-gate-automation-and-documentation.md] - Parent story
- [Source: stories/2.5-2-spacy-integration-and-end-to-end-testing.md] - End-to-end testing patterns

## Change Log

- **2025-11-12**: Story drafted as sub-story of 2.5.3
  - AC-2.5.3.7 from parent story expanded into full UAT framework
  - Four workflows designed: create-test-cases, build-test-context, execute-tests, review-uat-results
  - tmux-cli integration documented for CLI testing scenarios
  - Example UAT execution on parent story (Task 6) validates design

## Dev Agent Record

### Context Reference

<!-- Path(s) to story context XML will be added here by context workflow -->

### Agent Model Used

{{agent_model_name_version}}

### Debug Log References

### Completion Notes List

### File List
