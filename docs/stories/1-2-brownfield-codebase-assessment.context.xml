<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>2</storyId>
    <title>Brownfield Codebase Assessment</title>
    <status>drafted</status>
    <generatedAt>2025-11-10</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/1-2-brownfield-codebase-assessment.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a developer</asA>
    <iWant>to assess and document the existing brownfield extraction capabilities</iWant>
    <soThat>I understand what's already built and what gaps need to be filled</soThat>
    <tasks>
- Task 1: Inventory existing extraction capabilities (AC: 1.2.1)
  - 1.1: Analyze brownfield codebase structure (cli/, extractors/, processors/, formatters/, core/, pipeline/, infrastructure/)
  - 1.2: Document PDF extraction implementation (PyMuPDF usage patterns)
  - 1.3: Document Word document extraction (python-docx implementation)
  - 1.4: Document Excel extraction capabilities (openpyxl or xlrd)
  - 1.5: Document OCR capabilities (pytesseract integration)
  - 1.6: Identify existing output formats and file organization
  - 1.7: Document any existing normalization or cleaning logic

- Task 2: Map FR requirements to existing capabilities (AC: 1.2.2)
  - 2.1: Review PRD FR requirements (FR-E1 through FR-S5)
  - 2.2: Create mapping table: FR ID | Requirement | Existing Code | Gap Status
  - 2.3: Identify which FRs are fully met by brownfield code
  - 2.4: Identify which FRs are partially met (need enhancement)
  - 2.5: Identify which FRs are completely missing (Epic 2-4 scope)
  - 2.6: Quantify coverage percentage (e.g., "40% of FRs have existing code")

- Task 3: Map existing code to new architecture (AC: 1.2.3)
  - 3.1: Create mapping: brownfield extractors → src/data_extract/extract/
  - 3.2: Create mapping: brownfield processors → src/data_extract/normalize/
  - 3.3: Create mapping: brownfield formatters → src/data_extract/output/
  - 3.4: Identify code that fits pipeline pattern vs. needs refactoring
  - 3.5: Document which brownfield modules can be wrapped vs. rewritten
  - 3.6: Create refactoring plan with priorities (Phase 1: wrap, Phase 2: refactor, Phase 3: deprecate)

- Task 4: Document technical debt (AC: 1.2.4)
  - 4.1: Identify hardcoded paths and configuration values
  - 4.2: Analyze error handling coverage (try/except patterns, logging)
  - 4.3: Assess test coverage (run pytest --cov on brownfield code)
  - 4.4: Identify performance bottlenecks (large file handling, memory usage)
  - 4.5: Review code quality issues (complexity, duplication, type hints)
  - 4.6: Assign severity ratings: CRITICAL, HIGH, MEDIUM, LOW
  - 4.7: Prioritize technical debt for remediation

- Task 5: Create brownfield-assessment.md report (AC: 1.2.5)
  - 5.1: Write Executive Summary section (3-5 paragraphs)
  - 5.2: Write Existing Capabilities section (detailed inventory)
  - 5.3: Write FR Requirements Mapping section (table from Task 2)
  - 5.4: Write Code Mapping to New Architecture section (table from Task 3)
  - 5.5: Write Technical Debt section (categorized findings from Task 4)
  - 5.6: Write Recommendations section (prioritized action items)
  - 5.7: Add appendices (dependency inventory, file tree, code samples)

- Task 6: Analyze dependencies (AC: 1.2.6)
  - 6.1: Inventory all brownfield dependencies from existing requirements/imports
  - 6.2: Compare current versions to Epic 1 tech spec requirements
  - 6.3: Identify dependencies to upgrade (e.g., PyMuPDF version)
  - 6.4: Identify dependencies to replace (incompatible with Python 3.12 or architecture)
  - 6.5: Document breaking changes for upgrades
  - 6.6: Create migration plan with testing strategy
  - 6.7: Update pyproject.toml if dependencies need to be added/modified

- Task 7: Validate assessment completeness (AC: 1.2.1-1.2.6)
  - 7.1: Verify all AC criteria are addressed in brownfield-assessment.md
  - 7.2: Review report structure and completeness
  - 7.3: Validate mapping tables are accurate and comprehensive
  - 7.4: Ensure recommendations are actionable and prioritized
  - 7.5: Confirm report is ready for team review
    </tasks>
  </story>

  <acceptanceCriteria>
**AC-1.2.1:** Existing extraction capabilities documented by file type
- PDF extraction (PyMuPDF usage) documented
- Word document extraction (python-docx usage) documented
- Excel extraction capabilities identified
- OCR capabilities (pytesseract usage) documented

**AC-1.2.2:** FR requirements mapped to existing vs. missing capabilities
- Table showing: FR ID | Requirement | Existing Code | Gap
- Clear identification of what needs to be built vs. refactored

**AC-1.2.3:** Existing code mapped to new architecture structure
- Old code location → New module mapping documented
- Refactoring plan outlined (what to keep, what to rewrite)

**AC-1.2.4:** Technical debt documented with severity ratings
- Hardcoded paths identified
- Missing error handling noted
- Lack of tests quantified
- Performance bottlenecks identified

**AC-1.2.5:** brownfield-assessment.md report created in docs/
- Structured format: Executive Summary, Capabilities, Gaps, Technical Debt, Recommendations

**AC-1.2.6:** Dependencies requiring upgrade/replacement identified
- Current versions vs. recommended versions
- Breaking changes documented
- Migration plan outlined
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic Technical Specification: Foundation & Project Setup</title>
        <section>Story 1.2: Brownfield Codebase Assessment</section>
        <snippet>Epic 1 establishes foundational infrastructure, transforming brownfield codebase with basic extraction capabilities into production-ready, modular pipeline architecture. Brownfield assessment identifies existing capabilities and maps them to new architecture.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>Functional Requirements (FR-E1 through FR-S5)</section>
        <snippet>Knowledge quality gateway for enterprise Gen AI. Universal corporate file handling (PDF, Word, Excel, PowerPoint), purpose-built for RAG with semantic standardization, intelligent chunking, quality indicators, and metadata enrichment.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Project Initialization - Brownfield Integration</section>
        <snippet>Brownfield project with existing extraction capabilities. Architecture builds on foundation: PyMuPDF, python-docx, pytesseract. Refactor existing code into new modular pipeline (extract → normalize → chunk → analyze → output).</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Epic Breakdown</title>
        <section>Epic 1: Foundation & Project Setup</section>
        <snippet>Establishes Python 3.12 environment, testing infrastructure, brownfield assessment, and core pipeline architecture with Pydantic models defining contracts between stages.</snippet>
      </doc>
      <doc>
        <path>docs/stories/1-1-project-infrastructure-initialization.md</path>
        <title>Story 1.1: Project Infrastructure Initialization (Completed)</title>
        <section>Dev Agent Record - Learnings</section>
        <snippet>Infrastructure established: Python 3.13.9 venv, pyproject.toml with pinned dependencies, pre-commit hooks. Project structure: src/data_extract/ with 9 modules alongside brownfield packages (cli/, extractors/, processors/, formatters/, core/, pipeline/, infrastructure/). Existing test suite: 1007 tests (778 passing).</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/extractors/pdf_extractor.py</path>
        <kind>extractor</kind>
        <symbol>PDFExtractor</symbol>
        <lines>1-50</lines>
        <reason>Brownfield PDF extraction using pypdf, pytesseract OCR fallback, pdfplumber for tables. Needs assessment for Epic 2 integration.</reason>
      </artifact>
      <artifact>
        <path>src/extractors/docx_extractor.py</path>
        <kind>extractor</kind>
        <symbol>DocxExtractor</symbol>
        <lines>1-50</lines>
        <reason>Brownfield Word document extraction using python-docx. Spike implementation with paragraphs and tables. Assess completeness vs. Epic 2 requirements.</reason>
      </artifact>
      <artifact>
        <path>src/extractors/excel_extractor.py</path>
        <kind>extractor</kind>
        <symbol>ExcelExtractor</symbol>
        <lines>N/A</lines>
        <reason>Brownfield Excel extraction (openpyxl). Assess capabilities for Epic 2 data extraction requirements.</reason>
      </artifact>
      <artifact>
        <path>src/extractors/pptx_extractor.py</path>
        <kind>extractor</kind>
        <symbol>PPTXExtractor</symbol>
        <lines>N/A</lines>
        <reason>Brownfield PowerPoint extraction (python-pptx). Assess for Epic 2 slide content extraction.</reason>
      </artifact>
      <artifact>
        <path>src/core/interfaces.py</path>
        <kind>interface</kind>
        <symbol>BaseExtractor, BaseProcessor, BaseFormatter</symbol>
        <lines>1-80</lines>
        <reason>Brownfield interface contracts. Compare to Epic 1 PipelineStage protocol design. May inform new architecture or be adapted.</reason>
      </artifact>
      <artifact>
        <path>src/core/models.py</path>
        <kind>model</kind>
        <symbol>ContentBlock, ExtractionResult, DocumentMetadata, Position</symbol>
        <lines>1-80</lines>
        <reason>Brownfield data models using dataclasses. Compare to Epic 1 Pydantic models (Document, Chunk, Metadata). Assess migration path.</reason>
      </artifact>
      <artifact>
        <path>src/pipeline/extraction_pipeline.py</path>
        <kind>pipeline</kind>
        <symbol>ExtractionPipeline</symbol>
        <lines>1-80</lines>
        <reason>Brownfield pipeline orchestrator. Assess pattern fit with Epic 1 PipelineStage protocol. May inform or conflict with new architecture.</reason>
      </artifact>
      <artifact>
        <path>src/infrastructure/config_manager.py</path>
        <kind>infrastructure</kind>
        <symbol>ConfigManager</symbol>
        <lines>N/A</lines>
        <reason>Brownfield configuration system. Assess vs. Epic 5 configuration cascade pattern (CLI > env > YAML > defaults).</reason>
      </artifact>
      <artifact>
        <path>src/infrastructure/logging_framework.py</path>
        <kind>infrastructure</kind>
        <symbol>LoggingFramework</symbol>
        <lines>N/A</lines>
        <reason>Brownfield logging. Assess vs. Epic 1 structlog requirement for audit trail.</reason>
      </artifact>
      <artifact>
        <path>src/infrastructure/error_handler.py</path>
        <kind>infrastructure</kind>
        <symbol>ErrorHandler</symbol>
        <lines>N/A</lines>
        <reason>Brownfield error handling. Compare to Epic 1 exception hierarchy (DataExtractError → ProcessingError/CriticalError).</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="python-docx" version=">=0.8.11" status="brownfield-compatible"/>
        <package name="pypdf" version=">=3.0.0" status="brownfield-compatible"/>
        <package name="python-pptx" version=">=0.6.21" status="brownfield-compatible"/>
        <package name="openpyxl" version=">=3.0.10" status="brownfield-compatible"/>
        <package name="pdfplumber" version=">=0.10.0" status="brownfield-compatible"/>
        <package name="Pillow" version=">=10.0.0" status="brownfield-compatible"/>
        <package name="pytesseract" version=">=0.3.10" status="optional-ocr"/>
        <package name="pdf2image" version=">=1.16.0" status="optional-ocr"/>
        <package name="click" version=">=8.1.0" status="brownfield-cli" note="Epic 5 migrates to Typer"/>
        <package name="rich" version=">=13.0.0" status="compatible" note="Retained for Epic 5 CLI"/>
        <package name="pydantic" version=">=2.0.0,&lt;3.0" status="epic-1-new"/>
        <package name="PyYAML" version=">=6.0.0,&lt;7.0" status="epic-1-new"/>
        <package name="structlog" version=">=24.0.0,&lt;25.0" status="epic-1-new"/>
        <package name="pytest" version=">=8.0.0,&lt;9.0" status="dev"/>
        <package name="black" version=">=24.0.0,&lt;25.0" status="dev"/>
        <package name="mypy" version=">=1.11.0,&lt;2.0" status="dev"/>
        <package name="ruff" version=">=0.6.0,&lt;0.7" status="dev"/>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <development>
      <constraint>Python 3.12 minimum required (enterprise requirement, ADR-004)</constraint>
      <constraint>All dependencies must be pinned for deterministic builds (audit trail requirement)</constraint>
      <constraint>Classical NLP only - no transformer models (enterprise IT restriction)</constraint>
      <constraint>On-premise processing - no external LLM API calls (security policy)</constraint>
      <constraint>Mypy type checking enabled for new code (src/data_extract/), excluded for brownfield code</constraint>
      <constraint>Pre-commit hooks mandatory: black, ruff, mypy</constraint>
    </development>
    <architecture>
      <constraint>Pipeline stage pattern: Protocol-based interface (PipelineStage[Input, Output]) for all stages</constraint>
      <constraint>Pydantic v2 for all data models with runtime validation (ADR-002)</constraint>
      <constraint>File-based storage only - no database (ADR-003)</constraint>
      <constraint>Streaming pipeline for constant memory footprint (ADR-005)</constraint>
      <constraint>Exception hierarchy: DataExtractError → ProcessingError/CriticalError for continue-on-error batch processing</constraint>
      <constraint>Configuration cascade: CLI > env vars > YAML config > defaults (three-tier precedence)</constraint>
    </architecture>
    <brownfield-integration>
      <constraint>DO NOT delete or modify brownfield code during assessment (Story 1.2 is analysis only)</constraint>
      <constraint>Brownfield code excluded from mypy type checking via pyproject.toml exclude pattern</constraint>
      <constraint>Document which brownfield modules can be wrapped with adapters vs. need full rewrite</constraint>
      <constraint>Plan deprecation timeline for brownfield code as new modules come online in Epics 2-5</constraint>
      <constraint>Prioritize brownfield code reuse where quality is good to minimize reinvention</constraint>
    </brownfield-integration>
    <testing>
      <constraint>Existing test suite: 1007 tests (778 passing) - analyze coverage and quality</constraint>
      <constraint>Test structure mirrors src/ directory layout</constraint>
      <constraint>pytest with coverage reporting (--cov=src --cov-report=term-missing)</constraint>
      <constraint>Identify brownfield modules with no tests (highest refactoring risk)</constraint>
    </testing>
  </constraints>

  <interfaces>
    <interface>
      <name>BaseExtractor</name>
      <kind>Abstract Base Class (brownfield)</kind>
      <signature>
        class BaseExtractor(ABC):
            def extract(self, file_path: Path) -> ExtractionResult
            def supports_format(self, file_path: Path) -> bool
      </signature>
      <path>src/core/interfaces.py</path>
      <note>Brownfield extractor interface. Compare to Epic 1 PipelineStage[Path, Document] protocol.</note>
    </interface>
    <interface>
      <name>PipelineStage Protocol (Epic 1)</name>
      <kind>Protocol (planned)</kind>
      <signature>
        class PipelineStage(Protocol[Input, Output]):
            def process(self, input: Input) -> Output
      </signature>
      <path>src/data_extract/core/pipeline.py (planned in Story 1.4)</path>
      <note>New protocol-based interface. Assess compatibility with brownfield BaseExtractor/BaseProcessor/BaseFormatter.</note>
    </interface>
    <interface>
      <name>ExtractionResult (brownfield)</name>
      <kind>Data Model (dataclass)</kind>
      <signature>
        @dataclass(frozen=True)
        class ExtractionResult:
            content_blocks: tuple[ContentBlock, ...]
            metadata: DocumentMetadata
            success: bool
            errors: tuple[str, ...]
      </signature>
      <path>src/core/models.py</path>
      <note>Brownfield extraction result. Compare to Epic 1 Pydantic Document model. Assess migration strategy.</note>
    </interface>
    <interface>
      <name>Document Model (Epic 1 - Pydantic)</name>
      <kind>Pydantic Model (planned)</kind>
      <signature>
        class Document(BaseModel):
            source_file: Path
            content: str
            metadata: Metadata
            entities: List[Entity]
            model_config = ConfigDict(frozen=False)
      </signature>
      <path>src/data_extract/core/models.py (planned in Story 1.4)</path>
      <note>New Pydantic v2 model with runtime validation. Assess migration path from brownfield ExtractionResult.</note>
    </interface>
  </interfaces>
  <tests>
    <standards>
Brownfield test suite uses pytest 8.x with comprehensive coverage infrastructure. Testing organized into multiple layers: unit tests (test_extractors/, test_processors/, test_formatters/, test_infrastructure/), integration tests (integration/), performance benchmarks (performance/), and edge case validation (test_edge_cases/). Test fixtures defined in conftest.py files at multiple levels. Story 1.1 revealed 1007 tests with 778 passing - assessment must identify test quality (good tests to keep vs. brittle tests to replace) and coverage gaps. Key testing standards: pytest with coverage reporting (--cov=src --cov-report=term-missing), parallel execution support (pytest-xdist), mocking framework (pytest-mock).
    </standards>
    <locations>
      <location>tests/test_extractors/ - Unit tests for PDF, Word, Excel, PowerPoint, CSV, TXT extractors</location>
      <location>tests/test_processors/ - Unit tests for metadata aggregation, quality validation, context linking</location>
      <location>tests/test_formatters/ - Unit tests for JSON, Markdown, chunked text output</location>
      <location>tests/test_infrastructure/ - Unit tests for config manager, logging framework, error handler, progress tracker</location>
      <location>tests/test_pipeline/ - Unit tests for extraction pipeline, batch processor</location>
      <location>tests/test_cli/ - CLI command tests (extract, batch, config, version)</location>
      <location>tests/integration/ - Integration tests for end-to-end workflows, pipeline orchestration</location>
      <location>tests/performance/ - Performance benchmarks and baseline capture</location>
      <location>tests/test_edge_cases/ - Edge case validation (encoding, filesystem, threading, resource limits)</location>
      <location>tests/validation/ - Bug fix validation suite</location>
    </locations>
    <ideas>
      <idea ac="AC-1.2.1">Run pytest with coverage on extractors to quantify test coverage per file type (PDF, Word, Excel, OCR). Document coverage percentage and identify untested code paths.</idea>
      <idea ac="AC-1.2.2">Validate FR requirements mapping by checking if brownfield tests cover FR scenarios. Identify FRs with no test coverage (indicates missing capability or testing gap).</idea>
      <idea ac="AC-1.2.3">Review integration tests to understand brownfield pipeline flow. Map test patterns to Epic 1 PipelineStage architecture to identify refactoring compatibility.</idea>
      <idea ac="AC-1.2.4">Analyze test failures (229 failing tests from 1007 total) to identify technical debt categories: brittle tests, missing error handling, hardcoded paths, environmental dependencies.</idea>
      <idea ac="AC-1.2.5">Create test quality assessment matrix: A (excellent, keep as-is), B (good, minor refactor), C (poor, rewrite needed), D (broken, delete). Include in brownfield-assessment.md.</idea>
      <idea ac="AC-1.2.6">Test dependency compatibility: verify pytest, pytest-cov, pytest-xdist, pytest-mock versions align with Epic 1 pyproject.toml requirements. Document upgrade path if mismatched.</idea>
      <idea>Assess test fixture quality in tests/fixtures/ - determine if fixtures are reusable for Epic 1.3 testing framework or need replacement with cleaner test data.</idea>
      <idea>Performance baseline tests exist (performance/) - assess if benchmarks are accurate and useful for Epic 1-5 regression testing or need redefinition.</idea>
    </ideas>
  </tests>
</story-context>
