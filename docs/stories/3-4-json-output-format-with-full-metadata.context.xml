<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>3.4</storyId>
    <title>JSON Output Format with Full Metadata</title>
    <status>drafted</status>
    <generatedAt>2025-11-14</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/3-4-json-output-format-with-full-metadata.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>RAG engineer integrating document processing with vector databases</asA>
    <iWant>chunks output in valid JSON format with complete metadata</iWant>
    <soThat>I can programmatically ingest, query, and filter chunks for downstream LLM retrieval workflows</soThat>
    <tasks>
### Task 1: Create JSON Schema Definition (AC: #3.4-7)
- Create `src/data_extract/output/schemas/` directory
- Create `data-extract-chunk.schema.json` (JSON Schema Draft 7)
  - Define root object schema (metadata + chunks array)
  - Define metadata object schema (version, timestamp, configuration, sources)
  - Define chunk object schema (chunk_id, text, metadata, entities, quality)
  - Define ChunkMetadata schema (all fields from Story 3.3)
  - Define QualityScore schema (readability, ocr, completeness, coherence, overall, flags)
  - Define EntityReference schema (entity_type, entity_id, positions, is_partial)
  - Add enum definitions for document_type, entity_type, quality flags
  - Add validation rules: score ranges, string patterns, required fields
- Add JSON Schema to package manifest (include in distribution)
- Add type hints for schema validation function

### Task 2: Implement JsonFormatter Component (AC: #3.4-1, #3.4-2, #3.4-3, #3.4-4, #3.4-5, #3.4-6)
- Create `src/data_extract/output/formatters/` directory
- Create `src/data_extract/output/formatters/base.py`
  - Define `BaseFormatter` Protocol
  - Define `FormatResult` dataclass (frozen=True)
- Create `src/data_extract/output/formatters/json_formatter.py`
  - Implement `JsonFormatter` class with format_chunks(), _build_metadata_header(), _validate_against_schema() methods
- Update `src/data_extract/output/__init__.py` to export JsonFormatter, BaseFormatter, FormatResult

### Task 3: Extend Chunk Model Serialization (AC: #3.4-1, #3.4-3)
- Update `Chunk.to_dict()` in `src/data_extract/chunk/models.py`
- Update `ChunkMetadata.to_dict()` (verify from Story 3.3)
- Update `QualityScore.to_dict()` (verify from Story 3.3)
- Update `EntityReference.to_dict()` (verify from Story 3.2)

### Task 4: Unit Testing - JsonFormatter and Schema (AC: all)
- Create `tests/unit/test_output/test_json_formatter.py`
- Create `tests/unit/test_output/test_json_schema.py`
- Use fixtures from `tests/fixtures/chunks/`
- Achieve >90% coverage for json_formatter.py

### Task 5: Integration Testing - End-to-End JSON Generation (AC: all)
- Create `tests/integration/test_output/test_json_output_pipeline.py`
- Create `tests/integration/test_output/test_json_compatibility.py`
- Test JSON parsing with json.load(), pandas.read_json(), jq
- Test chunk queryability with jq and pandas

### Task 6: Performance Testing - JSON Generation Overhead (AC: NFR-P1-E3)
- Create `tests/performance/test_json_performance.py`
- Benchmark JSON generation for 100-chunk and 1000-chunk documents
- Update `docs/performance-baselines-epic-3.md`

### Task 7: Documentation and Validation (AC: all)
- Update `CLAUDE.md` with JsonFormatter usage patterns
- Update `docs/architecture.md` with JSON output format decision
- Create `docs/json-schema-reference.md`
- Run all quality gates (black, ruff, mypy, pytest)
- Validate all 7 ACs end-to-end
    </tasks>
  </story>

  <acceptanceCriteria>
**AC-3.4-1: JSON Structure Includes Chunk Text and Metadata (P0 - Critical)**
- JSON root object contains metadata object and chunks array
- Each chunk object includes chunk_id, text, metadata, entities, quality
- Validation: Unit tests verify structure matches schema, integration tests verify completeness
- UAT Required: Yes - Schema validation against real chunk data

**AC-3.4-2: Output is Valid, Parsable JSON (Not JSON Lines) (P0 - Critical)**
- Single JSON file per output (not newline-delimited JSON Lines format)
- Valid according to JSON specification (RFC 8259)
- Parsable by: Python json.load(), jq, Node.js JSON.parse(), pandas.read_json()
- UTF-8 encoding with BOM for Windows compatibility
- Validation: Unit tests parse output with multiple libraries
- UAT Required: Yes - Critical for downstream tool compatibility

**AC-3.4-3: Metadata Includes All Fields from ChunkMetadata (P1)**
- Each chunk's metadata object includes all fields: chunk_id, source_file, source_hash, document_type, section_context, position_index, entity_tags, quality, word_count, token_count, created_at, processing_version
- Fields never null (empty string or empty array for missing data)
- Datetime fields formatted as ISO 8601 strings
- Validation: Schema validation tests, completeness tests
- UAT Required: No - Covered by unit/schema tests

**AC-3.4-4: JSON is Pretty-Printed (Human Readable) (P2)**
- Output formatted with 2-space indentation
- Fields ordered logically: text first, then metadata, then entities, then quality
- No trailing commas (strict JSON compliance)
- Validation: Visual inspection during development, linting tests
- UAT Required: No - Development-time validation sufficient

**AC-3.4-5: Array of Chunks Filterable/Queryable (P0 - Critical)**
- Chunks stored as JSON array (not object with chunk IDs as keys)
- Supports jq filtering, pandas DataFrame conversion, JavaScript filtering
- Index-based access: chunks[0] returns first chunk
- Validation: Integration tests with jq queries, pandas DataFrame conversion
- UAT Required: Yes - Critical for programmatic filtering

**AC-3.4-6: Configuration and Version in JSON Header (P1)**
- Root metadata object includes processing_version, processing_timestamp, configuration (chunk_size, overlap_pct, entity_aware, quality_enrichment), source_documents, chunk_count
- Configuration enables reproducibility
- Validation: Unit tests verify header structure, integration tests verify accuracy
- UAT Required: No - Covered by unit tests

**AC-3.4-7: JSON Validates Against Schema (P0 - Critical)**
- JSON output validates against data-extract-chunk.schema.json (JSON Schema Draft 7)
- Schema defines required fields, types, enum values, numeric ranges, string patterns
- Schema validation integrated into unit tests
- Validation: Unit tests use jsonschema library, integration tests validate 100-chunk samples
- UAT Required: Yes - Critical for downstream tool integration
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="docs/PRD.md" title="Product Requirements Document" section="Functional Requirements - FR-3.3: Multiple Output Formats">
        Snippet: "JSON: Structured format with full metadata... TXT: Clean plain text, one chunk per file or concatenated... CSV: Tabular index with chunk text and metadata columns"
      </doc>
      <doc path="docs/architecture.md" title="Architecture" section="Epic to Architecture Mapping - Epic 3">
        Snippet: "Epic 3 (Chunking): chunk/ (engine, models, sentence_segmenter) - Uses spaCy from Story 2.5.2, feeds to output/ - Semantic boundary-aware chunking, Generator-based streaming"
      </doc>
      <doc path="docs/tech-spec-epic-3.md" title="Epic 3 Technical Specification" section="2.2 Core Services - OutputFormatter">
        Snippet: "OutputFormatter (src/data_extract/output/formatters/) - Generate JSON, TXT, CSV outputs from enriched chunks - Support parallel format generation (all three formats written concurrently) - Apply organization strategies (by_document, by_entity, flat) - Validate output correctness (JSON schema, CSV escaping, encoding)"
      </doc>
      <doc path="docs/tech-spec-epic-3.md" title="Epic 3 Technical Specification" section="Story 3.4 Acceptance Criteria">
        Snippet: "AC-3.4-1: JSON structure includes chunk text and metadata (P0 - Critical) - AC-3.4-2: Output is valid, parsable JSON (P0 - Critical) - AC-3.4-7: JSON validates against schema (P0 - Critical)"
      </doc>
      <doc path="docs/stories/3-3-chunk-metadata-and-quality-scoring.md" title="Story 3.3 Completion Notes" section="New Services Created">
        Snippet: "QualityScore at src/data_extract/chunk/quality.py - Use QualityScore.to_dict() for JSON serialization. MetadataEnricher at src/data_extract/chunk/metadata_enricher.py - Quality calculation pipeline. ProcessingResult model extended in src/data_extract/core/models.py - Epic 2/3 boundary model."
      </doc>
      <doc path="docs/tech-spec-epic-3.md" title="Epic 3 Technical Specification" section="2.3 Data Models">
        Snippet: "All data models use frozen dataclasses for immutability and structural sharing. Chunk: frozen dataclass with chunk_id, text, metadata (ChunkMetadata), entities (EntityReference[]), quality (QualityScore). FormatResult: frozen dataclass with format_type, output_path, chunk_count, file_size_bytes, duration_seconds, errors."
      </doc>
    </docs>
    <code>
      <artifact path="src/data_extract/chunk/models.py" kind="model" symbol="ChunkMetadata" reason="ChunkMetadata contains all fields to serialize: entity_tags, section_context, quality, source_hash, document_type, word_count, token_count, created_at, processing_version. Has to_dict() method (lines 97-119)">
      <artifact path="src/data_extract/chunk/quality.py" kind="model" symbol="QualityScore" reason="QualityScore frozen dataclass with readability_flesch_kincaid, readability_gunning_fog, ocr_confidence, completeness, coherence, overall, flags. Has to_dict() method (lines 99-118) for JSON serialization">
      <artifact path="src/data_extract/chunk/entity_preserver.py" kind="model" symbol="EntityReference" reason="EntityReference frozen dataclass tracking entity mentions: entity_type, entity_id, start_pos, end_pos, is_partial, context_snippet. Has to_dict() method (lines 60-73)">
      <artifact path="src/data_extract/core/models.py" kind="model" symbol="Chunk" lines="359-378" reason="Chunk BaseModel (Pydantic) is the primary data structure for Story 3.4. Contains: id, text, document_id, position_index, token_count, word_count, entities, section_context, quality_score, readability_scores, metadata">
      <artifact path="src/data_extract/output/__init__.py" kind="module" reason="Output module currently empty (14 lines placeholder). Story 3.4 will create formatters/ subdirectory here for JsonFormatter">
      <artifact path="src/data_extract/chunk/__init__.py" kind="export" reason="Exports Chunk, ChunkMetadata, QualityScore for use by output formatters. Will need to verify all serialization methods work correctly">
    </code>
    <dependencies>
      <python version="3.12">Required enterprise standard, uses modern type hints</python>
      <stdlib>
        <module name="json">JSON output format generation with indent=2 for pretty-printing, ensure_ascii=False for UTF-8</module>
        <module name="pathlib">Cross-platform path handling for output files</module>
        <module name="dataclasses">Frozen dataclass pattern for immutable FormatResult model</module>
        <module name="typing">Type hints for Protocol classes (BaseFormatter), generics, type safety</module>
      </stdlib>
      <external>
        <package name="jsonschema" version=">=4.0.0,&lt;5.0">JSON Schema Draft 7 validation library for AC-3.4-7. Used to validate output against data-extract-chunk.schema.json</package>
        <package name="pydantic" version="2.x">Already installed (Epic 1). Chunk is Pydantic BaseModel, use model_dump() for serialization</package>
      </external>
      <internal>
        <module path="src/data_extract/chunk/models.py">ChunkMetadata.to_dict() provides serialization for metadata field</module>
        <module path="src/data_extract/chunk/quality.py">QualityScore.to_dict() provides quality metrics serialization</module>
        <module path="src/data_extract/chunk/entity_preserver.py">EntityReference.to_dict() provides entity serialization</module>
        <module path="src/data_extract/core/models.py">Chunk BaseModel with Pydantic serialization capabilities</module>
      </internal>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="architecture">BaseFormatter Protocol pattern: All output formatters implement protocol with format_chunks() method returning FormatResult</constraint>
    <constraint type="architecture">Frozen dataclass for FormatResult (immutable per ADR-001): format_type, output_path, chunk_count, file_size_bytes, duration_seconds, errors</constraint>
    <constraint type="data_model">Chunk is Pydantic BaseModel (not frozen dataclass). Use model_dump(mode="python") for serialization, not to_dict()</constraint>
    <constraint type="serialization">All datetime fields MUST serialize to ISO 8601 strings (e.g., "2025-11-14T20:37:23Z") per AC-3.4-3</constraint>
    <constraint type="serialization">All Path fields MUST convert to strings before JSON serialization (pathlib.Path not JSON-serializable)</constraint>
    <constraint type="serialization">Field ordering: text first, then metadata, then entities, then quality (AC-3.4-4 - aids human readability)</constraint>
    <constraint type="format">Single JSON file per output, NOT JSON Lines (AC-3.4-2). Use standard JSON array format for chunks</constraint>
    <constraint type="format">UTF-8 encoding with BOM for Windows compatibility: open(file, 'w', encoding='utf-8-sig')</constraint>
    <constraint type="format">Pretty-printed JSON with 2-space indentation: json.dump(data, f, indent=2, ensure_ascii=False)</constraint>
    <constraint type="validation">JSON Schema Draft 7 compliance. Schema defines required fields, types, enums, numeric ranges (0.0-1.0 for scores)</constraint>
    <constraint type="validation">Schema validation via jsonschema library. Constructor parameter validate=True (default) enables validation</constraint>
    <constraint type="performance">Chunk materialization required for JSON array format (cannot stream). Memory impact ~2x during serialization acceptable per NFR-P2-E3</constraint>
    <constraint type="nfr">NFR-P2-E3: Output generation &lt;1 second per document (all 3 formats parallel). JSON generation is I/O-bound, suitable for concurrent writes</constraint>
    <constraint type="determinism">Same input chunks → same JSON output byte-for-byte (excluding metadata.processing_timestamp). Critical for audit trail (NFR-R3)</constraint>
  </constraints>

  <interfaces>
    <interface name="BaseFormatter" kind="protocol" path="src/data_extract/output/formatters/base.py">
      <signature>def format_chunks(self, chunks: Iterator[Chunk], output_path: Path) -> FormatResult</signature>
      <description>Protocol for all output format implementations. JsonFormatter, TxtFormatter, CsvFormatter implement this interface</description>
    </interface>
    <interface name="ChunkMetadata.to_dict" kind="method" path="src/data_extract/chunk/models.py">
      <signature>def to_dict(self) -> Dict[str, Any]</signature>
      <description>Serializes ChunkMetadata to JSON-compatible dict. Handles entity_tags, quality, datetimes. Lines 97-119</description>
    </interface>
    <interface name="QualityScore.to_dict" kind="method" path="src/data_extract/chunk/quality.py">
      <signature>def to_dict(self) -> Dict[str, Any]</signature>
      <description>Serializes QualityScore to dict with all 7 fields: readability_flesch_kincaid, readability_gunning_fog, ocr_confidence, completeness, coherence, overall, flags. Lines 99-118</description>
    </interface>
    <interface name="EntityReference.to_dict" kind="method" path="src/data_extract/chunk/entity_preserver.py">
      <signature>def to_dict(self) -> Dict[str, Any]</signature>
      <description>Serializes EntityReference with entity_type, entity_id, positions, is_partial, context_snippet. Lines 60-73</description>
    </interface>
    <interface name="Chunk.model_dump" kind="method" path="src/data_extract/core/models.py">
      <signature>def model_dump(self, mode="python") -> Dict[str, Any]</signature>
      <description>Pydantic serialization method. Use mode="python" for JSON-compatible dict. Handles nested BaseModel instances, validates types</description>
    </interface>
    <interface name="jsonschema.validate" kind="function" package="jsonschema">
      <signature>def validate(instance: dict, schema: dict) -> None</signature>
      <description>Validates JSON output against schema. Raises jsonschema.ValidationError if invalid. Use for AC-3.4-7 schema validation</description>
    </interface>
  </interfaces>

  <tests>
    <standards>
      <standard>pytest framework with markers: -m unit, -m integration, -m performance, -m output</standard>
      <standard>Test organization mirrors src/ structure: tests/unit/test_output/, tests/integration/test_output/</standard>
      <standard>Coverage requirement: &gt;90% for json_formatter.py (story-level target per AC)</standard>
      <standard>Pre-commit quality gates: black (formatting), ruff (linting), mypy (type checking) - 0 violations required</standard>
      <standard>CI quality gates: pytest (60% coverage minimum), performance regression check (&lt;10% degradation)</standard>
      <standard>UAT quality gates: 90% pass rate overall, 100% for critical ACs (AC-3.4-1, AC-3.4-2, AC-3.4-5, AC-3.4-7)</standard>
      <standard>Use fixtures from tests/fixtures/chunks/ (enriched chunk samples from Story 3.3)</standard>
      <standard>JSON validation: Use jsonschema library in every test that generates JSON output</standard>
    </standards>
    <locations>
      <location>tests/unit/test_output/test_json_formatter.py - Unit tests for JsonFormatter class (NEW)</location>
      <location>tests/unit/test_output/test_json_schema.py - JSON Schema validation tests (NEW)</location>
      <location>tests/integration/test_output/test_json_output_pipeline.py - End-to-end JSON generation (NEW)</location>
      <location>tests/integration/test_output/test_json_compatibility.py - Cross-platform, encoding tests (NEW)</location>
      <location>tests/performance/test_json_performance.py - JSON generation performance baselines (NEW)</location>
      <location>tests/fixtures/chunks/ - Enriched chunk fixtures from Story 3.3 (EXISTING)</location>
    </locations>
    <ideas>
      <test id="AC-3.4-1" priority="critical">Test JSON structure includes all chunk fields: chunk_id, text, metadata (nested), entities (array), quality (nested). Verify metadata completeness</test>
      <test id="AC-3.4-2" priority="critical">Test output is valid JSON: Parse with json.load(), verify not JSON Lines format. Test UTF-8 encoding, parsability by pandas.read_json(), jq</test>
      <test id="AC-3.4-3" priority="high">Test all ChunkMetadata fields present: chunk_id, source_file, source_hash, document_type, section_context, position_index, entity_tags, quality, word_count, token_count, created_at, processing_version. Verify datetimes as ISO 8601 strings</test>
      <test id="AC-3.4-4" priority="medium">Test pretty-printing: Verify 2-space indentation, field ordering (text first, metadata, entities, quality), no trailing commas, line length reasonable</test>
      <test id="AC-3.4-5" priority="critical">Test array queryability: jq filter by quality (.chunks[] | select(.quality.overall &gt;= 0.75)), pandas DataFrame conversion, JavaScript-style filtering</test>
      <test id="AC-3.4-6" priority="high">Test configuration header: Verify root metadata object includes processing_version, processing_timestamp, configuration (chunk_size, overlap_pct, entity_aware, quality_enrichment), source_documents, chunk_count</test>
      <test id="AC-3.4-7" priority="critical">Test JSON Schema validation: Load schema, validate output with jsonschema.validate(), test valid output passes, invalid output rejected with specific errors</test>
      <test id="integration" priority="high">Test complete pipeline: ProcessingResult → ChunkingEngine → JsonFormatter → JSON file. Verify file created, parsable, metadata accurate</test>
      <test id="performance" priority="medium">Benchmark JSON generation for 100-chunk document, measure time &lt;1 second, memory overhead during materialization</test>
      <test id="determinism" priority="high">Test same chunks → same JSON output (byte-for-byte comparison excluding timestamp). Validates NFR-P4 determinism requirement</test>
    </ideas>
  </tests>
</story-context>
