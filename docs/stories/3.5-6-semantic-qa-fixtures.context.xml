<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3.5</epicId>
    <storyId>3.5-6</storyId>
    <title>Semantic QA Fixtures</title>
    <status>drafted</status>
    <generatedAt>2025-11-18</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/3.5-6-semantic-qa-fixtures.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>QA engineer responsible for validating Epic 4 semantic analysis features</asA>
    <iWant>a semantic test corpus with ≥50 documents and gold-standard TF-IDF/LSA/entity annotations</iWant>
    <soThat>regression tests can validate semantic processor outputs against known-good baselines</soThat>
    <tasks>
      - Task 1: Source and sanitize documents (AC: 1-3)
        - Source ≥50 audit-domain documents from public datasets or generate synthetic docs
        - Review all documents for PII (names, SSNs, account numbers, addresses, etc.)
        - Sanitize PII using redaction or pseudonymization
        - Organize into 3+ document type folders (audit-reports/, risk-matrices/, compliance-docs/)
        - Verify word count totals ≥250k words

      - Task 2: Generate gold-standard annotations (AC: 4)
        - Use validated TF-IDF vectorizer (Story 3.5.4) to generate top terms for each document
        - Use validated LSA (Story 3.5.4) to assign primary topics
        - Manually annotate ≥40 documents with entity references (RISK-XXX, CTRL-XXX patterns)
        - Calculate textstat readability scores for each document
        - Create JSON annotation files matching schema (document_id, entities, tfidf_top_terms, lsa_primary_topic, readability_score)
        - Review annotations for accuracy and consistency

      - Task 3: Create comparison harness (AC: 5)
        - Implement compare-tfidf.py: Load gold-standard, run TF-IDF, compare top terms with tolerance
        - Implement compare-lsa.py: Load gold-standard, run LSA, compare topic assignments
        - Implement compare-entities.py: Load gold-standard, run entity extraction, compare precision/recall
        - Define reasonable tolerance thresholds (e.g., 80% overlap for top terms)
        - Add pytest integration for harness scripts

      - Task 4: Documentation and validation (AC: 6)
        - Create tests/fixtures/semantic/README.md with corpus statistics
        - Document annotation format and gold-standard generation process
        - Document harness usage and tolerance thresholds
        - Create metadata.json with corpus statistics (doc count, word count, type distribution)
        - QA review for completeness and usability
    </tasks>
  </story>

  <acceptanceCriteria>
    - AC-1: Corpus size: ≥50 documents total, ≥250,000 words total, representative of production audit documents
    - AC-2: Document types: Covers 3+ types (audit reports, risk matrices, compliance docs) with balanced distribution
    - AC-3: PII sanitization: All documents reviewed and sanitized (no real names, SSNs, account numbers, sensitive data)
    - AC-4: Gold-standard annotations: JSON files with expected TF-IDF top terms, LSA topics, entity annotations, readability scores for ≥40 documents
    - AC-5: Comparison harness: Scripts for regression testing (compare-tfidf.py, compare-lsa.py, compare-entities.py) with tolerance thresholds
    - AC-6: Documentation: README.md documenting corpus statistics, annotation format, harness usage, and corpus creation process
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <entry>
        <path>docs/tech-spec-epic-3.5.md</path>
        <title>Epic 3.5 Technical Specification</title>
        <section>2.1 In Scope - Semantic QA Fixtures</section>
        <snippet>50+ document test corpus (250k+ words) for semantic validation with gold-standard TF-IDF/LSA outputs</snippet>
      </entry>
      <entry>
        <path>docs/qa-fixtures-maintenance-guide.md</path>
        <title>QA Fixtures Maintenance Guide</title>
        <section>Fixture Architecture</section>
        <snippet>Comprehensive guide for maintaining QA fixtures infrastructure for Epic 4 semantic analysis testing</snippet>
      </entry>
      <entry>
        <path>docs/automation-guide.md</path>
        <title>Development Automation Guide</title>
        <section>P0 Scripts</section>
        <snippet>Story 3.5-4 smoke test semantic validates dependencies, Story 3.5-6 fixtures provide test corpus</snippet>
      </entry>
      <entry>
        <path>docs/processes/test-dependency-audit.md</path>
        <title>Test Dependency Audit Process</title>
        <section>Phase 3: Fixture Dependencies</section>
        <snippet>Process for auditing and documenting test fixture dependencies including semantic corpus requirements</snippet>
      </entry>
    </docs>
    <code>
      <entry>
        <path>tests/fixtures/semantic_corpus.py</path>
        <kind>fixture</kind>
        <symbol>get_technical_corpus, get_business_corpus</symbol>
        <lines>11-50</lines>
        <reason>Existing semantic corpus generators that can be extended for gold-standard fixtures</reason>
      </entry>
      <entry>
        <path>scripts/smoke_test_semantic.py</path>
        <kind>script</kind>
        <symbol>TestSemanticDependencies</symbol>
        <lines>all</lines>
        <reason>Validates semantic dependencies (scikit-learn, joblib, textstat) - use for gold-standard generation</reason>
      </entry>
      <entry>
        <path>tests/integration/test_fixtures/test_semantic_corpus.py</path>
        <kind>test</kind>
        <symbol>TestSemanticCorpus</symbol>
        <lines>all</lines>
        <reason>Existing semantic corpus validation tests - pattern to follow for QA fixtures</reason>
      </entry>
      <entry>
        <path>tests/integration/test_fixtures/validate_qa_fixtures.py</path>
        <kind>script</kind>
        <symbol>standalone validation runner</symbol>
        <lines>all</lines>
        <reason>Standalone validation runner for QA fixtures - use as baseline for comparison harness</reason>
      </entry>
      <entry>
        <path>tests/fixtures/greenfield/pii_scanner.py</path>
        <kind>utility</kind>
        <symbol>PIIScanner</symbol>
        <lines>all</lines>
        <reason>PII detection utility for sanitizing corpus documents</reason>
      </entry>
      <entry>
        <path>src/data_extract/semantic/__init__.py</path>
        <kind>module</kind>
        <symbol>semantic analysis placeholder</symbol>
        <lines>1-14</lines>
        <reason>Semantic module placeholder - defines Epic 4 implementation targets</reason>
      </entry>
    </code>
    <dependencies>
      <python>
        <scikit-learn>>=1.3.0,&lt;2.0</scikit-learn>
        <joblib>>=1.3.0,&lt;2.0</joblib>
        <textstat>>=0.7.3,&lt;1.0</textstat>
        <pytest>>=7.4.0</pytest>
        <spacy>>=3.6.0,&lt;4.0</spacy>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <item>Classical NLP only - no transformer models per enterprise constraint</item>
    <item>All documents must be PII-sanitized (no real names, SSNs, account numbers)</item>
    <item>Minimum 20 words per document in corpus</item>
    <item>Must follow existing test fixture patterns in tests/fixtures/</item>
    <item>Comparison harness must integrate with pytest</item>
    <item>Gold-standard annotations must use validated libraries from Story 3.5-4</item>
    <item>Corpus must reflect production audit document characteristics</item>
  </constraints>

  <interfaces>
    <item>
      <name>get_technical_corpus</name>
      <kind>function signature</kind>
      <signature>def get_technical_corpus() -> List[str]</signature>
      <path>tests/fixtures/semantic_corpus.py</path>
    </item>
    <item>
      <name>get_business_corpus</name>
      <kind>function signature</kind>
      <signature>def get_business_corpus() -> List[str]</signature>
      <path>tests/fixtures/semantic_corpus.py</path>
    </item>
    <item>
      <name>PIIScanner</name>
      <kind>class interface</kind>
      <signature>class PIIScanner: scan_text(text: str) -> List[PIIMatch]</signature>
      <path>tests/fixtures/greenfield/pii_scanner.py</path>
    </item>
  </interfaces>

  <tests>
    <standards>
      Use pytest framework for all tests. Follow existing patterns in tests/integration/test_fixtures/.
      QA fixtures validation uses 8 test cases: corpus size, document quality, PII absence, cross-format consistency,
      edge case handling, performance baselines, gold-standard availability, maintenance documentation.
      Comparison scripts should support tolerance thresholds (e.g., 80% overlap for top terms).
    </standards>
    <locations>
      <item>tests/fixtures/ - corpus and fixture definitions</item>
      <item>tests/fixtures/semantic/ - new semantic corpus location (to be created)</item>
      <item>tests/fixtures/semantic/harness/ - comparison harness scripts</item>
      <item>tests/integration/test_fixtures/ - integration tests</item>
      <item>tests/unit/test_semantic/ - unit tests for semantic features</item>
    </locations>
    <ideas>
      <item>AC-1: Test corpus reaches 50+ documents and 250k+ words total</item>
      <item>AC-2: Verify balanced distribution across 3+ document types</item>
      <item>AC-3: Run PII scanner on all documents to ensure sanitization</item>
      <item>AC-4: Validate JSON schema for gold-standard annotations</item>
      <item>AC-5: Test comparison harness scripts with known inputs/outputs</item>
      <item>AC-6: Verify README completeness and accuracy</item>
      <item>Performance: TF-IDF should process corpus in &lt;1s</item>
      <item>Regression: Compare outputs against previous baselines</item>
    </ideas>
  </tests>
</story-context>