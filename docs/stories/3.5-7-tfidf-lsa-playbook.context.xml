<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3.5</epicId>
    <storyId>3.5.7</storyId>
    <title>TF-IDF/LSA Playbook</title>
    <status>drafted</status>
    <generatedAt>2025-11-18</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/3.5-7-tfidf-lsa-playbook.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>junior developer unfamiliar with classical NLP techniques</asA>
    <iWant>a TF-IDF/LSA playbook (Jupyter notebook + reference guide) with code examples and best practices</iWant>
    <soThat>I can understand semantic analysis patterns and implement Epic 4 features in &lt;30 minutes of onboarding</soThat>
    <tasks>
      - **Task 1: Create Jupyter notebook structure (AC: 1)**
        - Create `docs/playbooks/semantic-analysis-intro.ipynb` with 8 sections
        - Section 1: Introduction - Classical NLP overview, enterprise constraints (no transformers)
        - Section 2: TF-IDF Basics - Vectorization, vocabulary, IDF weighting
        - Section 3: LSA Basics - Dimensionality reduction, topic extraction, TruncatedSVD
        - Section 4: Similarity Scoring - Cosine similarity, top-k retrieval
        - Section 5: Joblib Persistence - Saving/loading models, cache patterns
        - Section 6: Tuning &amp; Best Practices - Vocabulary size, n-grams, stopwords, stemming
        - Section 7: Performance Considerations - Batch processing, memory limits, sparse matrices
        - Section 8: Examples - Real corpus analysis with visualizations

      - **Task 2: Implement code examples (AC: 2, 4)**
        - Load semantic test corpus from Story 3.5.6
        - TF-IDF example: Fit vectorizer, transform corpus, inspect vocabulary and IDF weights
        - LSA example: Fit LSA model, project to topic space, interpret topics
        - Similarity example: Query document, compute similarities, retrieve top-k
        - Persistence example: Save models with joblib, reload, verify identical outputs
        - Add visualizations (word clouds, topic distributions, similarity heatmaps)
        - Test all cells execute without errors

      - **Task 3: Create reference guide (AC: 3)**
        - Create `docs/playbooks/semantic-analysis-reference.md`
        - Quick API reference: TFIDFVectorizer, TruncatedSVD, cosine_similarity, joblib.dump/load
        - Common pitfalls: Vocabulary drift, cache invalidation, memory issues with dense matrices
        - Troubleshooting guide: Import errors, dimension mismatches, slow performance
        - Links to scikit-learn official docs

      - **Task 4: Validation and integration (AC: 5-6)**
        - QA review: Junior dev reads playbook, provides feedback on clarity and completeness
        - Verify 30-minute understanding goal is met
        - Update CLAUDE.md to reference playbook in semantic analysis sections
        - Add playbook link to Epic 4 story template (coordinate with Story 3.5.1)
        - Ensure playbook is discoverable via `docs/playbooks/` directory
    </tasks>
  </story>

  <acceptanceCriteria>
    **AC-1: Jupyter notebook exists**
    - `docs/playbooks/semantic-analysis-intro.ipynb` with 8 sections (intro, TF-IDF, LSA, similarity, persistence, tuning, performance, examples)
    - Source: docs/tech-spec-epic-3.5.md#L253-L260

    **AC-2: Code examples execute**
    - All notebook cells run without errors, using semantic test corpus from Story 3.5.6
    - Source: docs/tech-spec-epic-3.5.md#L253-L260

    **AC-3: Companion reference guide**
    - `docs/playbooks/semantic-analysis-reference.md` with quick API reference, common pitfalls, troubleshooting
    - Source: docs/tech-spec-epic-3.5.md#L262-L266

    **AC-4: Covers key topics**
    - TF-IDF vectorization, LSA topic extraction, cosine similarity, joblib persistence, vocabulary management, performance considerations
    - Source: docs/tech-spec-epic-3.5.md#L253-L260

    **AC-5: Junior dev validation**
    - QA review confirms playbook enables understanding in &lt;30 minutes, code examples are clear
    - Source: docs/tech-spec-epic-3.5.md#L252

    **AC-6: Documentation links**
    - CLAUDE.md and Epic 4 story templates link to playbook for semantic development reference
    - Source: docs/tech-spec-epic-3.5.md#L252
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <!-- Primary Technical Specifications -->
      <doc>
        <path>docs/tech-spec-epic-3.5.md</path>
        <title>Epic 3.5 Technical Specification · Tooling &amp; Semantic Prep (Bridge Epic)</title>
        <section>3.6 TF-IDF/LSA Playbook Structure</section>
        <snippet>Jupyter Notebook structure with 8 cells covering classical NLP patterns: Introduction, TF-IDF Basics, LSA Basics, Similarity Scoring, Joblib Persistence, Tuning &amp; Best Practices, Performance Considerations, Examples with visualizations. Companion markdown provides quick API reference, common pitfalls (vocabulary drift, cache invalidation), troubleshooting guide.</snippet>
      </doc>

      <doc>
        <path>docs/retrospectives/epic-3-retro-2025-11-16.md</path>
        <title>Epic 3 Retrospective · Intelligent Chunking &amp; Output Formats (2025-11-16)</title>
        <section>10. Preparation Sprint Tasks</section>
        <snippet>TF-IDF/LSA playbook / starter notebook for juniors – 4h, Charlie + Elena. Covers vocabulary mgmt, joblib persistence. Addresses junior dev context gap for classical NLP patterns to avoid vocabulary drift, cache issues.</snippet>
      </doc>

      <!-- Related Stories (Dependencies) -->
      <doc>
        <path>docs/stories/3.5-4-semantic-dependencies-smoke-test.md</path>
        <title>Story 3.5.4: Semantic Dependencies + Smoke Test</title>
        <section>Acceptance Criteria</section>
        <snippet>Dependencies installed: pyproject.toml includes scikit-learn ≥1.3.0, joblib ≥1.3.0, textstat ≥0.7.3. Smoke test validates TF-IDF vectorization, LSA dimensionality reduction, textstat metrics with &lt;100ms performance baseline on 1k-word document.</snippet>
      </doc>

      <doc>
        <path>docs/stories/3.5-5-model-cache-adr.md</path>
        <title>Story 3.5.5: Model/Cache ADR</title>
        <section>Story Body</section>
        <snippet>ADR-012 documents semantic model cache strategy: joblib persistence, hash-based cache keys (corpus SHA-256 + model version), storage paths (.data-extract-cache/models/), 500 MB size limit with LRU eviction, CLI --clear-cache integration. 10-100x speedup on repeated analysis.</snippet>
      </doc>

      <doc>
        <path>docs/stories/3.5-6-semantic-qa-fixtures.md</path>
        <title>Story 3.5.6: Semantic QA Fixtures</title>
        <section>Acceptance Criteria</section>
        <snippet>Semantic test corpus in tests/fixtures/semantic/ with ≥50 documents (≥250k words) covering audit reports, risk matrices, compliance docs. Gold-standard annotations with expected TF-IDF top terms, LSA topics, entity references, readability scores. Comparison harness for regression testing.</snippet>
      </doc>

      <!-- Architecture Documentation -->
      <doc>
        <path>docs/architecture/architecture-decision-records-adrs.md</path>
        <title>Architecture Decision Records (ADRs)</title>
        <section>ADR-004: Classical NLP Only (No Transformers)</section>
        <snippet>Enterprise IT policy prohibits transformer-based models. Decision: Use classical NLP (TF-IDF, LSA, Word2Vec) via scikit-learn and gensim. Complies with enterprise restrictions, faster inference, lower memory, interpretable results. Trade-off: Less semantic understanding than transformers (acceptable for audit domain).</snippet>
      </doc>

      <!-- Product Requirements -->
      <doc>
        <path>docs/PRD/functional-requirements.md</path>
        <title>Functional Requirements</title>
        <section>FR-5: Foundational Semantic Analysis</section>
        <snippet>Classical NLP techniques for semantic understanding: TF-IDF vectorization for keyword importance, LSA/topic modeling for thematic analysis, similarity scoring for duplicate detection. Enterprise constraint: No transformer models allowed.</snippet>
      </doc>

      <doc>
        <path>docs/PRD/non-functional-requirements.md</path>
        <title>Non-Functional Requirements</title>
        <section>Usability</section>
        <snippet>NFR-U2: Playbook readability - Junior dev understands TF-IDF in &lt;30 min. Clear documentation and examples for classical NLP patterns to enable rapid onboarding.</snippet>
      </doc>

      <!-- Project Documentation -->
      <doc>
        <path>.claude/CLAUDE.md</path>
        <title>Claude.ai Code Project Instructions</title>
        <section>Technology Stack</section>
        <snippet>Semantic analysis: scikit-learn (TF-IDF, LSA, cosine similarity), gensim (Word2Vec, LDA optional), textstat (readability metrics). Enterprise Constraint: Classical NLP only - no transformer models allowed. Documentation: ADRs are code - require owners and deadlines.</snippet>
      </doc>

      <!-- Automation and Guidance Documentation -->
      <doc>
        <path>docs/automation-guide.md</path>
        <title>Development Automation Guide</title>
        <section>P0 Scripts</section>
        <snippet>P0 automation scripts provide 60% token reduction and 75% faster development. Includes generate_story_template.py, run_quality_gates.py, and init_claude_session.py for streamlined workflow automation.</snippet>
      </doc>

      <doc>
        <path>docs/retrospective-lessons.md</path>
        <title>Comprehensive Retrospective Lessons</title>
        <section>Testing and Quality</section>
        <snippet>Lessons from Epics 1-3 including quality gate enforcement (Black/Ruff/Mypy), test structure mirroring src/, and importance of integration tests for catching memory/NFR issues.</snippet>
      </doc>

      <doc>
        <path>docs/epic-3-reference.md</path>
        <title>Epic 3 Complete Implementation Reference</title>
        <section>Semantic-aware chunking</section>
        <snippet>Implementation patterns for semantic-aware chunking with entity preservation, performance baselines (~0.19s per 1,000 words, 255 MB peak memory), and output organization strategies.</snippet>
      </doc>
    </docs>

    <code>
      <!-- Dependency Configuration -->
      <artifact>
        <path>pyproject.toml</path>
        <kind>config</kind>
        <symbol>dependencies</symbol>
        <lines>36-60</lines>
        <reason>Semantic libraries (scikit-learn, joblib, textstat) to be installed in Story 3.5.4. Playbook examples will demonstrate usage of these dependencies. Also includes dev dependencies (pytest, pandas, black, ruff, mypy) relevant for playbook development.</reason>
      </artifact>

      <!-- Testing Infrastructure Patterns -->
      <artifact>
        <path>tests/conftest.py</path>
        <kind>test-fixture</kind>
        <symbol>sample_content_block, sample_table_block</symbol>
        <lines>1-100</lines>
        <reason>Pytest fixture patterns that playbook can reference for creating test data structures. Demonstrates how to structure reusable test fixtures similar to corpus loading patterns needed in playbook examples.</reason>
      </artifact>

      <!-- Script Patterns -->
      <artifact>
        <path>scripts/generate_large_pdf_fixture.py</path>
        <kind>script</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>Example of fixture generation script pattern. Playbook corpus loading and gold-standard generation should follow similar structure with clear documentation and reusable functions.</reason>
      </artifact>

      <!-- Test Directory Structure -->
      <artifact>
        <path>tests/fixtures/</path>
        <kind>directory</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>Existing fixture directory structure. Semantic corpus (Story 3.5.6) will be located at tests/fixtures/semantic/. Playbook examples will load corpus from this location.</reason>
      </artifact>

      <!-- Performance Testing Patterns -->
      <artifact>
        <path>tests/performance/test_baseline_capture.py</path>
        <kind>test</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>Performance testing pattern for establishing baselines. Playbook should reference similar approach for TF-IDF/LSA performance considerations (Section 7).</reason>
      </artifact>

      <!-- Semantic Smoke Test Implementation -->
      <artifact>
        <path>scripts/smoke_test_semantic.py</path>
        <kind>script</kind>
        <symbol>test_tfidf_performance, test_lsa_functionality, test_textstat_metrics</symbol>
        <lines>1-200</lines>
        <reason>Working implementation of TF-IDF and LSA tests from Story 3.5.4. Demonstrates performance baselines (&lt;100ms TF-IDF, &lt;200ms LSA), proper usage patterns, and can serve as code examples for the playbook.</reason>
      </artifact>

      <!-- Semantic Test Corpus -->
      <artifact>
        <path>tests/fixtures/semantic_corpus.py</path>
        <kind>test-fixture</kind>
        <symbol>SEMANTIC_TEST_CORPUS</symbol>
        <lines>1-500</lines>
        <reason>Test corpus from Story 3.5.6 containing audit documents for realistic examples. Playbook will use this corpus for demonstrating TF-IDF vectorization, LSA topic extraction, and similarity calculations.</reason>
      </artifact>

      <!-- P0 Automation Scripts -->
      <artifact>
        <path>scripts/generate_story_template.py</path>
        <kind>script</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>P0-1 script that demonstrates documentation generation patterns. Playbook creation could follow similar template-based approach for consistency.</reason>
      </artifact>

      <artifact>
        <path>scripts/run_quality_gates.py</path>
        <kind>script</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>P0-2 quality gate runner that enforces Black/Ruff/Mypy standards. All playbook code examples should pass these quality gates.</reason>
      </artifact>

      <artifact>
        <path>scripts/init_claude_session.py</path>
        <kind>script</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>P0-3 session initializer that sets up development environment. Junior developers using the playbook should run this first to ensure proper environment setup.</reason>
      </artifact>
    </code>

    <dependencies>
      <!-- Python Dependencies (from pyproject.toml) -->
      <python>
        <!-- Semantic Analysis Libraries (Story 3.5.4) -->
        <package>scikit-learn&gt;=1.3.0,&lt;2.0</package><!-- TF-IDF, LSA, cosine similarity -->
        <package>joblib&gt;=1.3.0</package><!-- Model persistence -->
        <package>textstat&gt;=0.7.3</package><!-- Readability metrics -->

        <!-- Jupyter Notebook Dependencies -->
        <package>jupyter</package><!-- Notebook environment (to be added) -->
        <package>matplotlib</package><!-- Visualizations (to be added) -->
        <package>seaborn</package><!-- Enhanced visualizations (to be added) -->
        <package>wordcloud</package><!-- Word cloud visualizations (to be added) -->

        <!-- Data Manipulation -->
        <package>pandas&gt;=2.0.0,&lt;3.0</package><!-- Already installed for CSV validation -->

        <!-- Core Dependencies (already installed) -->
        <package>pydantic&gt;=2.0.0,&lt;3.0</package><!-- Data models -->
        <package>spacy&gt;=3.7.2,&lt;4.0</package><!-- NLP infrastructure -->

        <!-- Development Dependencies (already installed) -->
        <package>pytest&gt;=8.0.0,&lt;9.0</package><!-- Testing -->
        <package>black&gt;=24.0.0,&lt;25.0</package><!-- Code formatting -->
        <package>ruff&gt;=0.6.0,&lt;0.7</package><!-- Linting -->
        <package>mypy&gt;=1.11.0,&lt;2.0</package><!-- Type checking -->
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    **Enterprise Constraints:**
    - Classical NLP only - NO transformer models allowed (ADR-004)
    - Must comply with enterprise IT policy prohibiting deep learning models
    - On-premise processing required (no cloud-based NLP APIs)

    **Development Constraints:**
    - All code must pass Black/Ruff/Mypy quality gates (0 violations)
    - Playbook must be version-controlled and reproducible
    - Examples must use semantic test corpus from Story 3.5.6 (PII-free)
    - Follow existing docs/playbooks/ directory structure

    **Educational Constraints:**
    - Junior dev must understand TF-IDF patterns in &lt;30 minutes (NFR-U2)
    - Code examples must execute without errors on first run
    - Documentation must be clear, concise, and avoid jargon
    - Include troubleshooting guidance for common issues

    **Technical Constraints:**
    - TF-IDF baseline: &lt;100ms fit/transform on 1k-word document (Story 3.5.4)
    - LSA baseline: &lt;200ms fit/transform on 1k-word document
    - Notebook must be compatible with Jupyter Lab and VS Code Jupyter extension
    - Examples must demonstrate memory-efficient sparse matrix handling

    **Integration Constraints:**
    - Coordinate with Story 3.5.1 (story template generator) for Epic 4 template links
    - Reference Story 3.5.5 (Model/Cache ADR) for joblib persistence patterns
    - Use Story 3.5.6 (semantic QA fixtures) corpus for realistic examples
    - Update CLAUDE.md to reference playbook for semantic development guidance

    **Documentation Standards:**
    - Follow Google-style docstrings for code examples
    - Include inline comments explaining NLP concepts
    - Provide links to scikit-learn official documentation
    - Document expected outputs and common pitfalls
    - Use markdown formatting for readability
  </constraints>

  <interfaces>
    **scikit-learn TF-IDF APIs:**
    - `TfidfVectorizer()` - Initialize vectorizer with parameters (max_features, ngram_range, stop_words)
    - `.fit(corpus)` - Learn vocabulary and IDF weights from corpus
    - `.transform(documents)` - Transform documents to TF-IDF sparse matrix
    - `.fit_transform(corpus)` - Combined fit and transform
    - `.get_feature_names_out()` - Retrieve vocabulary terms
    - `.vocabulary_` - Dictionary mapping terms to feature indices
    - `.idf_` - IDF weights array

    **scikit-learn LSA APIs:**
    - `TruncatedSVD(n_components)` - Initialize LSA model with number of topics
    - `.fit(tfidf_matrix)` - Learn topic representations from TF-IDF matrix
    - `.transform(tfidf_matrix)` - Project documents to topic space
    - `.fit_transform(tfidf_matrix)` - Combined fit and transform
    - `.components_` - Topic-term matrix for interpretation
    - `.explained_variance_ratio_` - Variance explained by each topic

    **scikit-learn Similarity APIs:**
    - `cosine_similarity(X, Y)` - Compute pairwise cosine similarity between vectors
    - Returns similarity matrix (range: -1 to 1, typically 0 to 1 for TF-IDF)
    - Used for document similarity, duplicate detection, top-k retrieval

    **joblib Persistence APIs:**
    - `joblib.dump(model, filepath)` - Save model to disk (uses pickle protocol)
    - `joblib.load(filepath)` - Load model from disk
    - Supports compression (compress=3 recommended for large models)
    - Cache key pattern: `{model_type}_v{version}_{corpus_hash}.joblib`

    **textstat Readability APIs:**
    - `textstat.flesch_reading_ease(text)` - Readability score (0-100, higher = easier)
    - `textstat.flesch_kincaid_grade(text)` - U.S. grade level
    - `textstat.gunning_fog(text)` - Years of education needed
    - Used for quality assessment and corpus characterization

    **Expected Notebook Cell Outputs:**
    - TF-IDF vocabulary inspection (top 20 terms with IDF weights)
    - LSA topic interpretation (top 10 terms per topic)
    - Similarity matrix visualization (heatmap of document similarities)
    - Cache performance comparison (fit time: uncached vs. cached)
    - Word cloud visualization of corpus top terms
  </interfaces>

  <tests>
    <standards>
      **Testing Framework:**
      - pytest for all Python tests
      - Tests mirror src/ structure exactly
      - Markers for selective execution (unit, integration, performance, slow)
      - Coverage requirements: greenfield ≥80%, critical paths ≥90%

      **Quality Gates (Pre-commit):**
      - Black code formatting (100 char lines, Python 3.12)
      - Ruff linting (replaces flake8 + isort)
      - Mypy strict type checking (excludes brownfield during migration)
      - All tests must pass before commit

      **Jupyter Notebook Testing:**
      - All cells must execute sequentially without errors
      - Cell outputs should be reproducible (deterministic results)
      - Manual QA review for clarity and educational value
      - Junior dev validation: 30-minute understanding goal (NFR-U2)

      **Documentation Standards:**
      - Google-style docstrings for public APIs
      - Inline comments for complex NLP concepts
      - Example outputs shown in notebook cells
      - Links to scikit-learn official documentation
    </standards>

    <locations>
      **Test Directories:**
      - `tests/unit/` - Fast, isolated unit tests
      - `tests/integration/` - Multi-component end-to-end tests
      - `tests/performance/` - Benchmarks and stress tests
      - `tests/fixtures/` - Shared test data (includes semantic corpus)
      - `tests/fixtures/semantic/` - Semantic QA fixtures (Story 3.5.6)

      **Playbook Location:**
      - `docs/playbooks/` - NEW directory for educational resources
      - `docs/playbooks/semantic-analysis-intro.ipynb` - Jupyter notebook
      - `docs/playbooks/semantic-analysis-reference.md` - Quick reference guide

      **Related Test Files:**
      - `tests/performance/test_baseline_capture.py` - Performance baseline patterns
      - `tests/conftest.py` - Shared pytest fixtures
      - `tests/fixtures/semantic/harness/` - Comparison scripts (Story 3.5.6)
    </locations>

    <ideas>
      **Test Ideas for Playbook Validation (mapped to ACs):**

      **AC-1 (Notebook Structure):**
      - Manual verification: All 8 sections present (intro, TF-IDF, LSA, similarity, persistence, tuning, performance, examples)
      - Manual verification: Sections follow logical progression for learning
      - Manual verification: Each section has clear headings and explanations

      **AC-2 (Code Examples Execute):**
      - Automated: Execute all notebook cells sequentially using nbconvert or papermill
      - Automated: Verify no exceptions raised during cell execution
      - Automated: Verify semantic corpus loaded successfully from tests/fixtures/semantic/
      - Manual: Review cell outputs for correctness and clarity

      **AC-3 (Reference Guide):**
      - Manual verification: `semantic-analysis-reference.md` exists
      - Manual verification: Contains API quick reference section
      - Manual verification: Contains common pitfalls section
      - Manual verification: Contains troubleshooting section
      - Manual verification: Links to scikit-learn docs present

      **AC-4 (Key Topics Coverage):**
      - Manual verification: TF-IDF vectorization explained with code example
      - Manual verification: LSA topic extraction demonstrated
      - Manual verification: Cosine similarity calculation shown
      - Manual verification: Joblib persistence pattern documented
      - Manual verification: Vocabulary management discussed (drift, stopwords, stemming)
      - Manual verification: Performance considerations addressed (sparse matrices, memory)

      **AC-5 (Junior Dev Validation):**
      - Manual test: QA team member (junior dev persona) reads playbook start-to-finish
      - Manual test: Measure time to complete reading and understanding (target: &lt;30 min)
      - Manual test: Collect feedback on clarity, completeness, helpful examples
      - Manual test: Ask comprehension questions about TF-IDF, LSA, caching patterns
      - Manual test: Have junior dev modify an example to verify understanding

      **AC-6 (Documentation Links):**
      - Automated: Verify CLAUDE.md contains reference to playbook (grep test)
      - Manual verification: Epic 4 story template links to playbook (coordinate with Story 3.5.1)
      - Manual verification: Playbook discoverable via docs/playbooks/ directory
      - Manual verification: README.md or index.md links to playbook

      **Additional Quality Checks:**
      - Performance: TF-IDF example completes in &lt;100ms (baseline from Story 3.5.4)
      - Performance: LSA example completes in &lt;200ms
      - Memory: Examples use sparse matrices (verify via notebook cell outputs)
      - Reproducibility: Re-running cells produces identical outputs (determinism)
      - Visualization: Word clouds, heatmaps render correctly in Jupyter
      - Error handling: Examples include try-except for common errors (import failures, missing data)
    </ideas>
  </tests>
</story-context>
