<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3.5</epicId>
    <storyId>3</storyId>
    <title>Test-Dependency Audit Documentation</title>
    <status>todo</status>
    <generatedAt>2025-11-17</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/3.5-3-test-dependency-audit-doc.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>developer adding new dependencies to data-extraction-tool</asA>
    <iWant>documented process for auditing test dependencies and their impacts</iWant>
    <soThat>I can validate new dependencies don't break existing tests or introduce performance regressions</soThat>
    <tasks>
      ### Task 1: Create Dependency Audit Process Documentation (AC: #3.5.3-1)
      - Create `docs/dependency-audit-process.md`
      - Write introduction explaining purpose and when to use audit process
      - Document Pre-Audit step (pip freeze, test baseline, coverage baseline)
      - Document Install Dependency step (pyproject.toml, venv, lock file)
      - Document Security Audit step (pip-audit)
      - Document Test Execution step (unit/integration/performance)
      - Document Coverage Check step (coverage comparison)
      - Document Performance Comparison step (pytest-benchmark)
      - Document CI/CD Validation step (feature branch push, verify CI)
      - Document Documentation Update step (CLAUDE.md, tech specs)

      ### Task 2: Document Test Impact Categories (AC: #3.5.3-2)
      - Add "## Test Impact Categories" section
      - Document Unit Tests impact (imports, API, mocking)
      - Document Integration Tests impact (components, data, pipeline)
      - Document Performance Tests impact (import time, memory, execution)
      - Document Fixtures impact (data compatibility, serialization)
      - Add cross-references to Epic 3 dependency stories

      ### Task 3: Document Performance Baseline Process (AC: #3.5.3-3)
      - Add "## Performance Baseline Process" section
      - Document baseline creation (pytest-benchmark before dependency)
      - Document baseline storage (docs/performance-baselines-epic-*.md)
      - Document comparison method (pytest-benchmark after dependency)
      - Document acceptable regression thresholds
      - Document regression handling process

      ### Task 4: Document Troubleshooting Guide (AC: #3.5.3-4)
      - Add "## Troubleshooting Common Issues" section
      - Document version conflict resolution
      - Document import error debugging
      - Document test failure debugging
      - Document performance regression debugging
      - Document CI/CD failure debugging
      - Add examples from Epic 2.5-3 actual issues

      ### Task 5: Document CI/CD Integration (AC: #3.5.3-5)
      - Add "## CI/CD Integration" section
      - Document pre-commit hooks
      - Document CI pipeline stages
      - Document caching strategy
      - Document failure handling process
      - Document security scanning (pip-audit in CI)

      ### Task 6: Add Examples from Epic 3 Dependencies
      - Extract spaCy audit example from Story 2.5.2
      - Extract pandas audit example from Story 3.6
      - Extract textstat audit example from Story 3.3
      - Extract jq audit example from Story 3.4

      ### Task 7: Quality Gates and UAT
      - Manual review of dependency-audit-process.md for completeness
      - Spell check and grammar review
      - Verify all cross-references are valid
      - UAT: Test process by auditing scikit-learn for Epic 4
      - UAT: Team reviews troubleshooting guide
      - Update CLAUDE.md to reference dependency-audit-process.md
    </tasks>
  </story>

  <acceptanceCriteria>
    **AC-3.5.3-1: Dependency Audit Process Documented (P0 - Critical)**
    - Documentation file created at `docs/dependency-audit-process.md`
    - Process includes 8 step-by-step instructions with example commands
    - Each step includes example commands and expected outputs
    - Validation: Manual review by team for completeness and clarity
    - UAT Required: Yes - Test process by auditing scikit-learn for Epic 4

    **AC-3.5.3-2: Test Impact Categories Documented (P0)**
    - Document how dependencies affect unit, integration, performance, and fixture tests
    - Include examples from Epic 3 dependencies (spaCy, pandas, textstat, jq)
    - Validation: Cross-reference with actual dependency additions from Epic 2.5-3
    - UAT Required: No - Examples verify accuracy

    **AC-3.5.3-3: Performance Baseline Process Documented (P1)**
    - Document baseline creation, storage, comparison, thresholds, and regression handling
    - Include example from Story 2.5.2.1 (spaCy performance impact analysis)
    - Validation: Compare with existing performance baseline docs
    - UAT Required: No - Process validation via Epic 4 prep

    **AC-3.5.3-4: Troubleshooting Guide Documented (P1)**
    - Document common issues: version conflicts, imports, tests, performance, CI/CD
    - Include examples from Epic 2.5-3 (spaCy caching, jq binary, pandas)
    - Validation: Review against actual issues encountered in Epic 2.5-3
    - UAT Required: Yes - Team validates guide covers common cases

    **AC-3.5.3-5: CI/CD Integration Documented (P0)**
    - Document pre-commit hooks, CI stages, caching, failures, security scanning
    - Include references to .github/workflows/ and .pre-commit-config.yaml
    - Validation: Cross-reference with actual CI/CD configuration files
    - UAT Required: No - CI/CD configuration validates accuracy
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <!-- Epic 3 Retrospective - Source of action item -->
      <doc>
        <path>docs/retrospectives/epic-3-retro-2025-11-16.md</path>
        <title>Epic 3 Retrospective</title>
        <section>4. Challenges &amp; Root Causes</section>
        <snippet>Action item from Epic 2 deprioritized; no owner. This story directly addresses "Dependency audit process still undocumented" finding with Winston as owner.</snippet>
      </doc>

      <!-- Epic 3.5 Bridge Epic Summary - Story context -->
      <doc>
        <path>docs/epic-3.5-bridge-epic-summary.md</path>
        <title>Epic 3.5: Bridge Epic - Tooling &amp; Semantic Prep</title>
        <section>Story 3.5.3: Test-Dependency Audit Documentation</section>
        <snippet>Document comprehensive process for auditing test dependencies and their impacts (performance, compatibility, security). Deliverables include step-by-step audit process, test impact categories, performance baseline process, troubleshooting guide, and CI/CD integration.</snippet>
      </doc>

      <!-- Performance Baselines - Dependency impact examples -->
      <doc>
        <path>docs/performance-baselines-epic-3.md</path>
        <title>Performance Baselines - Epic 3</title>
        <section>Story 3.4 JSON Output Performance</section>
        <snippet>JSON formatter performance with pandas validation: 100-chunk benchmark 0.10s, 1000-chunk 0.80s. Demonstrates dependency impact measurement and baseline establishment patterns.</snippet>
      </doc>

      <!-- CI/CD Pipeline Documentation -->
      <doc>
        <path>docs/ci-cd-pipeline.md</path>
        <title>CI/CD Pipeline Documentation</title>
        <section>Workflows</section>
        <snippet>GitHub Actions workflows with quality gates, caching (pip + spaCy), fail-fast testing. Demonstrates CI/CD integration patterns for dependency validation.</snippet>
      </doc>

      <!-- spaCy Troubleshooting - Dependency troubleshooting example -->
      <doc>
        <path>docs/troubleshooting-spacy.md</path>
        <title>spaCy Troubleshooting Guide</title>
        <section>Common Issues</section>
        <snippet>Comprehensive troubleshooting for spaCy dependency: model not found, version compatibility, performance issues, installation failures, network/proxy issues. Excellent pattern for dependency troubleshooting documentation.</snippet>
      </doc>

      <!-- Story 2.5.2 - spaCy integration example -->
      <doc>
        <path>docs/stories/2.5-2-spacy-integration-and-end-to-end-testing.md</path>
        <title>Story 2.5.2: spaCy Integration &amp; Validation</title>
        <section>Acceptance Criteria</section>
        <snippet>spaCy 3.7.2 installation, model download, accuracy validation (95%+), performance validation (&lt;5s model load, 4000+ words/sec). Demonstrates complete dependency audit process.</snippet>
      </doc>

      <!-- CLAUDE.md - Project instructions with dependency guidance -->
      <doc>
        <path>.claude/CLAUDE.md</path>
        <title>CLAUDE.md</title>
        <section>spaCy Model Setup</section>
        <snippet>spaCy setup instructions, model download, verification, performance benchmarks, CI/CD caching, troubleshooting reference. Pattern for documenting dependency setup in project instructions.</snippet>
      </doc>
    </docs>

    <code>
      <!-- pyproject.toml - Dependency manifest -->
      <artifact>
        <path>pyproject.toml</path>
        <kind>config</kind>
        <symbol>dependencies</symbol>
        <lines>36-60</lines>
        <reason>Main dependency manifest showing Epic 2-3 dependencies (spacy, jsonschema, beautifulsoup4, pandas, csvkit). Reference for dependency version constraints and organization.</reason>
      </artifact>

      <!-- Pre-commit configuration -->
      <artifact>
        <path>.pre-commit-config.yaml</path>
        <kind>config</kind>
        <symbol>repos</symbol>
        <lines>1-43</lines>
        <reason>Pre-commit hooks configuration (black, ruff, mypy) showing automated quality gates. Example of CI/CD integration for dependency validation.</reason>
      </artifact>

      <!-- GitHub Actions test workflow -->
      <artifact>
        <path>.github/workflows/test.yml</path>
        <kind>config</kind>
        <symbol>jobs</symbol>
        <lines>17-262</lines>
        <reason>CI/CD pipeline with dependency caching (pip, spaCy), test execution, coverage validation. Demonstrates automated dependency audit in CI.</reason>
      </artifact>

      <!-- Greenfield modules using dependencies -->
      <artifact>
        <path>src/data_extract/utils/nlp.py</path>
        <kind>module</kind>
        <symbol>get_sentence_boundaries</symbol>
        <lines>all</lines>
        <reason>spaCy dependency usage example with lazy loading, error handling, performance optimization. Pattern for dependency integration in production code.</reason>
      </artifact>

      <artifact>
        <path>src/data_extract/chunk/engine.py</path>
        <kind>module</kind>
        <symbol>ChunkingEngine</symbol>
        <lines>all</lines>
        <reason>Uses spaCy-based sentence segmentation, demonstrates dependency impact on core pipeline. Example of testing dependency integration.</reason>
      </artifact>

      <artifact>
        <path>src/data_extract/output/formatters/json_formatter.py</path>
        <kind>module</kind>
        <symbol>JsonFormatter</symbol>
        <lines>all</lines>
        <reason>Uses jsonschema dependency for validation, demonstrates optional validation toggle. Pattern for managing dependency performance impact.</reason>
      </artifact>

      <artifact>
        <path>src/data_extract/output/formatters/csv_formatter.py</path>
        <kind>module</kind>
        <symbol>CsvFormatter</symbol>
        <lines>all</lines>
        <reason>CSV formatting without pandas dependency in production code. Shows dependency selection trade-offs (pandas only for testing/validation).</reason>
      </artifact>
    </code>

    <dependencies>
      <python>
        <!-- Production dependencies from pyproject.toml -->
        <package name="pydantic" version=">=2.0.0,<3.0" epic="1" />
        <package name="PyYAML" version=">=6.0.0,<7.0" epic="1" />
        <package name="structlog" version=">=24.0.0,<25.0" epic="1" />
        <package name="spacy" version=">=3.7.2,<4.0" epic="2.5" story="2.5.2" />
        <package name="jsonschema" version=">=4.0.0,<5.0" epic="3" story="3.4" />
        <package name="beautifulsoup4" version=">=4.12.0,<5.0" epic="2" />
        <package name="lxml" version=">=5.0.0,<6.0" epic="2" />

        <!-- Development/testing dependencies -->
        <package name="pytest" version=">=8.0.0,<9.0" scope="dev" />
        <package name="pytest-cov" version=">=5.0.0,<6.0" scope="dev" />
        <package name="pytest-benchmark" version="implicit" scope="dev" note="Not in pyproject.toml yet, needed for AC-3.5.3-3" />
        <package name="pip-audit" version="implicit" scope="dev" note="Not in pyproject.toml yet, needed for AC-3.5.3-1" />
        <package name="black" version=">=24.0.0,<25.0" scope="dev" />
        <package name="ruff" version=">=0.6.0,<0.7" scope="dev" />
        <package name="mypy" version=">=1.11.0,<2.0" scope="dev" />
        <package name="psutil" version=">=5.9.0,<6.0" scope="dev" story="2.5.1" />
        <package name="pandas" version=">=2.0.0,<3.0" scope="dev" story="3.6" />
        <package name="csvkit" version=">=2.0.0,<3.0" scope="dev" story="3.6" />

        <!-- Upcoming Epic 4 dependencies (mentioned in Epic 3.5 context) -->
        <package name="scikit-learn" version="TBD" epic="4" story="3.5.4" note="To be added and audited" />
        <package name="joblib" version="TBD" epic="4" story="3.5.4" note="To be added and audited" />
        <package name="gensim" version="TBD" epic="4" story="3.5.4" note="To be added and audited" />
      </python>

      <external>
        <!-- External binary dependencies -->
        <tool name="jq" platform="linux,macos,windows" story="3.4" note="JSON parsing in tests, platform-specific installation" />
        <tool name="tesseract" platform="all" optional="true" note="OCR support, system dependency" />
        <tool name="spacy-model:en_core_web_md" version="3.8.0" size="43MB" story="2.5.2" note="Language model, requires download step" />
      </external>
    </dependencies>
  </artifacts>

  <constraints>
    <!-- Development constraints from Dev Notes and architecture -->
    <constraint type="process">Story 3.5.3 is P0 and must be completed before Epic 4 (semantic dependencies) to establish audit process</constraint>
    <constraint type="process">UAT validation required: Test process by auditing scikit-learn for Epic 4 (AC-3.5.3-1)</constraint>
    <constraint type="process">UAT validation required: Team reviews troubleshooting guide for common cases coverage (AC-3.5.3-4)</constraint>
    <constraint type="documentation">Document structure must include 8-step audit process, test impact categories, performance baselines, troubleshooting, CI/CD integration</constraint>
    <constraint type="documentation">All examples must reference actual Epic 2.5-3 dependency work (spaCy, pandas, textstat, jq)</constraint>
    <constraint type="documentation">Performance regression thresholds: Critical path &lt;5%, Non-critical &lt;10%</constraint>
    <constraint type="documentation">Commands must be tested locally for accuracy before documenting</constraint>
    <constraint type="quality">Manual review required: Markdown formatting, link validity, spelling/grammar</constraint>
    <constraint type="testing">No code quality gates (markdown documentation only)</constraint>
    <constraint type="testing">Cross-reference examples with actual Epic 2.5-3 dependency stories</constraint>
    <constraint type="provenance">Document should include "Last Updated: YYYY-MM-DD" timestamp and reference Epic 3.5, Story 3.5.3</constraint>
  </constraints>

  <interfaces>
    <!-- Key interfaces this story documentation will reference -->
    <interface>
      <name>pip-audit CLI</name>
      <kind>command-line</kind>
      <signature>pip-audit --desc</signature>
      <path>External tool (pip-audit package)</path>
      <description>Security vulnerability scanning for dependencies. Used in AC-3.5.3-1 Security Audit step.</description>
    </interface>

    <interface>
      <name>pytest-benchmark CLI</name>
      <kind>command-line</kind>
      <signature>pytest --benchmark-only --benchmark-save=NAME --benchmark-compare=NAME</signature>
      <path>External tool (pytest-benchmark package)</path>
      <description>Performance baseline creation and comparison. Used in AC-3.5.3-3 Performance Baseline Process.</description>
    </interface>

    <interface>
      <name>pytest with markers</name>
      <kind>command-line</kind>
      <signature>pytest -m unit|integration|performance -v</signature>
      <path>tests/ directory</path>
      <description>Test execution with category markers. Used in AC-3.5.3-1 Test Execution step and AC-3.5.3-2 Test Impact Categories.</description>
    </interface>

    <interface>
      <name>pre-commit</name>
      <kind>command-line</kind>
      <signature>pre-commit run --all-files</signature>
      <path>.pre-commit-config.yaml</path>
      <description>Automated quality checks (black, ruff, mypy). Referenced in AC-3.5.3-5 CI/CD Integration.</description>
    </interface>

    <interface>
      <name>GitHub Actions workflows</name>
      <kind>CI/CD</kind>
      <signature>Multiple jobs: test, lint, type-check, format-check</signature>
      <path>.github/workflows/test.yml</path>
      <description>CI/CD pipeline with dependency caching, test execution, coverage validation. Referenced in AC-3.5.3-5.</description>
    </interface>

    <interface>
      <name>pyproject.toml dependencies</name>
      <kind>configuration</kind>
      <signature>[project.dependencies] and [project.optional-dependencies]</signature>
      <path>pyproject.toml</path>
      <description>Dependency manifest for pip installation. Referenced in AC-3.5.3-1 Install Dependency step.</description>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Testing standards for this story: No automated tests required (documentation-only story). Manual validation includes: (1) Markdown formatting correctness, (2) Link validity for file paths and story references, (3) Command example accuracy (test commands locally before documenting), (4) Spelling and grammar review. UAT testing: (1) Test audit process by auditing scikit-learn for Epic 4 (validates AC-3.5.3-1 completeness), (2) Team reviews troubleshooting guide for common cases coverage (validates AC-3.5.3-4). Quality gates: Manual review checklist completion, UAT sign-off from team.
    </standards>

    <locations>
      No test files for this story (documentation only). Referenced test locations in documentation:
      - tests/unit/ - Unit tests (fast, isolated)
      - tests/integration/ - Integration tests (multi-component)
      - tests/performance/ - Performance benchmarks
      - tests/fixtures/ - Shared test data and fixtures
    </locations>

    <ideas>
      <!-- UAT test ideas for validating the documentation -->
      <test-idea ac="AC-3.5.3-1">
        <description>UAT: Follow documented audit process to add scikit-learn for Epic 4</description>
        <approach>Execute all 8 documented steps (Pre-Audit → Install → Security → Tests → Coverage → Performance → CI/CD → Documentation) and note any missing instructions or unclear commands</approach>
        <success-criteria>Successfully audit scikit-learn using only the documented process without needing additional guidance</success-criteria>
      </test-idea>

      <test-idea ac="AC-3.5.3-2">
        <description>Validate test impact category examples against actual Epic 3 stories</description>
        <approach>Cross-reference spaCy, pandas, textstat, jq examples with Stories 2.5.2, 3.4, 3.3, 3.6 to ensure accuracy</approach>
        <success-criteria>All examples match actual dependency impact documented in respective stories</success-criteria>
      </test-idea>

      <test-idea ac="AC-3.5.3-3">
        <description>Validate performance baseline process against existing baselines</description>
        <approach>Compare documented baseline creation/comparison process with docs/performance-baselines-epic-3.md structure and pytest-benchmark usage patterns</approach>
        <success-criteria>Documented process matches actual baseline establishment patterns used in Epic 2.5-3</success-criteria>
      </test-idea>

      <test-idea ac="AC-3.5.3-4">
        <description>UAT: Team validates troubleshooting guide covers common dependency issues</description>
        <approach>Team members review troubleshooting section against Epic 2.5-3 actual issues encountered (spaCy caching, jq installation, pandas compatibility)</approach>
        <success-criteria>Team confirms guide would have helped resolve actual issues encountered, no major gaps identified</success-criteria>
      </test-idea>

      <test-idea ac="AC-3.5.3-5">
        <description>Validate CI/CD integration documentation against actual workflows</description>
        <approach>Compare documented CI/CD integration with .github/workflows/test.yml and .pre-commit-config.yaml configurations</approach>
        <success-criteria>All documented CI/CD stages, caching, and hooks match actual implementation</success-criteria>
      </test-idea>

      <test-idea ac="ALL">
        <description>Link validity check for all cross-references</description>
        <approach>Verify all story IDs, file paths, documentation references are valid and accessible</approach>
        <success-criteria>All links resolve correctly, no broken references</success-criteria>
      </test-idea>

      <test-idea ac="ALL">
        <description>Command accuracy validation</description>
        <approach>Test all example commands locally in development environment before documenting</approach>
        <success-criteria>All commands execute successfully and produce expected outputs</success-criteria>
      </test-idea>
    </ideas>
  </tests>
</story-context>
