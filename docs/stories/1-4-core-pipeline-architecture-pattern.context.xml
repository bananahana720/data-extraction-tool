<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>4</storyId>
    <title>Core Pipeline Architecture Pattern</title>
    <status>drafted</status>
    <generatedAt>2025-11-10</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/1-4-core-pipeline-architecture-pattern.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a developer</asA>
    <iWant>a well-defined pipeline architecture pattern for modular processing</iWant>
    <soThat>I can build composable components (extract → normalize → chunk → analyze) with clear contracts and deterministic data flow</soThat>
    <tasks>
- Task 1: Define core data models (AC: 1.4.3)
  - Create src/data_extract/core/models.py
  - Implement Entity model with type, id, text, confidence fields
  - Implement Metadata model with source_file, file_hash, processing_timestamp, tool_version, config_version, document_type, quality_scores, quality_flags
  - Implement Document model with id, text, entities, metadata, structure
  - Implement Chunk model with id, text, document_id, position_index, token_count, word_count, entities, section_context, quality_score, readability_scores, metadata
  - Implement ProcessingContext model with config, logger, metrics
  - Add Pydantic v2 field validation constraints (confidence: 0.0-1.0, quality_score: 0.0-1.0)
  - Write unit tests for all models (valid/invalid data, field validation)

- Task 2: Define pipeline architecture protocol (AC: 1.4.1, 1.4.5)
  - Create src/data_extract/core/pipeline.py
  - Define PipelineStage Protocol with Generic[Input, Output]
  - Implement process() method signature with input_data, context parameters
  - Document protocol contract in docstrings
  - Implement Pipeline orchestrator class
  - Add __init__(stages: List[PipelineStage]) constructor
  - Implement process() method that chains stage outputs to inputs
  - Write unit tests for Pipeline class with mock stages

- Task 3: Define exception hierarchy (AC: 1.4.6)
  - Create src/data_extract/core/exceptions.py
  - Implement DataExtractError base exception
  - Implement ProcessingError (recoverable) extending DataExtractError
  - Implement CriticalError (unrecoverable) extending DataExtractError
  - Implement ConfigurationError extending CriticalError
  - Implement ExtractionError extending ProcessingError
  - Implement ValidationError extending ProcessingError
  - Add docstrings explaining when to use each exception type
  - Write unit tests for exception hierarchy (raise and catch each type)

- Task 4: Create module structure for pipeline stages (AC: 1.4.2)
  - Create directory src/data_extract/extract/ with __init__.py
  - Create directory src/data_extract/normalize/ with __init__.py
  - Create directory src/data_extract/chunk/ with __init__.py
  - Create directory src/data_extract/semantic/ with __init__.py
  - Create directory src/data_extract/output/ with __init__.py
  - Add placeholder docstrings in each __init__.py describing module purpose
  - Write test to verify all module directories exist and contain __init__.py
  - Test that each module is importable (import succeeds without errors)

- Task 5: Document architecture patterns (AC: 1.4.7)
  - Update docs/architecture.md with Pipeline Stage Pattern section
  - Document PipelineStage protocol contract and generic typing
  - Document data models (Entity, Metadata, Document, Chunk, ProcessingContext)
  - Document exception hierarchy with usage guidance
  - Add example code demonstrating pipeline orchestration
  - Document type contracts between stages (Extract→Normalize→Chunk→Semantic→Output)
  - Document configuration cascade pattern with ProcessingContext

- Task 6: Create integration tests (AC: 1.4.5)
  - Create tests/integration/test_pipeline_architecture.py
  - Write test demonstrating end-to-end pipeline flow with mock stages
  - Test Pipeline orchestrator chains stages correctly
  - Test ProcessingContext passed through all stages
  - Test ProcessingError handling (continue batch processing)
  - Test CriticalError handling (halt processing)
  - Test individual stage can execute standalone

- Task 7: Configuration integration (AC: 1.4.4)
  - Verify ProcessingContext accepts config dict
  - Test logger integration with ProcessingContext
  - Test metrics tracking in ProcessingContext
  - Document configuration structure expected in ProcessingContext.config

- Task 8: Code quality and testing (Quality gate)
  - Run pytest with coverage (target: >90% for core modules)
  - Verify mypy type checking passes for all new code
  - Run black formatter on all new files
  - Run ruff linter and fix any issues
  - Verify all pre-commit hooks pass
    </tasks>
  </story>

  <acceptanceCriteria>
AC-1.4.1: Pipeline interface defined with clear contracts
  - PipelineStage protocol in core/pipeline.py
  - Generic type parameters (Input, Output)
  - process() method signature documented

AC-1.4.2: Pipeline stages have standalone module structure
  - src/data_extract/extract/ exists with __init__.py
  - src/data_extract/normalize/ exists with __init__.py
  - src/data_extract/chunk/ exists with __init__.py
  - src/data_extract/semantic/ exists with __init__.py
  - src/data_extract/output/ exists with __init__.py

AC-1.4.3: Data models defined with Pydantic
  - Document model with validation
  - Chunk model with validation
  - Metadata model with validation
  - Entity model with validation
  - ProcessingContext model

AC-1.4.4: Pipeline configuration centralized
  - ProcessingContext carries config, logger, metrics
  - Config passed through all pipeline stages
  - Type-safe configuration structure

AC-1.4.5: Architecture supports pipeline and single-command execution
  - Pipeline class orchestrates multiple stages
  - Individual stages can be executed standalone
  - Demonstrated in example test

AC-1.4.6: Error handling strategy consistent
  - Exception hierarchy defined in core/exceptions.py
  - ProcessingError for recoverable errors
  - CriticalError for unrecoverable errors
  - All exceptions documented

AC-1.4.7: Architecture documented in docs/architecture.md
  - Pipeline pattern explained
  - Data models documented
  - Error handling strategy described
  - Examples provided
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic 1 Technical Specification</title>
        <section>Detailed Design - Data Models and Contracts</section>
        <snippet>Defines core Pydantic models (Entity, Metadata, Document, Chunk, ProcessingContext). Entity model has type, id, text, confidence fields with validation constraints (0.0-1.0).</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic 1 Technical Specification</title>
        <section>Detailed Design - APIs and Interfaces</section>
        <snippet>PipelineStage Protocol with Generic[Input, Output] type parameters. Pipeline orchestrator class chains stages. All stages implement this protocol contract.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic 1 Technical Specification</title>
        <section>Exception Interface</section>
        <snippet>Exception hierarchy: DataExtractError base, ProcessingError (recoverable, continue batch), CriticalError (unrecoverable, halt processing), ConfigurationError, ExtractionError, ValidationError.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Implementation Patterns - Pipeline Stage Pattern</section>
        <snippet>Protocol-based PipelineStage interface with Generic[Input, Output]. All stages implement process(input_data: Input, context: ProcessingContext) -> Output method signature.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Implementation Patterns - Error Handling Pattern</section>
        <snippet>Try-except pattern: catch ProcessingError for logging/quarantine/continue, catch CriticalError to halt. Supports batch processing resilience.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Data Architecture - Core Data Models</section>
        <snippet>Complete Pydantic v2 model specifications for Document (id, text, entities, metadata, structure), Chunk (with quality scoring), Entity (with confidence), Metadata (provenance tracking), ProcessingContext (config, logger, metrics).</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Project Structure</section>
        <snippet>Module structure: core/models.py, core/pipeline.py, core/exceptions.py. Placeholder modules: extract/, normalize/, chunk/, semantic/, output/ with __init__.py files.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Product Epics</title>
        <section>Epic 1 Story 1.4</section>
        <snippet>Story goal: composable pipeline components with clear contracts and deterministic data flow. Design for determinism (no hidden state), streaming/memory-efficient processing, Protocol/ABC interfaces.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>Non-Functional Requirements - Maintainability</section>
        <snippet>NFR-M1: Clear separation of concerns. NFR-M3: Independent, pluggable, replaceable pipeline components. Modular architecture for extract→normalize→chunk→analyze flow.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>Domain Requirements</section>
        <snippet>Mandatory: Deterministic processing for audit reproducibility, comprehensive logging for audit trails, graceful error handling (no silent failures), quality validation and flagging.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/data_extract/core/__init__.py</path>
        <kind>module</kind>
        <symbol>__init__</symbol>
        <lines>1-7</lines>
        <reason>Placeholder for Story 1.4 implementation. Documents that models.py, pipeline.py, and exception hierarchy will be added.</reason>
      </artifact>
      <artifact>
        <path>src/core/models.py</path>
        <kind>module</kind>
        <symbol>ContentBlock, ExtractionResult, ProcessingResult</symbol>
        <lines>N/A</lines>
        <reason>Brownfield models using Pydantic. Reference pattern for Story 1.4's new models (Entity, Document, Chunk). Existing structure uses BaseModel with Field validators.</reason>
      </artifact>
      <artifact>
        <path>src/core/interfaces.py</path>
        <kind>module</kind>
        <symbol>BaseExtractor, BaseProcessor, BasePipeline</symbol>
        <lines>N/A</lines>
        <reason>Brownfield interfaces using Protocol pattern. Story 1.4 follows same Protocol[Generic[Input, Output]] pattern for PipelineStage.</reason>
      </artifact>
      <artifact>
        <path>tests/conftest.py</path>
        <kind>test-fixture</kind>
        <symbol>fixture_dir, temp_test_file, validate_extraction_result</symbol>
        <lines>333-340, 270-296, 348-380</lines>
        <reason>Established test patterns from Story 1.3. CRITICAL: Use Path(__file__).parent / "fixtures" pattern for robust path resolution. Fixtures use yield...finally for cleanup. Type hints required for callbacks.</reason>
      </artifact>
      <artifact>
        <path>tests/integration/</path>
        <kind>directory</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>Integration test location for Story 1.4's test_pipeline_architecture.py. Tests will demonstrate end-to-end pipeline with mock stages.</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="pydantic" version="&gt;=2.0.0,&lt;3.0" scope="required">Data validation with Field constraints. Story 1.4 uses for Entity, Metadata, Document, Chunk, ProcessingContext models.</package>
        <package name="structlog" version="&gt;=24.0.0,&lt;25.0" scope="required">Structured logging. ProcessingContext carries logger instance through pipeline.</package>
        <package name="PyYAML" version="&gt;=6.0.0,&lt;7.0" scope="required">YAML config parsing. Three-tier precedence: CLI > env vars > YAML > defaults.</package>
        <package name="pytest" version="&gt;=8.0.0,&lt;9.0" scope="dev">Testing framework. Story 1.4 targets &gt;90% coverage for core modules.</package>
        <package name="pytest-cov" version="&gt;=5.0.0,&lt;6.0" scope="dev">Coverage reporting. Epic 1 baseline: 60% (pytest.ini fail_under=60).</package>
        <package name="mypy" version="&gt;=1.11.0,&lt;2.0" scope="dev">Type checking. All new code must pass mypy with Generic type parameters.</package>
        <package name="black" version="&gt;=24.0.0,&lt;25.0" scope="dev">Code formatting. Line length 100, target Python 3.12.</package>
        <package name="ruff" version="&gt;=0.6.0,&lt;0.7" scope="dev">Linting. Pre-commit enforces E, F, I, N, W rules.</package>
        <package name="pre-commit" version="&gt;=3.0.0,&lt;4.0" scope="dev">Git hooks. Runs black, ruff, mypy before commit.</package>
      </python>
      <system>
        <requirement>Python 3.12.x (ADR-004 mandates 3.12, not 3.13)</requirement>
      </system>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="architectural">Pipeline Stage Pattern: All stages implement PipelineStage[Input, Output] Protocol. Generic type parameters enforce compile-time type safety. No inheritance - use composition.</constraint>
    <constraint type="architectural">Deterministic Processing: No hidden state between stages. All state passed via ProcessingContext. Enables audit reproducibility (domain requirement).</constraint>
    <constraint type="architectural">Configuration Cascade: Three-tier precedence (CLI flags > env vars > YAML config > defaults). ProcessingContext carries config dict through all stages.</constraint>
    <constraint type="error-handling">Exception Hierarchy: ProcessingError (recoverable, log+continue batch), CriticalError (unrecoverable, halt). Never silent failures (domain mandate).</constraint>
    <constraint type="data-model">Pydantic v2 with Field Validators: confidence and quality_score constrained to 0.0-1.0 range. Use frozen=False for mutability where needed.</constraint>
    <constraint type="testing">Path Resolution: CRITICAL - Use Path(__file__).parent.parent / "fixtures" pattern (NOT hardcoded relative paths). Established in Story 1.3.</constraint>
    <constraint type="testing">Coverage Target: &gt;90% for new core modules (models.py, pipeline.py, exceptions.py). Epic 1 baseline: 60% overall (pytest.ini fail_under=60).</constraint>
    <constraint type="testing">Type Hints Required: All callbacks need explicit type hints (def callback(status: dict) -> None). Established in Story 1.3.</constraint>
    <constraint type="testing">Cleanup Pattern: Use yield...finally for temp file cleanup in fixtures. Established pattern in tests/conftest.py.</constraint>
    <constraint type="quality">Pre-commit Gates: Code must pass black, ruff, mypy before commit. CI pipeline blocks PRs if checks fail.</constraint>
    <constraint type="module-structure">Module Placeholders: extract/, normalize/, chunk/, semantic/, output/ get __init__.py with docstrings. No implementation in Story 1.4 (deferred to future epics).</constraint>
    <constraint type="documentation">Architecture.md Updates: Document Pipeline Stage Pattern, data models, exception hierarchy, type contracts, configuration cascade. Include example code.</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>PipelineStage[Input, Output]</name>
      <kind>Protocol</kind>
      <signature>def process(self, input_data: Input, context: ProcessingContext) -> Output</signature>
      <path>src/data_extract/core/pipeline.py</path>
      <description>Protocol defining contract for all pipeline stages. Generic type parameters provide compile-time type safety. All stages (extract, normalize, chunk, semantic, output) implement this.</description>
    </interface>
    <interface>
      <name>Pipeline</name>
      <kind>Class</kind>
      <signature>class Pipeline: __init__(self, stages: List[PipelineStage]) / def process(self, initial_input, context) -> Any</signature>
      <path>src/data_extract/core/pipeline.py</path>
      <description>Orchestrator class that chains pipeline stages. Passes output of stage N as input to stage N+1. Handles ProcessingContext propagation.</description>
    </interface>
    <interface>
      <name>Entity</name>
      <kind>Pydantic Model</kind>
      <signature>class Entity(BaseModel): type: str, id: str, text: str, confidence: float = Field(ge=0.0, le=1.0)</signature>
      <path>src/data_extract/core/models.py</path>
      <description>Domain entity model (risk, control, policy, process, regulation, issue). Used by Document and Chunk models.</description>
    </interface>
    <interface>
      <name>Metadata</name>
      <kind>Pydantic Model</kind>
      <signature>class Metadata(BaseModel): source_file: Path, file_hash: str, processing_timestamp: datetime, tool_version: str, config_version: str, document_type: str, quality_scores: Dict, quality_flags: List</signature>
      <path>src/data_extract/core/models.py</path>
      <description>Provenance and quality tracking metadata. Embedded in Document and Chunk models. file_hash uses SHA-256.</description>
    </interface>
    <interface>
      <name>Document</name>
      <kind>Pydantic Model</kind>
      <signature>class Document(BaseModel): id: str, text: str, entities: List[Entity], metadata: Metadata, structure: Dict[str, Any]</signature>
      <path>src/data_extract/core/models.py</path>
      <description>Processed document model. Type contract: Extract → Normalize stage. Represents document after extraction with raw/cleaned text.</description>
    </interface>
    <interface>
      <name>Chunk</name>
      <kind>Pydantic Model</kind>
      <signature>class Chunk(BaseModel): id: str (format: {source}_{index:03d}), text: str, document_id: str, position_index: int, token_count: int, word_count: int, entities: List[Entity], section_context: str, quality_score: float = Field(ge=0.0, le=1.0), readability_scores: Dict, metadata: Metadata</signature>
      <path>src/data_extract/core/models.py</path>
      <description>Semantic chunk for RAG. Type contract: Chunk → Semantic stage. Includes quality scoring and readability metrics.</description>
    </interface>
    <interface>
      <name>ProcessingContext</name>
      <kind>Pydantic Model</kind>
      <signature>class ProcessingContext(BaseModel): config: Dict, logger: structlog.BoundLogger, metrics: Dict</signature>
      <path>src/data_extract/core/models.py</path>
      <description>Shared pipeline state. Passed through all stages. Carries config (three-tier precedence), logger instance, metrics tracker.</description>
    </interface>
    <interface>
      <name>DataExtractError</name>
      <kind>Exception</kind>
      <signature>class DataExtractError(Exception)</signature>
      <path>src/data_extract/core/exceptions.py</path>
      <description>Base exception for all tool errors. Subclassed by ProcessingError and CriticalError.</description>
    </interface>
    <interface>
      <name>ProcessingError</name>
      <kind>Exception</kind>
      <signature>class ProcessingError(DataExtractError)</signature>
      <path>src/data_extract/core/exceptions.py</path>
      <description>Recoverable error - log warning, quarantine file, continue batch processing. Subclasses: ExtractionError, ValidationError.</description>
    </interface>
    <interface>
      <name>CriticalError</name>
      <kind>Exception</kind>
      <signature>class CriticalError(DataExtractError)</signature>
      <path>src/data_extract/core/exceptions.py</path>
      <description>Unrecoverable error - log error and halt processing immediately. Subclass: ConfigurationError.</description>
    </interface>
  </interfaces>
  <tests>
    <standards>
Testing Framework: pytest with pytest-cov for coverage reporting. Coverage target: &gt;90% for new core modules (models.py, pipeline.py, exceptions.py), &gt;60% overall Epic 1 baseline (pytest.ini fail_under=60).

Test Markers: Use pytest markers for selective execution: @pytest.mark.unit (fast, isolated), @pytest.mark.integration (multiple components), @pytest.mark.pipeline (pipeline orchestration), @pytest.mark.infrastructure (core components).

Path Resolution Pattern (CRITICAL): Use Path(__file__).parent.parent / "fixtures" for test fixture paths. NOT hardcoded relative paths like "tests/fixtures". This pattern established in Story 1.3 prevents working directory issues.

Fixture Patterns: Use yield...finally for cleanup (temp files, resources). Type hints required for all callbacks: def callback(status: dict) -> None. Leverage shared fixtures in tests/conftest.py for ContentBlock, ExtractionResult, ProcessingResult patterns.

Error Handling Tests: Wrap pipeline calls in try-except with descriptive messages. Test both ProcessingError (recoverable, continue batch) and CriticalError (unrecoverable, halt) behaviors.

Quality Gates: All tests must pass pytest, ruff, mypy, black before commit (pre-commit hooks enforce). CI pipeline blocks PRs if any check fails.
    </standards>
    <locations>
      <location>tests/unit/core/ - Unit tests for models.py, pipeline.py, exceptions.py</location>
      <location>tests/integration/ - Integration test test_pipeline_architecture.py for end-to-end pipeline flow</location>
      <location>tests/fixtures/ - Test fixture files (use Path(__file__).parent.parent / "fixtures" to access)</location>
      <location>tests/conftest.py - Shared pytest fixtures and configuration</location>
    </locations>
    <ideas>
      <test ac="AC-1.4.1" description="Pipeline interface defined with clear contracts">
        <idea>Test PipelineStage Protocol compliance with mock implementation using Generic[str, int] type parameters</idea>
        <idea>Test process() method signature accepts input_data and context parameters with correct types</idea>
        <idea>Test mypy type checking passes for PipelineStage implementations with various Input/Output types</idea>
      </test>
      <test ac="AC-1.4.2" description="Pipeline stages have standalone module structure">
        <idea>Test all module directories exist: extract/, normalize/, chunk/, semantic/, output/</idea>
        <idea>Test each module has __init__.py with docstring describing purpose</idea>
        <idea>Test all modules are importable without errors (import data_extract.extract, etc.)</idea>
      </test>
      <test ac="AC-1.4.3" description="Data models defined with Pydantic">
        <idea>Test Entity model instantiation with valid data (type, id, text, confidence=0.5)</idea>
        <idea>Test Entity confidence validation rejects values outside 0.0-1.0 range</idea>
        <idea>Test Metadata model with all fields (source_file, file_hash, processing_timestamp, etc.)</idea>
        <idea>Test Document model with entities list and metadata embedding</idea>
        <idea>Test Chunk model with quality_score validation (0.0-1.0), id format {source}_{index:03d}</idea>
        <idea>Test ProcessingContext model with config dict, logger, metrics</idea>
        <idea>Test Pydantic validation raises ValidationError for invalid field types</idea>
      </test>
      <test ac="AC-1.4.4" description="Pipeline configuration centralized">
        <idea>Test ProcessingContext accepts config dict and makes it accessible</idea>
        <idea>Test ProcessingContext carries structlog logger instance through pipeline stages</idea>
        <idea>Test ProcessingContext metrics dict is mutable for accumulating metrics</idea>
        <idea>Test configuration precedence: CLI > env vars > YAML > defaults (integration test)</idea>
      </test>
      <test ac="AC-1.4.5" description="Architecture supports pipeline and single-command execution">
        <idea>Test Pipeline class chains 3 mock stages (Input->Middle->Output) correctly</idea>
        <idea>Test Pipeline.process() passes output of stage N as input to stage N+1</idea>
        <idea>Test individual PipelineStage can execute standalone without Pipeline orchestrator</idea>
        <idea>Test empty pipeline (no stages) returns initial input unchanged</idea>
      </test>
      <test ac="AC-1.4.6" description="Error handling strategy consistent">
        <idea>Test DataExtractError base exception can be raised and caught</idea>
        <idea>Test ProcessingError (recoverable) extends DataExtractError</idea>
        <idea>Test CriticalError (unrecoverable) extends DataExtractError</idea>
        <idea>Test ConfigurationError extends CriticalError</idea>
        <idea>Test ExtractionError and ValidationError extend ProcessingError</idea>
        <idea>Test exception hierarchy: catch DataExtractError catches all subclasses</idea>
        <idea>Test docstrings document when to use each exception type</idea>
      </test>
      <test ac="AC-1.4.7" description="Architecture documented in docs/architecture.md">
        <idea>Verify docs/architecture.md contains Pipeline Stage Pattern section (manual/automated check)</idea>
        <idea>Verify data models documented with field descriptions</idea>
        <idea>Verify exception hierarchy documented with usage examples</idea>
        <idea>Verify type contracts between stages documented (Extract→Normalize→Chunk→Semantic→Output)</idea>
      </test>
      <test type="integration" description="End-to-end pipeline flow">
        <idea>Test Pipeline with 3 mock stages (StringToInt->IntToFloat->FloatToString) demonstrates type safety</idea>
        <idea>Test ProcessingContext passed through all stages and accessible in each stage</idea>
        <idea>Test ProcessingError in stage 2: logs warning, stage 3 still executes (resilient batch processing)</idea>
        <idea>Test CriticalError in stage 2: pipeline halts immediately, stage 3 never executes</idea>
        <idea>Test pipeline metrics accumulation: each stage increments metrics dict</idea>
      </test>
      <test type="edge_case" description="Edge cases and error conditions">
        <idea>Test Pydantic models with missing required fields raise ValidationError</idea>
        <idea>Test Entity with confidence=-0.1 raises ValidationError (below 0.0)</idea>
        <idea>Test Entity with confidence=1.5 raises ValidationError (above 1.0)</idea>
        <idea>Test Chunk with empty text field (valid but edge case)</idea>
        <idea>Test ProcessingContext with empty config dict (should work with defaults)</idea>
        <idea>Test Pipeline with single stage (edge case of N=1)</idea>
      </test>
    </ideas>
  </tests>
</story-context>
