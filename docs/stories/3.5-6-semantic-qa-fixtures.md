# Story 3.5.6: Semantic QA Fixtures

Status: backlog

## Story

As a QA engineer responsible for validating Epic 4 semantic analysis features,
I want a semantic test corpus with ≥50 documents and gold-standard TF-IDF/LSA/entity annotations,
so that regression tests can validate semantic processor outputs against known-good baselines.

### Story Header

- **Story Key:** `3.5-6-semantic-qa-fixtures` (Epic 3.5, Story ID 3.5.6)
- **Context:** Epic 4 cannot validate TF-IDF/LSA without representative test corpus and expected outputs. [Source: docs/retrospectives/epic-3-retro-2025-11-16.md#L58]
- **Dependencies:** Story 3.5.4 (semantic libraries for generating gold-standard outputs)
- **Estimate:** 6 hours
- **Owner:** Dana

### Story Body

Implementation will create a semantic test corpus repository in `tests/fixtures/semantic/` containing ≥50 audit documents (≥250k words total) covering 3+ document types (audit reports, risk matrices, compliance documents). All documents must be PII-sanitized with no real names, SSNs, or account numbers. Gold-standard annotations will include expected TF-IDF top terms, LSA primary topics, entity annotations, and readability scores, generated using validated semantic libraries and manually reviewed. A comparison harness (`tests/fixtures/semantic/harness/`) will provide regression test scripts for TF-IDF, LSA, and entity extraction validation. [Source: docs/retrospectives/epic-3-retro-2025-11-16.md#L58]

## Acceptance Criteria

1. **Corpus size:** ≥50 documents total, ≥250,000 words total, representative of production audit documents. [Source: docs/tech-spec-epic-3.5.md#L236-L237]
2. **Document types:** Covers 3+ types (audit reports, risk matrices, compliance docs) with balanced distribution. [Source: docs/tech-spec-epic-3.5.md#L236-L237]
3. **PII sanitization:** All documents reviewed and sanitized (no real names, SSNs, account numbers, sensitive data). [Source: docs/tech-spec-epic-3.5.md#L237]
4. **Gold-standard annotations:** JSON files with expected TF-IDF top terms, LSA topics, entity annotations, readability scores for ≥40 documents. [Source: docs/tech-spec-epic-3.5.md#L239-L249]
5. **Comparison harness:** Scripts for regression testing (compare-tfidf.py, compare-lsa.py, compare-entities.py) with tolerance thresholds. [Source: docs/tech-spec-epic-3.5.md#L228-L232]
6. **Documentation:** README.md documenting corpus statistics, annotation format, harness usage, and corpus creation process. [Source: docs/tech-spec-epic-3.5.md#L233]

## Tasks / Subtasks

- [ ] **Task 1: Source and sanitize documents (AC: 1-3)**
  - [ ] Source ≥50 audit-domain documents from public datasets or generate synthetic docs
  - [ ] Review all documents for PII (names, SSNs, account numbers, addresses, etc.)
  - [ ] Sanitize PII using redaction or pseudonymization
  - [ ] Organize into 3+ document type folders (audit-reports/, risk-matrices/, compliance-docs/)
  - [ ] Verify word count totals ≥250k words

- [ ] **Task 2: Generate gold-standard annotations (AC: 4)**
  - [ ] Use validated TF-IDF vectorizer (Story 3.5.4) to generate top terms for each document
  - [ ] Use validated LSA (Story 3.5.4) to assign primary topics
  - [ ] Manually annotate ≥40 documents with entity references (RISK-XXX, CTRL-XXX patterns)
  - [ ] Calculate textstat readability scores for each document
  - [ ] Create JSON annotation files matching schema (document_id, entities, tfidf_top_terms, lsa_primary_topic, readability_score)
  - [ ] Review annotations for accuracy and consistency

- [ ] **Task 3: Create comparison harness (AC: 5)**
  - [ ] Implement `compare-tfidf.py`: Load gold-standard, run TF-IDF, compare top terms with tolerance
  - [ ] Implement `compare-lsa.py`: Load gold-standard, run LSA, compare topic assignments
  - [ ] Implement `compare-entities.py`: Load gold-standard, run entity extraction, compare precision/recall
  - [ ] Define reasonable tolerance thresholds (e.g., 80% overlap for top terms)
  - [ ] Add pytest integration for harness scripts

- [ ] **Task 4: Documentation and validation (AC: 6)**
  - [ ] Create `tests/fixtures/semantic/README.md` with corpus statistics
  - [ ] Document annotation format and gold-standard generation process
  - [ ] Document harness usage and tolerance thresholds
  - [ ] Create metadata.json with corpus statistics (doc count, word count, type distribution)
  - [ ] QA review for completeness and usability

## Dev Notes

### Requirements Context Summary

- **Retrospective prep task #3:** Semantic QA fixtures + comparison harness (TF-IDF/LSA expected outputs) – 6h, Dana. [Source: docs/retrospectives/epic-3-retro-2025-11-16.md#L58]
- **Epic 4 blocker:** Cannot validate semantic processor without test corpus and gold-standard outputs. [Source: docs/tech-spec-epic-3.5.md#L45-L67]
- **Representative corpus:** Must reflect production audit document characteristics (terminology, structure, readability). [Source: docs/tech-spec-epic-3.5.md#L238]

### Structure Alignment Summary

- Fixtures follow existing `tests/fixtures/` directory structure.
- Corpus organization mirrors Epic 2 extraction test fixtures (by document type).
- Comparison harness integrates with pytest for automated regression testing.

### References

- `docs/retrospectives/epic-3-retro-2025-11-16.md#L58` – Preparation sprint task #3
- `docs/tech-spec-epic-3.5.md#L224-L249` – Semantic QA fixtures structure and requirements
- `tests/fixtures/` – Existing test fixture patterns
- `docs/stories/3.5-4-semantic-dependencies-smoke-test.md` – Validated TF-IDF/LSA for gold-standard generation

## Dev Agent Record

### Context Reference

- docs/retrospectives/epic-3-retro-2025-11-16.md
- docs/tech-spec-epic-3.5.md
- tests/fixtures/

### Agent Model Used

claude-sonnet-4-5-20250929

### Debug Log References

Session 2025-11-17: Story 3.5.6 drafted from Epic 3.5 tech spec and retrospective preparation tasks.

### Completion Notes List

- **2025-11-17:** Story created with 6 ACs, 4 task groups, references to retrospective and tech spec.

### File List

**Created:**
- `docs/stories/3.5-6-semantic-qa-fixtures.md` (this document)
