# Story 3.5.6: Semantic QA Fixtures

Status: todo - Ready for implementation (Epic 3.5 - Bridge Epic)

## Story

As a **QA engineer validating Epic 4 semantic analysis features**,
I want **comprehensive test fixtures with known semantic relationships**,
so that **I can verify TF-IDF, LSA, and similarity calculations produce expected results**.

## Context Summary

**Epic Context:** Story 3.5.6 is part of Epic 3.5 (Bridge Epic - Tooling & Semantic Prep), which prepares the project for Epic 4 (Foundational Semantic Analysis). This story addresses a preparation task from the Epic 3 retrospective: "Semantic QA fixtures (Owner: Dana, Est: 6h)."

**Business Value:**
- Enables systematic QA validation of Epic 4 semantic analysis (not manual guesswork)
- Provides ground truth for similarity calculations (known similar/dissimilar document pairs)
- Accelerates Epic 4 story validation (fixtures ready upfront, no mid-story delays)
- Supports regression testing (fixtures persist across dependency updates)
- Documents expected semantic behavior (fixtures double as specification)

**Dependencies:**
- **Epic 3 Complete (Chunk & Output)** - Chunk data model and metadata available
- **Story 3.5.4 (Semantic Dependencies)** - scikit-learn, joblib, gensim installed
- **Story 3.5.5 (Model/Cache ADR)** - Caching strategy for test fixtures (if needed)
- **Epic 2/3 Fixtures:** Existing test fixtures (audit documents, risk registers, compliance reports)

**Technical Foundation:**
- **pytest fixtures** - Python testing framework fixture system
- **Audit domain knowledge** - Risk findings, controls, compliance requirements
- **Semantic relationships** - Similar risks, related controls, control→risk mitigation
- **Ground truth labels** - Human-annotated similarity scores (0.0-1.0 scale)

**Key Requirements:**
1. **Document Corpus:** 20+ audit document snippets covering diverse topics
2. **Similarity Ground Truth:** Labeled pairs with expected similarity scores
3. **Topic Clusters:** Documents grouped by topic (risks, controls, compliance, processes)
4. **Edge Cases:** Very similar documents, very dissimilar, ambiguous cases
5. **Fixture Format:** pytest fixtures returning structured data (corpus, labels, metadata)

## Acceptance Criteria

**AC-3.5.6-1: Audit Document Corpus Created (P0 - Critical)**
- Fixture file created: `tests/fixtures/semantic_corpus.py`
- Corpus contains 20-30 audit document snippets covering:
  - **Risks (8-10 docs):** Data breaches, compliance violations, operational failures
  - **Controls (8-10 docs):** Authentication, encryption, access controls, audit logging
  - **Compliance findings (4-6 docs):** SOC2, GDPR, HIPAA, PCI-DSS requirements
  - **Processes (4-6 docs):** Incident response, change management, vendor management
- Each document is 50-200 words (realistic chunk size from Epic 3)
- Documents are realistic audit language (not lorem ipsum)
- **Validation:** Manual review by QA and domain expert (audit background)
- **UAT Required:** Yes - Domain expert validates realism and diversity

**AC-3.5.6-2: Similarity Ground Truth Labels Created (P0 - Critical)**
- Fixture includes 30+ labeled document pairs with expected similarity scores:
  - **High similarity (0.7-1.0):** 10+ pairs
    - Example: "Data breach from weak passwords" vs "Password security failure"
  - **Medium similarity (0.3-0.7):** 10+ pairs
    - Example: "Data breach from weak passwords" vs "Multi-factor authentication control"
  - **Low similarity (0.0-0.3):** 10+ pairs
    - Example: "Data breach from weak passwords" vs "Vendor contract management"
- Labels created by human annotation (Dana + domain expert)
- Labels stored in structured format:
  ```python
  SIMILARITY_GROUND_TRUTH = [
      {
          "doc_id_1": "RISK-001",
          "doc_id_2": "RISK-002",
          "expected_similarity": 0.85,
          "rationale": "Both describe data breach risks from authentication failures"
      },
      # ... more pairs
  ]
  ```
- **Validation:** Inter-annotator agreement >80% (Dana + domain expert independently label sample)
- **UAT Required:** Yes - Validate labels make semantic sense

**AC-3.5.6-3: Topic Cluster Labels Created (P1)**
- Fixture includes topic cluster assignments for all documents:
  ```python
  TOPIC_CLUSTERS = {
      "data_security": ["RISK-001", "RISK-002", "CTRL-001", "CTRL-002"],
      "compliance_audit": ["COMP-001", "COMP-002", "COMP-003"],
      "access_management": ["CTRL-003", "CTRL-004", "RISK-003"],
      "vendor_risk": ["RISK-004", "PROC-001", "PROC-002"],
      # ... more clusters
  }
  ```
- Clusters support topic modeling validation in Epic 4
- Each cluster contains 3-8 documents
- Clusters are mutually exclusive (each document in exactly one cluster)
- **Validation:** Manual review of cluster coherence by domain expert
- **UAT Required:** No - Epic 4 will validate clustering algorithms

**AC-3.5.6-4: Edge Case Documents Included (P1)**
- Corpus includes challenging edge cases:
  - **Very similar documents (0.9+ similarity):** Near-duplicates, paraphrases
  - **Very dissimilar documents (0.1- similarity):** Completely unrelated topics
  - **Ambiguous cases (0.4-0.6 similarity):** Related but distinct (e.g., risk and mitigation control)
  - **Short documents (<50 words):** Edge case for TF-IDF (sparse vectors)
  - **Long documents (>200 words):** Edge case for chunking (multi-sentence)
  - **Domain jargon:** Compliance acronyms (SOC2, GDPR, PCI-DSS), audit terminology
- Edge cases labeled with rationale explaining expected behavior
- **Validation:** Review edge cases with Epic 4 lead (Charlie)
- **UAT Required:** Yes - Validate edge cases test relevant scenarios

**AC-3.5.6-5: pytest Fixtures Implemented (P0)**
- pytest fixtures created in `tests/fixtures/semantic_corpus.py`:
  ```python
  import pytest

  @pytest.fixture
  def audit_corpus():
      """Returns list of audit document snippets with metadata."""
      return [
          {
              "doc_id": "RISK-001",
              "text": "Data breach due to inadequate password policies...",
              "topic": "data_security",
              "type": "risk",
              "word_count": 120
          },
          # ... more documents
      ]

  @pytest.fixture
  def similarity_ground_truth():
      """Returns labeled document pairs with expected similarity scores."""
      return SIMILARITY_GROUND_TRUTH  # From AC-3.5.6-2

  @pytest.fixture
  def topic_clusters():
      """Returns topic cluster assignments for documents."""
      return TOPIC_CLUSTERS  # From AC-3.5.6-3
  ```
- Fixtures registered in `tests/conftest.py` for global access
- **Validation:** Unit test imports fixtures, verifies structure
- **UAT Required:** No - Integration test in Epic 4 validates usage

**AC-3.5.6-6: Fixture Documentation Created (P1)**
- README created: `tests/fixtures/semantic_README.md`
- Documentation includes:
  - Purpose: "Fixtures for Epic 4 semantic analysis QA validation"
  - Corpus overview: Document counts, topics, word count distribution
  - Ground truth labels: How similarity scores were determined
  - Usage examples: pytest test using fixtures
  - Maintenance: How to add new documents or update labels
- **Validation:** Manual review of documentation completeness
- **UAT Required:** No - Epic 4 developers will validate usability

## Acceptance Criteria Trade-offs and Deferrals

**AC-3.5.6-2 Trade-off (Human Annotation Cost):**
- **Issue:** Human annotation of 30+ pairs is time-consuming (6h estimate)
- **Resolution:** Dana annotates with domain expert review (shared workload)
- **Rationale:** Ground truth quality critical for Epic 4 validation (worth the investment)
- **Documented In:** Dev Notes, Fixture README

**AC-3.5.6-3 Reduced Priority (P1):**
- **Issue:** Topic clusters less critical than similarity ground truth
- **Resolution:** Include clusters but reduce priority to P1 (important but not blocking)
- **Rationale:** Epic 4 may not implement topic modeling initially (LSA/TF-IDF focus first)
- **Documented In:** AC priority levels

## Tasks / Subtasks

### Task 1: Research Audit Domain Examples (AC: #3.5.6-1, #3.5.6-4)
- [ ] Review existing Epic 2/3 test fixtures for audit document examples
- [ ] Research real SOC2, GDPR, HIPAA compliance reports (public examples)
- [ ] Identify common risk categories (data security, access control, vendor management)
- [ ] Identify common control types (authentication, encryption, logging, monitoring)
- [ ] Collect domain jargon and acronyms (NIST, ISO 27001, COBIT)

### Task 2: Create Risk Finding Documents (AC: #3.5.6-1)
- [ ] Write 8-10 risk finding documents (50-200 words each)
- [ ] Cover diverse risk categories:
  - Data security risks (breaches, leaks, unauthorized access)
  - Compliance risks (regulatory violations, audit failures)
  - Operational risks (system downtime, process failures)
  - Vendor risks (third-party failures, supply chain issues)
- [ ] Use realistic audit language and terminology
- [ ] Assign unique IDs (RISK-001, RISK-002, ...)

### Task 3: Create Control Description Documents (AC: #3.5.6-1)
- [ ] Write 8-10 control description documents (50-200 words each)
- [ ] Cover diverse control types:
  - Authentication controls (MFA, SSO, password policies)
  - Encryption controls (data at rest, data in transit)
  - Access controls (RBAC, least privilege, separation of duties)
  - Monitoring controls (logging, alerting, SIEM)
- [ ] Use realistic control description language
- [ ] Assign unique IDs (CTRL-001, CTRL-002, ...)

### Task 4: Create Compliance Finding Documents (AC: #3.5.6-1)
- [ ] Write 4-6 compliance finding documents (50-200 words each)
- [ ] Cover diverse compliance frameworks:
  - SOC2 Type II (security, availability, confidentiality)
  - GDPR (data privacy, consent, right to erasure)
  - HIPAA (PHI protection, access logging)
  - PCI-DSS (cardholder data, network segmentation)
- [ ] Use realistic compliance report language
- [ ] Assign unique IDs (COMP-001, COMP-002, ...)

### Task 5: Create Process Description Documents (AC: #3.5.6-1)
- [ ] Write 4-6 process description documents (50-200 words each)
- [ ] Cover diverse processes:
  - Incident response (detection, containment, recovery)
  - Change management (approval, testing, rollback)
  - Vendor management (onboarding, risk assessment, monitoring)
  - Backup and recovery (schedule, testing, retention)
- [ ] Use realistic process documentation language
- [ ] Assign unique IDs (PROC-001, PROC-002, ...)

### Task 6: Create Similarity Ground Truth Labels (AC: #3.5.6-2)
- [ ] Select 30+ document pairs covering similarity spectrum:
  - 10+ high similarity pairs (0.7-1.0)
  - 10+ medium similarity pairs (0.3-0.7)
  - 10+ low similarity pairs (0.0-0.3)
- [ ] Human annotation session (Dana + domain expert):
  - Independently score each pair on 0.0-1.0 scale
  - Compare scores, resolve disagreements (average or discussion)
  - Document rationale for each score
- [ ] Calculate inter-annotator agreement (target >80%)
- [ ] Store labels in SIMILARITY_GROUND_TRUTH structure

### Task 7: Create Topic Cluster Labels (AC: #3.5.6-3)
- [ ] Identify 5-8 topic clusters from corpus:
  - data_security, compliance_audit, access_management, vendor_risk, etc.
- [ ] Assign each document to exactly one cluster
- [ ] Validate cluster coherence (documents in cluster are semantically related)
- [ ] Store labels in TOPIC_CLUSTERS structure

### Task 8: Add Edge Case Documents (AC: #3.5.6-4)
- [ ] Create very similar document pairs (near-duplicates, paraphrases)
- [ ] Create very dissimilar document pairs (unrelated topics)
- [ ] Create ambiguous document pairs (related but distinct)
- [ ] Create short documents (<50 words) - edge case for TF-IDF
- [ ] Create long documents (>200 words) - edge case for chunking
- [ ] Add domain jargon examples (SOC2, GDPR, NIST, ISO 27001)
- [ ] Label edge cases with rationale

### Task 9: Implement pytest Fixtures (AC: #3.5.6-5)
- [ ] Create `tests/fixtures/semantic_corpus.py`
- [ ] Implement `@pytest.fixture audit_corpus()` returning document list
- [ ] Implement `@pytest.fixture similarity_ground_truth()` returning labeled pairs
- [ ] Implement `@pytest.fixture topic_clusters()` returning cluster assignments
- [ ] Add docstrings explaining fixture purpose and structure
- [ ] Register fixtures in `tests/conftest.py` for global access

### Task 10: Create Fixture Documentation (AC: #3.5.6-6)
- [ ] Create `tests/fixtures/semantic_README.md`
- [ ] Document fixture purpose and Epic 4 usage
- [ ] Document corpus overview (counts, topics, word distribution)
- [ ] Document ground truth annotation process
- [ ] Include usage examples (pytest test using fixtures)
- [ ] Document maintenance process (adding documents, updating labels)

### Task 11: Validation and UAT
- [ ] Manual review of corpus realism and diversity
- [ ] Calculate inter-annotator agreement for similarity labels (target >80%)
- [ ] Unit test: Import fixtures, verify structure and counts
- [ ] UAT: Domain expert validates corpus realism (AC-3.5.6-1)
- [ ] UAT: Domain expert validates similarity labels (AC-3.5.6-2)
- [ ] UAT: Epic 4 lead validates edge cases (AC-3.5.6-4)

## Dev Notes

**Provenance Tracking:**
- Semantic fixtures are test data (no source_hash needed)
- Document fixture creation date and author in semantic_README.md
- Reference Epic 3.5, Story 3.5.6 in fixture documentation

**Structured Logging:**
- No structured logging needed (test fixtures, not pipeline code)
- pytest output captures fixture loading success/failure

**Pipeline Wiring:**
- Fixtures are test infrastructure (not part of main extraction pipeline)
- Integration point: Epic 4 semantic analysis tests import fixtures
- Usage: `def test_tfidf_similarity(audit_corpus, similarity_ground_truth): ...`

**Quality Gates:**
- No code quality gates (pytest fixtures, not production code)
- Validation gates:
  - Inter-annotator agreement >80% (similarity labels)
  - Domain expert approval (corpus realism)
  - Epic 4 lead approval (edge cases cover relevant scenarios)

**Testing Strategy:**
- Unit test: Verify fixture structure (lists, dicts, expected fields)
- Integration test: Epic 4 stories will use fixtures for semantic analysis validation
- No performance tests needed (fixtures loaded once per test session)

**Corpus Statistics Target:**
- Total documents: 20-30
- Risks: 8-10 (33%)
- Controls: 8-10 (33%)
- Compliance: 4-6 (17%)
- Processes: 4-6 (17%)
- Word count: 50-200 per document (avg ~120 words)
- Total corpus: ~2400-3600 words (realistic test corpus)

**Similarity Annotation Process:**
- **Step 1:** Dana independently scores all 30+ pairs (0.0-1.0 scale)
- **Step 2:** Domain expert independently scores same pairs
- **Step 3:** Compare scores, calculate inter-annotator agreement
- **Step 4:** Resolve disagreements (average scores or discuss until consensus)
- **Step 5:** Document final labels with rationale
- **Target:** >80% agreement (within ±0.1 similarity score)

**Example Risk Document:**
```python
{
    "doc_id": "RISK-001",
    "text": """
    Data breach risk identified due to inadequate password policies.
    Current password requirements allow simple 6-character passwords
    without complexity requirements (uppercase, numbers, special characters).
    No multi-factor authentication enforced for remote access.
    Recent security assessment found 23% of user accounts use common
    passwords listed in breach databases. Recommendation: Implement
    password complexity requirements (minimum 12 characters, mixed case,
    numbers, special characters) and enforce MFA for all remote access
    within 30 days.
    """,
    "topic": "data_security",
    "type": "risk",
    "severity": "high",
    "word_count": 87
}
```

**Example Similarity Label:**
```python
{
    "doc_id_1": "RISK-001",
    "doc_id_2": "RISK-002",
    "expected_similarity": 0.85,
    "rationale": """
    Both documents describe data breach risks from authentication weaknesses.
    RISK-001 focuses on password policies, RISK-002 focuses on access controls.
    Similar domain (data security), similar root cause (authentication),
    but different specific controls (passwords vs access restrictions).
    High similarity due to shared concepts: data breach, authentication,
    access control, security assessment.
    """,
    "annotation_agreement": 0.90  # Dana: 0.83, Expert: 0.87
}
```

**pytest Fixture Usage Example:**
```python
# tests/integration/test_semantic_tfidf.py
import pytest
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def test_tfidf_similarity_ground_truth(audit_corpus, similarity_ground_truth):
    """Validate TF-IDF similarity matches ground truth labels (within tolerance)."""
    # Extract texts
    texts = {doc["doc_id"]: doc["text"] for doc in audit_corpus}

    # Vectorize corpus
    vectorizer = TfidfVectorizer(max_features=100)
    doc_ids = list(texts.keys())
    tfidf_matrix = vectorizer.fit_transform([texts[doc_id] for doc_id in doc_ids])

    # Calculate similarities
    for label in similarity_ground_truth:
        idx1 = doc_ids.index(label["doc_id_1"])
        idx2 = doc_ids.index(label["doc_id_2"])
        calculated_sim = cosine_similarity(tfidf_matrix[idx1], tfidf_matrix[idx2])[0, 0]
        expected_sim = label["expected_similarity"]

        # Validate within tolerance (±0.15 for TF-IDF approximation)
        tolerance = 0.15
        assert abs(calculated_sim - expected_sim) < tolerance, \
            f"Similarity mismatch for {label['doc_id_1']} vs {label['doc_id_2']}: " \
            f"calculated={calculated_sim:.2f}, expected={expected_sim:.2f}"
```

**Edge Cases to Include:**

1. **Very Similar (0.9+ similarity):**
   - RISK-001 (password weakness) vs RISK-001b (password policy failure) - paraphrase

2. **Very Dissimilar (0.1- similarity):**
   - RISK-001 (password weakness) vs PROC-001 (vendor onboarding) - unrelated topics

3. **Ambiguous (0.4-0.6 similarity):**
   - RISK-001 (password weakness) vs CTRL-001 (MFA implementation) - related (mitigation) but distinct

4. **Short Document (<50 words):**
   - "Data breach from weak passwords. Implement MFA immediately."

5. **Long Document (>200 words):**
   - Detailed incident response process description (250 words)

6. **Domain Jargon:**
   - "SOC2 Type II audit identified non-compliance with CC6.1 (logical access controls)"

**Retrospective Learnings Applied:**
- This story directly addresses "Semantic QA fixtures (Owner: Dana, Est: 6h)" (Epic 3 retro prep task)
- Prevents Epic 4 mid-story delays (fixtures ready upfront)
- Provides ground truth for systematic validation (not manual guesswork)
- Documents expected semantic behavior (fixtures double as specification)

**Next Story Dependencies:**
- Story 3.5.7 (TF-IDF/LSA playbook) will reference fixtures as examples
- Epic 4 stories will import fixtures for semantic analysis validation tests
- Fixtures provide regression testing baseline (persist across dependency updates)
