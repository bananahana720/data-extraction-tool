# Story 3.5.4: Semantic Dependencies + Smoke Test

Status: todo - Ready for implementation (Epic 3.5 - Bridge Epic)

## Story

As a **developer preparing for Epic 4 (Foundational Semantic Analysis)**,
I want **semantic analysis dependencies installed and validated with smoke tests**,
so that **Epic 4 stories can start immediately without dependency setup blockers**.

## Context Summary

**Epic Context:** Story 3.5.4 is part of Epic 3.5 (Bridge Epic - Tooling & Semantic Prep), which prepares the project for Epic 4 (Foundational Semantic Analysis). This story addresses a preparation task from the Epic 3 retrospective: "Add semantic dependencies + smoke test (Owner: Charlie, Est: 4h)."

**Business Value:**
- Eliminates dependency setup blockers at start of Epic 4 (no first-story delays)
- Validates semantic libraries work in project environment before Epic 4 implementation
- Catches dependency conflicts early (scikit-learn, joblib, gensim compatibility)
- Provides smoke test baseline for Epic 4 semantic analysis correctness
- Enables parallel Epic 4 story work (no waiting for dependency resolution)

**Dependencies:**
- **Epic 3 Complete (Chunk & Output)** - All chunk metadata available for semantic analysis
- **Story 3.5.3 (Test-Dependency Audit Doc)** - Audit process documented for following
- **Python 3.12+** - Project requirement (semantic libraries must be compatible)
- **Enterprise Constraint:** Classical NLP only (no transformer models like BERT, GPT)

**Technical Foundation:**
- **scikit-learn 1.3+** - TF-IDF vectorization, LSA (Truncated SVD), cosine similarity
- **joblib 1.3+** - Model serialization/deserialization for caching
- **gensim 4.3+** - Optional advanced NLP (Word2Vec, LDA) if needed by Epic 4
- **numpy/scipy** - Transitive dependencies for linear algebra operations
- **Existing chunk pipeline** - ProcessingResult â†’ Chunk with text and metadata

**Key Requirements:**
1. **Dependency Installation:** Add scikit-learn, joblib, gensim to pyproject.toml
2. **Security Audit:** Run pip-audit to check for vulnerabilities
3. **Import Smoke Tests:** Verify all semantic libraries import successfully
4. **TF-IDF Smoke Test:** Validate TF-IDF vectorization on sample text
5. **LSA Smoke Test:** Validate LSA (Truncated SVD) on sample corpus
6. **Cosine Similarity Smoke Test:** Validate similarity calculations between vectors
7. **Model Caching Smoke Test:** Validate joblib serialization/deserialization

## Acceptance Criteria

**AC-3.5.4-1: Semantic Dependencies Added to pyproject.toml (P0 - Critical)**
- Dependencies added to `[project.dependencies]` section:
  - `scikit-learn>=1.3.0,<2.0` - TF-IDF, LSA, cosine similarity
  - `joblib>=1.3.0,<2.0` - Model serialization for caching
  - `gensim>=4.3.0,<5.0` - Advanced NLP (Word2Vec, LDA) - optional
- Version constraints prevent breaking changes (major version pinning)
- Dependencies installed in virtual environment: `pip install -e ".[dev]"`
- **Validation:** `pip freeze` shows correct versions installed
- **UAT Required:** No - Automated installation verification sufficient

**AC-3.5.4-2: Security Audit Passed (P0 - Critical)**
- Run `pip-audit --desc` on updated dependencies
- No critical or high-severity vulnerabilities reported
- Medium/low vulnerabilities documented in issue tracker (not blockers)
- Security scan added to CI/CD pipeline (GitHub Actions)
- **Validation:** pip-audit exit code 0 (no critical vulnerabilities)
- **UAT Required:** Yes - Security review of audit report

**AC-3.5.4-3: Import Smoke Tests Pass (P0 - Critical)**
- Test file created: `tests/smoke/test_semantic_imports.py`
- Smoke tests verify imports:
  ```python
  def test_sklearn_imports():
      from sklearn.feature_extraction.text import TfidfVectorizer
      from sklearn.decomposition import TruncatedSVD
      from sklearn.metrics.pairwise import cosine_similarity
      assert TfidfVectorizer is not None

  def test_joblib_imports():
      import joblib
      assert joblib.dump is not None
      assert joblib.load is not None

  def test_gensim_imports():
      import gensim
      from gensim.models import Word2Vec, LdaModel
      assert Word2Vec is not None
  ```
- All import tests pass in <1 second total
- **Validation:** `pytest tests/smoke/test_semantic_imports.py -v`
- **UAT Required:** No - Automated test sufficient

**AC-3.5.4-4: TF-IDF Smoke Test Pass (P0 - Critical)**
- Smoke test validates TF-IDF vectorization on sample audit document text
- Test file: `tests/smoke/test_semantic_tfidf.py`
- Test logic:
  ```python
  def test_tfidf_vectorization():
      from sklearn.feature_extraction.text import TfidfVectorizer

      # Sample corpus (audit document snippets)
      corpus = [
          "RISK-001: Data breach due to inadequate access controls",
          "CTRL-042: Implement multi-factor authentication for all users",
          "RISK-002: Compliance violation from incomplete audit trail"
      ]

      # Vectorize
      vectorizer = TfidfVectorizer(max_features=100)
      tfidf_matrix = vectorizer.fit_transform(corpus)

      # Validate output shape and sparsity
      assert tfidf_matrix.shape == (3, min(100, len(vectorizer.vocabulary_)))
      assert tfidf_matrix.nnz > 0  # Non-zero elements exist
  ```
- Test passes in <100ms
- **Validation:** `pytest tests/smoke/test_semantic_tfidf.py -v`
- **UAT Required:** Yes - Validate TF-IDF output makes sense for audit domain

**AC-3.5.4-5: LSA Smoke Test Pass (P1)**
- Smoke test validates LSA (Latent Semantic Analysis) using Truncated SVD
- Test file: `tests/smoke/test_semantic_lsa.py`
- Test logic:
  ```python
  def test_lsa_decomposition():
      from sklearn.feature_extraction.text import TfidfVectorizer
      from sklearn.decomposition import TruncatedSVD

      # Sample corpus
      corpus = [...]  # 10+ audit document snippets

      # Vectorize and reduce dimensionality
      vectorizer = TfidfVectorizer(max_features=100)
      tfidf_matrix = vectorizer.fit_transform(corpus)

      svd = TruncatedSVD(n_components=5)
      lsa_matrix = svd.fit_transform(tfidf_matrix)

      # Validate output shape
      assert lsa_matrix.shape == (len(corpus), 5)
      assert svd.explained_variance_ratio_.sum() > 0
  ```
- Test passes in <200ms
- **Validation:** `pytest tests/smoke/test_semantic_lsa.py -v`
- **UAT Required:** No - Automated test sufficient

**AC-3.5.4-6: Cosine Similarity Smoke Test Pass (P1)**
- Smoke test validates cosine similarity calculations between document vectors
- Test file: `tests/smoke/test_semantic_similarity.py`
- Test logic:
  ```python
  def test_cosine_similarity():
      from sklearn.feature_extraction.text import TfidfVectorizer
      from sklearn.metrics.pairwise import cosine_similarity

      # Sample documents
      docs = [
          "RISK-001: Data breach due to inadequate access controls",
          "RISK-002: Data leak from insufficient access restrictions",
          "CTRL-042: Implement multi-factor authentication"
      ]

      # Vectorize and calculate similarity
      vectorizer = TfidfVectorizer()
      tfidf_matrix = vectorizer.fit_transform(docs)
      similarities = cosine_similarity(tfidf_matrix)

      # Validate: RISK-001 and RISK-002 more similar than RISK-001 and CTRL-042
      assert similarities[0, 1] > similarities[0, 2]  # Similar risks
      assert 0 <= similarities[0, 1] <= 1  # Valid similarity range
  ```
- Test passes in <100ms
- **Validation:** `pytest tests/smoke/test_semantic_similarity.py -v`
- **UAT Required:** Yes - Validate similarity rankings make semantic sense

**AC-3.5.4-7: Model Caching Smoke Test Pass (P1)**
- Smoke test validates joblib model serialization/deserialization
- Test file: `tests/smoke/test_semantic_caching.py`
- Test logic:
  ```python
  def test_model_caching():
      import joblib
      from sklearn.feature_extraction.text import TfidfVectorizer
      from pathlib import Path
      import tempfile

      # Train model
      corpus = [...]  # Sample corpus
      vectorizer = TfidfVectorizer()
      vectorizer.fit(corpus)

      # Serialize model
      with tempfile.TemporaryDirectory() as tmpdir:
          model_path = Path(tmpdir) / "tfidf_model.joblib"
          joblib.dump(vectorizer, model_path)

          # Deserialize model
          loaded_vectorizer = joblib.load(model_path)

          # Validate loaded model works
          test_text = ["New audit finding"]
          original_output = vectorizer.transform(test_text)
          loaded_output = loaded_vectorizer.transform(test_text)

          assert (original_output != loaded_output).nnz == 0  # Identical output
  ```
- Test passes in <200ms
- **Validation:** `pytest tests/smoke/test_semantic_caching.py -v`
- **UAT Required:** No - Automated test sufficient

## Acceptance Criteria Trade-offs and Deferrals

**AC-3.5.4-1 Trade-off (gensim Optional):**
- **Issue:** gensim adds significant dependency weight (~50MB), may not be needed
- **Resolution:** Install gensim but mark as optional in Epic 4 (use if needed)
- **Rationale:** Better to have it available than add mid-Epic 4 (causes story delays)
- **Documented In:** Dev Notes, pyproject.toml comments

**AC-3.5.4-5, AC-3.5.4-6, AC-3.5.4-7 Priority Reduction:**
- **Issue:** LSA, similarity, caching tests are validation not critical path
- **Resolution:** Reduce from P0 to P1 (important but not blocking)
- **Rationale:** Import and TF-IDF tests (P0) catch 90% of issues; advanced tests are confidence builders
- **Documented In:** AC priority levels

## Tasks / Subtasks

### Task 1: Add Dependencies to pyproject.toml (AC: #3.5.4-1)
- [ ] Open `pyproject.toml` in editor
- [ ] Add to `[project.dependencies]` section:
  ```toml
  "scikit-learn>=1.3.0,<2.0",
  "joblib>=1.3.0,<2.0",
  "gensim>=4.3.0,<5.0",  # Optional - advanced NLP for Epic 4
  ```
- [ ] Add comment explaining why dependencies added (Epic 4 prep)
- [ ] Save file
- [ ] Install dependencies: `pip install -e ".[dev]"`
- [ ] Verify installation: `pip freeze | grep -E "(scikit-learn|joblib|gensim)"`

### Task 2: Run Security Audit (AC: #3.5.4-2)
- [ ] Install pip-audit if not present: `pip install pip-audit`
- [ ] Run security scan: `pip-audit --desc`
- [ ] Review vulnerability report
- [ ] If critical/high vulnerabilities: Research and resolve before continuing
- [ ] If medium/low vulnerabilities: Document in GitHub issue, defer to backlog
- [ ] Add pip-audit to CI/CD pipeline (.github/workflows/ci.yml)

### Task 3: Create Smoke Test Directory (AC: All)
- [ ] Create `tests/smoke/` directory if not exists
- [ ] Create `tests/smoke/__init__.py` (empty file for package)
- [ ] Add pytest marker to `pyproject.toml`:
  ```toml
  [tool.pytest.ini_options]
  markers = [
      "smoke: Smoke tests for quick validation",
      # ... existing markers
  ]
  ```

### Task 4: Create Import Smoke Tests (AC: #3.5.4-3)
- [ ] Create `tests/smoke/test_semantic_imports.py`
- [ ] Add test_sklearn_imports() function
- [ ] Add test_joblib_imports() function
- [ ] Add test_gensim_imports() function (mark as optional if fails)
- [ ] Add docstrings explaining what each test validates
- [ ] Run tests: `pytest tests/smoke/test_semantic_imports.py -v`

### Task 5: Create TF-IDF Smoke Test (AC: #3.5.4-4)
- [ ] Create `tests/smoke/test_semantic_tfidf.py`
- [ ] Create sample audit document corpus (3-5 snippets from Epic 2/3 fixtures)
- [ ] Implement test_tfidf_vectorization() function
- [ ] Validate TF-IDF matrix shape and sparsity
- [ ] Add assertions for vocabulary size, feature count
- [ ] Run test: `pytest tests/smoke/test_semantic_tfidf.py -v`

### Task 6: Create LSA Smoke Test (AC: #3.5.4-5)
- [ ] Create `tests/smoke/test_semantic_lsa.py`
- [ ] Create sample corpus (10+ audit snippets for dimensionality reduction)
- [ ] Implement test_lsa_decomposition() function
- [ ] Validate LSA output shape, explained variance ratio
- [ ] Add assertions for component count, matrix dimensions
- [ ] Run test: `pytest tests/smoke/test_semantic_lsa.py -v`

### Task 7: Create Cosine Similarity Smoke Test (AC: #3.5.4-6)
- [ ] Create `tests/smoke/test_semantic_similarity.py`
- [ ] Create sample documents with known similarity relationships
  - Similar: RISK-001 (data breach) and RISK-002 (data leak)
  - Dissimilar: RISK-001 and CTRL-042 (authentication control)
- [ ] Implement test_cosine_similarity() function
- [ ] Validate similarity rankings match semantic expectations
- [ ] Add assertions for similarity range [0, 1], ranking order
- [ ] Run test: `pytest tests/smoke/test_semantic_similarity.py -v`

### Task 8: Create Model Caching Smoke Test (AC: #3.5.4-7)
- [ ] Create `tests/smoke/test_semantic_caching.py`
- [ ] Implement test_model_caching() function with tempfile
- [ ] Train TF-IDF vectorizer on sample corpus
- [ ] Serialize model with joblib.dump()
- [ ] Deserialize model with joblib.load()
- [ ] Validate loaded model produces identical output
- [ ] Run test: `pytest tests/smoke/test_semantic_caching.py -v`

### Task 9: Update CI/CD Pipeline (AC: #3.5.4-2)
- [ ] Open `.github/workflows/ci.yml`
- [ ] Add pip-audit step to security-scan job (or create new job)
- [ ] Configure pip-audit to fail on critical/high vulnerabilities
- [ ] Add smoke test step: `pytest -m smoke -v`
- [ ] Commit CI/CD changes

### Task 10: Quality Gates and UAT
- [ ] Run all smoke tests: `pytest -m smoke -v` (all pass)
- [ ] Run full test suite: `pytest` (no regressions)
- [ ] Check coverage impact: `pytest --cov=src --cov-report=term`
- [ ] Run pre-commit hooks: `pre-commit run --all-files`
- [ ] UAT: Security team reviews pip-audit report (AC-3.5.4-2)
- [ ] UAT: Data scientist validates TF-IDF output for audit domain (AC-3.5.4-4)
- [ ] UAT: Data scientist validates similarity rankings (AC-3.5.4-6)

### Task 11: Documentation Updates
- [ ] Update CLAUDE.md Technology Stack table with scikit-learn, joblib, gensim
- [ ] Update CLAUDE.md Dependencies section with semantic libraries
- [ ] Add smoke test section to CLAUDE.md Testing Strategy
- [ ] Reference Story 3.5.4 in dependency-audit-process.md as example
- [ ] Update docs/sprint-status.yaml to reflect Epic 3.5 progress

## Dev Notes

**Provenance Tracking:**
- Smoke tests are validation code (no source_hash needed)
- pyproject.toml dependency versions create provenance (pinned ranges)
- Document scikit-learn/joblib/gensim versions in tech-spec-epic-4.md

**Structured Logging:**
- No structured logging needed (smoke tests, not pipeline code)
- Smoke test output captured by pytest (stdout/stderr)

**Pipeline Wiring:**
- Semantic dependencies will be used in Epic 4 semantic analysis stage
- Integration point: src/data_extract/semantic/ module (Epic 4)
- No wiring needed in Epic 3.5 (preparation only)

**Quality Gates:**
```bash
# Run smoke tests
pytest -m smoke -v

# Run all tests (ensure no regressions)
pytest

# Check coverage (should not drop)
pytest --cov=src --cov-report=term

# Security audit
pip-audit --desc

# Pre-commit hooks
pre-commit run --all-files
```

**Performance Considerations:**
- **Import overhead:** scikit-learn import ~500ms first time (acceptable for Epic 4 prep)
- **Smoke test execution:** <1 second total (all tests combined)
- **No performance baselines needed:** Smoke tests are not critical path
- **Epic 4 will establish:** TF-IDF/LSA performance baselines for large corpora

**Dependencies Added:**
```toml
[project.dependencies]
# ... existing dependencies ...
# Epic 4 Semantic Analysis (Added in Story 3.5.4)
"scikit-learn>=1.3.0,<2.0",  # TF-IDF, LSA, cosine similarity
"joblib>=1.3.0,<2.0",         # Model serialization for caching
"gensim>=4.3.0,<5.0",         # Optional - Word2Vec, LDA for advanced NLP
```

**Testing Strategy:**
- Smoke tests validate dependency installation and basic functionality
- NOT comprehensive semantic analysis tests (Epic 4 will add those)
- Focus: Import success, basic operations work, no crashes
- Execution time: <1 second for all smoke tests (fast feedback)

**Audit Domain Sample Corpus:**
Use Epic 2/3 fixture data or create realistic audit document snippets:
- Risk findings: "RISK-001: Data breach due to inadequate access controls"
- Control descriptions: "CTRL-042: Implement multi-factor authentication for all users"
- Compliance findings: "FIND-123: Incomplete audit trail violates SOC2 requirements"
- Recommendations: "REC-456: Enhance encryption for data at rest and in transit"

**Edge Cases:**
- **Platform differences:** scikit-learn builds may differ (Windows vs Linux vs macOS)
  - Resolution: Test in CI/CD across platforms
- **Version conflicts:** scikit-learn may conflict with existing dependencies
  - Resolution: Use pip-tools or poetry for dependency resolution
- **Import failures:** Module not found errors
  - Resolution: Verify installation with `pip list`, reinstall if needed
- **Numerical precision:** Similarity scores may vary slightly across platforms
  - Resolution: Use tolerance in assertions (e.g., `assert abs(sim - 0.95) < 0.01`)

**Retrospective Learnings Applied:**
- This story directly addresses "Add semantic dependencies + smoke test" (Epic 3 retro prep task)
- Follows dependency audit process from Story 3.5.3 (documented process)
- Prevents Epic 4 first-story blockers (dependencies ready upfront)
- Uses smoke tests for fast validation (Epic 2.5.4 CI/CD enhancement pattern)

**Next Story Dependencies:**
- Story 3.5.5 (Model/Cache ADR) will reference joblib caching validated here
- Story 3.5.6 (Semantic QA fixtures) will use semantic dependencies for test data
- Story 3.5.7 (TF-IDF/LSA playbook) will reference smoke tests as examples
- Epic 4 stories will assume semantic dependencies are installed and validated

**gensim Optional Justification:**
- **If Epic 4 needs advanced NLP (Word2Vec, LDA):** gensim already installed
- **If Epic 4 only needs TF-IDF/LSA:** gensim installed but unused (minor overhead)
- **Trade-off:** ~50MB dependency weight vs mid-Epic 4 installation delay
- **Resolution:** Install now (better safe than sorry for 2.5-day prep sprint)
