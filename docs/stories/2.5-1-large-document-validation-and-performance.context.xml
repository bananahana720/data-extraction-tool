<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2.5</epicId>
    <storyId>1</storyId>
    <title>Performance Validation & Optimization</title>
    <status>drafted</status>
    <generatedAt>2025-11-12</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/2.5-1-large-document-validation-and-performance.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>audit professional and DevOps engineer</asA>
    <iWant>automated performance validation that the Extract &amp; Normalize pipeline meets NFR-P1 (&lt;10 min for 100 files) and NFR-P2 (&lt;2GB memory) requirements</iWant>
    <soThat>I have confidence the system is production-ready and established baselines prevent future performance regressions</soThat>
    <tasks>
      - Task 1: Create 100-file performance test batch (AC: #2.5.1.1)
      - Task 2: Install profiling dependencies (AC: #2.5.1.2, #2.5.1.3)
      - Task 3: Run baseline performance test and profiling (AC: #2.5.1.1, #2.5.1.2, #2.5.1.3)
      - Task 4: Identify and document bottlenecks (AC: #2.5.1.3)
      - Task 5: Optimize critical bottlenecks (AC: #2.5.1.4)
      - Task 6: Create automated performance test suite (AC: #2.5.1.5)
      - Task 7: Configure CI performance job (AC: #2.5.1.5)
      - Task 8: Document baseline metrics (AC: #2.5.1.6)
      - Task 9: Comprehensive testing and validation (AC: all)
      - Task 10: Documentation and completion (AC: all)
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="2.5.1.1">
      100-file batch processes within NFR-P1 target:
      - 100 mixed-format files (PDFs, DOCX, XLSX) process in &lt;10 minutes
      - Measured with real timer, not estimates
      - Test batch includes variety of file sizes and formats
      - Sustained throughput ~10 files/minute
    </criterion>
    <criterion id="2.5.1.2">
      Memory usage stays within NFR-P2 limits:
      - Peak memory &lt;2GB (2048MB) during 100-file batch processing
      - Measured with psutil process monitoring
      - No memory leaks detected (memory returns to baseline after batch)
      - Memory monitoring validates ADR-005 streaming architecture
    </criterion>
    <criterion id="2.5.1.3">
      Performance bottlenecks identified and documented:
      - cProfile run generates profile.stats file
      - Top 10 slowest functions identified with line numbers
      - Bottlenecks documented in Story 2.5.1 completion notes
      - Call graph analysis shows critical path timing
    </criterion>
    <criterion id="2.5.1.4">
      Critical bottlenecks optimized:
      - Slowest operation(s) improved by measurable amount (&gt;10% improvement)
      - Performance improvement validated with before/after benchmarks
      - No regressions introduced in optimization (all 307+ tests still pass)
      - Optimization documented with rationale
    </criterion>
    <criterion id="2.5.1.5">
      Automated performance test suite created:
      - tests/performance/test_throughput.py exists and runs successfully
      - Tests validate NFR-P1 (throughput) and NFR-P2 (memory)
      - Performance CI job configured (weekly schedule, not every commit)
      - Tests use @pytest.mark.performance marker
    </criterion>
    <criterion id="2.5.1.6">
      Baseline metrics documented:
      - Throughput (files/min), memory (peak/avg), CPU utilization recorded
      - Baseline documented in test docstrings or docs/performance-baselines.md
      - Future epics can compare against baseline to detect regressions (&gt;10% degradation triggers investigation)
      - Hardware specs and test conditions documented with baseline
    </criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-2.5.md</path>
        <title>Tech Spec Epic 2.5</title>
        <section>Story 2.5.1 Requirements</section>
        <snippet>Install profiling tools (cProfile, memory_profiler, psutil). Create 100-file test batch (mix of PDFs, DOCX, XLSX). Run baseline performance test and profile execution.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-2.5.md</path>
        <title>Tech Spec Epic 2.5</title>
        <section>NFR-P1: Batch Processing Throughput</section>
        <snippet>Target: 100 mixed-format files in &lt;10 minutes. Current state unknown - Story 2.5.1 establishes baseline. Validation via automated performance tests in CI (weekly runs).</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-2.5.md</path>
        <title>Tech Spec Epic 2.5</title>
        <section>NFR-P2: Memory Footprint</section>
        <snippet>Target: Peak memory &lt;2GB during batch processing. Story 2.5.1 validates streaming architecture. Measured with psutil memory monitoring in integration tests.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-2.5.md</path>
        <title>Tech Spec Epic 2.5</title>
        <section>NFR-P3: Individual File Processing</section>
        <snippet>Target: &lt;5 seconds per document (excluding OCR), &lt;10 seconds per scanned page. Assumed compliant from Epic 2 - Story 2.5.1 confirms.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>NFR-P1: Batch Processing Throughput</section>
        <snippet>Process 100 mixed-format files in &lt;10 minutes on typical workstation (Intel i5/i7, 16GB RAM, SSD). Individual file processing: &lt;5 seconds per document (excluding OCR).</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>NFR-P2: Memory Efficiency</section>
        <snippet>Maximum memory footprint: 2GB RAM during batch processing. Streaming processing architecture (don't load all files into memory). Memory released after each file processed.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>ADR-005: Streaming Pipeline</section>
        <snippet>Decision: Process files one at a time through pipeline, release memory after each. Consequences: Constant memory usage (2GB max), can process arbitrarily large batches, graceful error handling.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>ADR-006: Continue-On-Error Batch Processing</section>
        <snippet>Decision: Catch per-file errors, log, quarantine, continue with remaining files. Consequences: Resilient batch processing, detailed error reporting, user can re-run only failed files.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>Target Performance</section>
        <snippet>100 mixed files in &lt;10 minutes (sustained throughput: ~10 files/min). Individual file processing: &lt;5 seconds (excluding OCR). Max memory footprint: 2GB during batch processing.</snippet>
      </doc>
      <doc>
        <path>docs/retrospectives/epic-2-retro-20250111.md</path>
        <title>Epic 2 Retrospective</title>
        <section>Missing Performance Validation</section>
        <snippet>Epic 2 pipeline tested with small sample documents only. No validation with 50-100 page documents (realistic audit report size). Unknown bottlenecks in entity normalization, validation, or metadata enrichment.</snippet>
      </doc>
      <doc>
        <path>docs/retrospectives/epic-2-retro-20250111.md</path>
        <title>Epic 2 Retrospective</title>
        <section>Bridge Epic 2.5 Creation Rationale</section>
        <snippet>Proactively addressing infrastructure gaps in a bridge epic prevents mid-epic surprises and timeline disruptions. Investing 1.5 days in foundation work (performance validation, spaCy setup) prevents days of debugging during Epic 3.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/pipeline/batch_processor.py</path>
        <kind>controller</kind>
        <symbol>BatchProcessor.process_batch()</symbol>
        <lines>49-317</lines>
        <reason>Main batch processing orchestrator using thread pool executor. Critical for NFR-P1 throughput validation (100 files in &lt;10 min). Profile parallelization efficiency and memory scaling with worker count.</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/core/pipeline.py</path>
        <kind>protocol</kind>
        <symbol>PipelineStage, Pipeline.process()</symbol>
        <lines>20-129</lines>
        <reason>Core pipeline orchestration protocol. Profile stage coordination overhead and context propagation cost during chaining.</reason>
      </artifact>
      <artifact>
        <path>src/extractors/pdf_extractor.py</path>
        <kind>extractor</kind>
        <symbol>PdfExtractor.extract(), _extract_text_from_pdf(), _perform_ocr()</symbol>
        <lines>1-877</lines>
        <reason>CRITICAL BOTTLENECK CANDIDATE. Most complex extractor with native vs OCR code paths. Profile separately for native (&lt;2s/MB target) vs OCR (&lt;15s/page target) performance.</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/normalize/cleaning.py</path>
        <kind>service</kind>
        <symbol>TextCleaner.clean_text(), remove_ocr_artifacts()</symbol>
        <lines>50-300</lines>
        <reason>First normalize stage with regex pattern matching. Profile pattern compilation vs matching time. Test scaling with document size and OCR artifact density.</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/normalize/entities.py</path>
        <kind>service</kind>
        <symbol>EntityNormalizer.process(), recognize_entity_type()</symbol>
        <lines>44-491</lines>
        <reason>Second normalize stage with 6 entity types and context window analysis. Profile pattern matching scaling with text length and entity density.</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/normalize/validation.py</path>
        <kind>service</kind>
        <symbol>QualityValidator.process(), validate_ocr_confidence(), _preprocess_image()</symbol>
        <lines>34-876</lines>
        <reason>CRITICAL BOTTLENECK CANDIDATE. pytesseract OCR confidence calls are SLOW (15s+ per page for scanned documents). Profile image preprocessing vs pytesseract separately. Fourth normalize stage.</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/normalize/metadata.py</path>
        <kind>service</kind>
        <symbol>MetadataEnricher.enrich_metadata(), calculate_file_hash()</symbol>
        <lines>1-321</lines>
        <reason>Fifth normalize stage. File I/O hashing (SHA-256 chunked) and dict aggregation. Should have minimal overhead but verify with large files. Profile hashing time separately.</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/normalize/normalizer.py</path>
        <kind>orchestrator</kind>
        <symbol>Normalizer.process()</symbol>
        <lines>25-423</lines>
        <reason>Main coordinator for all 5 normalize stages. Profile each stage separately and in chain to identify stage interaction overhead. Tests continue-on-error pattern (ADR-006).</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/core/models.py</path>
        <kind>model</kind>
        <symbol>Document, Metadata, ProcessingContext, ValidationReport</symbol>
        <lines>1-359</lines>
        <reason>Pydantic v2 data models. Profile model serialization/deserialization overhead and metadata accumulation in ProcessingContext.</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/normalize/config.py</path>
        <kind>config</kind>
        <symbol>NormalizationConfig, load_config()</symbol>
        <lines>23-318</lines>
        <reason>Configuration cascade loader with 4-tier precedence. Test YAML parsing overhead and performance impact of different config combinations.</reason>
      </artifact>
      <artifact>
        <path>tests/performance/conftest.py</path>
        <kind>test</kind>
        <symbol>PerformanceMeasurement, BenchmarkResult</symbol>
        <lines>1-100</lines>
        <reason>Existing performance measurement infrastructure using time.perf_counter() and tracemalloc. Will extend for normalization stages.</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="psutil" version="&gt;=5.9.0,&lt;6.0">Process memory monitoring for NFR-P2 validation</package>
        <package name="memory_profiler" version="&gt;=0.61.0,&lt;1.0">Optional line-by-line memory profiling</package>
        <package name="cProfile" version="built-in">Python standard profiling tool for bottleneck identification</package>
        <package name="pytesseract" version="existing">OCR engine (known bottleneck candidate)</package>
        <package name="pymupdf" version="existing">PDF extraction library</package>
        <package name="python-docx" version="existing">DOCX extraction</package>
        <package name="openpyxl" version="existing">Excel extraction</package>
        <package name="python-pptx" version="existing">PPTX extraction</package>
        <package name="pydantic" version="v2">Data validation and serialization</package>
        <package name="pytest" version="existing">Test framework with @pytest.mark.performance marker</package>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>ADR-005 Streaming Pipeline: Process files one at a time, release memory after each. Must validate constant memory usage &lt;2GB regardless of batch size. This is a CRITICAL validation - if it fails, may require Epic 1/2 rework.</constraint>
    <constraint>ADR-006 Continue-On-Error: Catch per-file errors, log, quarantine, continue with remaining files. Test with some corrupted files in batch to verify graceful degradation.</constraint>
    <constraint>Performance Optimization Hierarchy: (1) OCR pytesseract ~10s/page highest impact, (2) spaCy sentence segmentation ~0.5s/doc (Story 2.5.2), (3) TF-IDF vectorization ~1s/100 docs (Epic 4), (4) File I/O.</constraint>
    <constraint>Avoid Premature Optimization: Profile FIRST with cProfile to identify actual bottlenecks, THEN optimize. Do not optimize based on assumptions.</constraint>
    <constraint>No Breaking Changes: Any optimizations must maintain all 307+ existing tests passing. No regressions in Black/Ruff/Mypy compliance.</constraint>
    <constraint>Memory Efficiency Patterns: Streaming processing (one file at a time), sparse matrices for TF-IDF, avoid loading entire corpus into RAM, use generator patterns for batch processing.</constraint>
    <constraint>Error Handling (ADR-006): ProcessingError (recoverable) - log, quarantine, continue. CriticalError (unrecoverable) - halt immediately. Monitor memory pressure with psutil.</constraint>
    <constraint>Configuration Cascade: CLI flags &gt; Environment variables (DATA_EXTRACT_*) &gt; YAML config &gt; defaults. Performance tests should use default configuration (no special tuning).</constraint>
    <constraint>Logging Pattern: Use structlog with structured JSON. Include timestamp, file_path, processing_time_ms, memory_mb, bottleneck_detected. Performance logs separate from operational logs.</constraint>
    <constraint>Quality Gates: Black formatting (100 char lines), Ruff linting (0 errors), Mypy strict mode for src/data_extract/, All 307+ tests passing (0 failures).</constraint>
  </constraints>
  <interfaces>
    <interface>
      <name>PipelineStage Protocol</name>
      <kind>Protocol</kind>
      <signature>def process(self, input_data: Input, context: ProcessingContext) -&gt; Output</signature>
      <path>src/data_extract/core/pipeline.py</path>
      <description>All normalize stages implement this protocol. Profile coordination overhead during stage chaining.</description>
    </interface>
    <interface>
      <name>BatchProcessor.process_batch()</name>
      <kind>Method</kind>
      <signature>def process_batch(self, files: List[Path], config: Config) -&gt; BatchResult</signature>
      <path>src/pipeline/batch_processor.py</path>
      <description>Main entry point for batch processing. Uses ThreadPoolExecutor with configurable max_workers (default 4). Critical for NFR-P1 validation.</description>
    </interface>
    <interface>
      <name>Normalizer.process()</name>
      <kind>PipelineStage Implementation</kind>
      <signature>def process(self, doc: Document, context: ProcessingContext) -&gt; Document</signature>
      <path>src/data_extract/normalize/normalizer.py</path>
      <description>Main normalizer orchestrating 5 stages: TextCleaner, EntityNormalizer, SchemaStandardizer, QualityValidator, MetadataEnricher. Profile each stage separately.</description>
    </interface>
    <interface>
      <name>PerformanceMeasurement Context Manager</name>
      <kind>Test Utility</kind>
      <signature>with PerformanceMeasurement(operation_name) as pm: ...</signature>
      <path>tests/performance/conftest.py</path>
      <description>Existing timing and memory measurement infrastructure using time.perf_counter() and tracemalloc. Extend for normalization benchmarks.</description>
    </interface>
  </interfaces>
  <tests>
    <standards>
      Performance tests organized in tests/performance/ directory (NEW). Use @pytest.mark.performance marker for selective execution.
      Tests run weekly in CI (not every commit - too slow ~10+ minutes). Manual execution via 'pytest -m performance'.
      Test classes: TestBatchThroughput (AC-2.5.1.1), TestMemoryUsage (AC-2.5.1.2), TestPerformanceRegression (AC-2.5.1.6).
      No specific coverage target for performance tests - focus on validation, not coverage.
      Regression coverage: Ensure all 307+ existing tests still pass (0 regressions).
      Quality gates maintained: Black formatting (100 char lines), Ruff linting (0 errors), Mypy strict mode for src/data_extract/.
      Performance test fixtures: 100-file batch (40 PDFs, 30 DOCX, 20 XLSX, 10 mixed). Document composition in tests/performance/README.md.
    </standards>
    <locations>
      tests/performance/test_throughput.py (NEW) - Main performance test suite
      tests/performance/README.md (NEW) - Test batch composition documentation
      tests/performance/conftest.py (EXISTING) - Performance measurement fixtures
      scripts/profile_pipeline.py (NEW) - One-time profiling script for baseline
      .github/workflows/performance.yml (NEW) - Weekly CI job configuration
    </locations>
    <ideas>
      <idea ac="2.5.1.1">
        test_batch_throughput_100_files(): Process 100 mixed-format files through Extract &amp; Normalize pipeline with timer and psutil monitoring.
        Assert elapsed_time &lt; 10 minutes, throughput &gt;= 10 files/min. Validate with real timer, not estimates.
      </idea>
      <idea ac="2.5.1.2">
        test_memory_usage_within_limits(): Monitor peak memory during 100-file batch with psutil.Process().memory_info().rss.
        Assert peak_memory &lt; 2048MB (2GB). Test ADR-005 streaming architecture validation.
      </idea>
      <idea ac="2.5.1.2">
        test_no_memory_leaks(): Measure memory before batch, after batch, assert memory returns to baseline (within 10% tolerance).
        Validates memory release after file processing.
      </idea>
      <idea ac="2.5.1.3">
        Manual profiling with cProfile: Run 'python -m cProfile -o profile.stats scripts/profile_pipeline.py'.
        Analyze with pstats: 'sort cumtime' and 'stats 10' to identify top 10 bottlenecks with line numbers.
      </idea>
      <idea ac="2.5.1.4">
        Optimization benchmarks: Run before/after performance tests for critical bottlenecks.
        Measure % improvement (&gt;10% required). Ensure all 307+ tests still pass (no regressions).
      </idea>
      <idea ac="2.5.1.5">
        test_performance_ci_job_configured(): Verify .github/workflows/performance.yml exists with weekly schedule (cron: '0 2 * * 1').
        Check job steps: checkout, setup Python, install deps, run pytest -m performance, upload artifacts.
      </idea>
      <idea ac="2.5.1.6">
        test_baseline_metrics_documented(): Verify baseline metrics in test docstrings or docs/performance-baselines.md.
        Check includes: throughput (files/min), peak/avg memory (MB), CPU %, hardware specs, date, commit hash, regression threshold (&gt;10% degradation).
      </idea>
      <idea ac="all">
        Integration with existing tests: Run full test suite 'pytest' to ensure 0 regressions in 307+ tests.
        Run 'pytest -m performance' for performance validation. Separate performance logs from operational logs.
      </idea>
    </ideas>
  </tests>
</story-context>
