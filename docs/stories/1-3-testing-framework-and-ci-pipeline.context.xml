<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>3</storyId>
    <title>Testing Framework and CI Pipeline</title>
    <status>drafted</status>
    <generatedAt>2025-11-10</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/1-3-testing-framework-and-ci-pipeline.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>developer</asA>
    <iWant>a comprehensive testing framework with CI automation</iWant>
    <soThat>I can develop confidently with automated quality checks</soThat>
    <tasks>
      <task id="1" acs="1,3">
        <description>Configure pytest testing infrastructure</description>
        <subtasks>
          <subtask id="1.1">Create pytest.ini with test discovery configuration</subtask>
          <subtask id="1.2">Set coverage thresholds (target: >80%)</subtask>
          <subtask id="1.3">Configure test markers (unit, integration, performance)</subtask>
          <subtask id="1.4">Create tests/ directory structure mirroring src/</subtask>
          <subtask id="1.5">Create tests/__init__.py and tests/conftest.py</subtask>
        </subtasks>
      </task>
      <task id="2" acs="2">
        <description>Create test fixtures for document formats</description>
        <subtasks>
          <subtask id="2.1">Create tests/fixtures/ directory with subdirectories (pdfs/, docx/, xlsx/, images/, archer/)</subtask>
          <subtask id="2.2">Add sample PDF file (<100KB, sanitized)</subtask>
          <subtask id="2.3">Add sample Word document (<100KB)</subtask>
          <subtask id="2.4">Add sample Excel file (<100KB)</subtask>
          <subtask id="2.5">Add sample image for OCR testing (<100KB)</subtask>
          <subtask id="2.6">Document fixture contents in tests/fixtures/README.md</subtask>
        </subtasks>
      </task>
      <task id="3" acs="6">
        <description>Set up pytest-cov coverage tracking</description>
        <subtasks>
          <subtask id="3.1">Configure pytest-cov in pytest.ini</subtask>
          <subtask id="3.2">Test coverage generation (pytest --cov command)</subtask>
          <subtask id="3.3">Verify HTML report generation</subtask>
          <subtask id="3.4">Run coverage on brownfield codebase to establish baseline</subtask>
          <subtask id="3.5">Document baseline coverage metrics</subtask>
        </subtasks>
      </task>
      <task id="4" acs="4">
        <description>Create example integration test</description>
        <subtasks>
          <subtask id="4.1">Create tests/integration/test_pipeline_basic.py</subtask>
          <subtask id="4.2">Implement fixture loading test</subtask>
          <subtask id="4.3">Implement Document object creation test</subtask>
          <subtask id="4.4">Verify test passes with pytest</subtask>
          <subtask id="4.5">Document integration test patterns</subtask>
        </subtasks>
      </task>
      <task id="5" acs="7">
        <description>Configure pre-commit hooks</description>
        <subtasks>
          <subtask id="5.1">Create .pre-commit-config.yaml</subtask>
          <subtask id="5.2">Configure black hook (code formatting)</subtask>
          <subtask id="5.3">Configure ruff hook (linting)</subtask>
          <subtask id="5.4">Configure mypy hook (type checking)</subtask>
          <subtask id="5.5">Install pre-commit hooks (pre-commit install)</subtask>
          <subtask id="5.6">Test pre-commit hooks on sample files</subtask>
          <subtask id="5.7">Document pre-commit usage in README</subtask>
        </subtasks>
      </task>
      <task id="6" acs="5">
        <description>Set up CI pipeline configuration</description>
        <subtasks>
          <subtask id="6.1">Create .github/workflows/test.yml (or equivalent CI config)</subtask>
          <subtask id="6.2">Configure pytest execution step</subtask>
          <subtask id="6.3">Configure coverage reporting step</subtask>
          <subtask id="6.4">Configure ruff linting step</subtask>
          <subtask id="6.5">Configure mypy type checking step</subtask>
          <subtask id="6.6">Configure black formatting check step</subtask>
          <subtask id="6.7">Test CI pipeline on test commit</subtask>
        </subtasks>
      </task>
      <task id="7" acs="1,2,3,4,5,6,7">
        <description>Validate all acceptance criteria</description>
        <subtasks>
          <subtask id="7.1">Verify pytest configuration works (pytest --collect-only)</subtask>
          <subtask id="7.2">Verify all test fixtures are accessible</subtask>
          <subtask id="7.3">Verify test structure organization</subtask>
          <subtask id="7.4">Run integration tests successfully</subtask>
          <subtask id="7.5">Trigger CI pipeline and verify all checks pass</subtask>
          <subtask id="7.6">Generate coverage report and document baseline</subtask>
          <subtask id="7.7">Test pre-commit hooks block commits with quality issues</subtask>
        </subtasks>
      </task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="AC-1.3.1">
      <description>pytest is configured with comprehensive settings</description>
      <conditions>
        <condition>pytest.ini exists with test discovery paths</condition>
        <condition>Coverage thresholds defined (target: >80%)</condition>
        <condition>Test markers configured (unit, integration, performance)</condition>
      </conditions>
    </criterion>
    <criterion id="AC-1.3.2">
      <description>Test fixtures exist for document formats</description>
      <conditions>
        <condition>Sample PDF (sanitized, no sensitive data)</condition>
        <condition>Sample Word document</condition>
        <condition>Sample Excel file</condition>
        <condition>Sample image for OCR</condition>
        <condition>All fixtures <100KB, stored in tests/fixtures/</condition>
      </conditions>
    </criterion>
    <criterion id="AC-1.3.3">
      <description>Test structure mirrors src/ organization</description>
      <conditions>
        <condition>tests/unit/ with subdirectories matching src/</condition>
        <condition>tests/integration/ for end-to-end tests</condition>
        <condition>tests/performance/ for benchmarking</condition>
        <condition>tests/conftest.py with shared fixtures</condition>
      </conditions>
    </criterion>
    <criterion id="AC-1.3.4">
      <description>Integration test framework functional</description>
      <conditions>
        <condition>Can load test fixtures</condition>
        <condition>Can create mock Document objects</condition>
        <condition>Example integration test passes</condition>
      </conditions>
    </criterion>
    <criterion id="AC-1.3.5">
      <description>CI pipeline runs on every commit</description>
      <conditions>
        <condition>GitHub Actions (or equivalent) configuration file</condition>
        <condition>Runs: pytest, coverage, ruff, mypy, black --check</condition>
        <condition>Reports results to PR/commit status</condition>
      </conditions>
    </criterion>
    <criterion id="AC-1.3.6">
      <description>Code coverage tracked</description>
      <conditions>
        <condition>pytest-cov generates HTML and terminal reports</condition>
        <condition>Coverage percentage displayed in CI logs</condition>
        <condition>Baseline coverage established (even if low initially)</condition>
      </conditions>
    </criterion>
    <criterion id="AC-1.3.7">
      <description>Pre-commit hooks enforce code quality</description>
      <conditions>
        <condition>.pre-commit-config.yaml configured</condition>
        <condition>Hooks: black (formatting), ruff (linting), mypy (type checking)</condition>
        <condition>Hooks run automatically on git commit</condition>
      </conditions>
    </criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic 1 Technical Specification</title>
        <section>Story 1.3: Testing Framework and CI Pipeline</section>
        <snippet>Configure pytest with pytest.ini (test discovery, coverage thresholds >80%). Set up test structure mirroring src/ (unit/, integration/, performance/). Create conftest.py with shared fixtures. CI pipeline with GitHub Actions running pytest, coverage, ruff, mypy, black.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic 1 Technical Specification</title>
        <section>Test Strategy Summary</section>
        <snippet>Unit Tests: >90% coverage for core modules. Integration Tests: All major workflows (extract → normalize → chunk → output). Performance Tests: Baseline metrics. Coverage Strategy: Phase 1 (Epic 1) = >60% baseline; Phase 2 (Epics 2-4) = >80%.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Documentation</title>
        <section>Development Environment Setup</section>
        <snippet>Run tests to verify setup: pytest tests/ -v. Development environment includes: pytest, pytest-cov, black, mypy, ruff, pre-commit. Performance targets: Unit test suite execution <30 seconds, pytest test discovery <2 seconds.</snippet>
      </doc>
      <doc>
        <path>docs/brownfield-assessment.md</path>
        <title>Brownfield Codebase Assessment</title>
        <section>Testing Summary and Coverage Assessment</section>
        <snippet>Total tests: 1007. Passing: 778 (77%). Failing: 229 (23%). Coverage: Unknown (never run). Critical debt: Run coverage analysis pytest --cov=src, fix 229 failing tests, target 80% coverage for brownfield code before Epic 2.</snippet>
      </doc>
      <doc>
        <path>docs/stories/1-2-brownfield-codebase-assessment.md</path>
        <title>Story 1.2: Brownfield Codebase Assessment</title>
        <section>Learnings and Constraints Discovered</section>
        <snippet>Test coverage gap identified: 229 failing tests (23%) must be analyzed. Coverage metrics unknown - pytest --cov NOT yet run. Assessment marked this as CRITICAL technical debt requiring immediate action in Story 1.3.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>pytest.ini</path>
        <kind>config</kind>
        <symbol>pytest configuration</symbol>
        <lines>1-78</lines>
        <reason>Existing pytest.ini with test discovery patterns and markers (unit, integration, slow, performance, extraction, processing, formatting, pipeline, cli, edge_case, stress, infrastructure, cross_format)</reason>
      </artifact>
      <artifact>
        <path>pyproject.toml</path>
        <kind>config</kind>
        <symbol>[tool.pytest.ini_options]</symbol>
        <lines>102-107</lines>
        <reason>Pytest configuration with testpaths and coverage settings; includes dev dependencies pytest>=8.0.0, pytest-cov>=5.0.0, pytest-xdist>=3.6.0, pytest-mock>=3.11.0</reason>
      </artifact>
      <artifact>
        <path>tests/conftest.py</path>
        <kind>conftest</kind>
        <symbol>pytest_configure, global fixtures</symbol>
        <lines>1-426</lines>
        <reason>Global test configuration with 15+ shared fixtures: sample content blocks, document metadata, extraction results, processing results, temp files, validation helpers</reason>
      </artifact>
      <artifact>
        <path>tests/test_extractors/conftest.py</path>
        <kind>conftest</kind>
        <symbol>sample_pdf_file</symbol>
        <lines>12-58</lines>
        <reason>Extractor-specific fixtures including PDF generation with reportlab for testing</reason>
      </artifact>
      <artifact>
        <path>tests/test_cli/conftest.py</path>
        <kind>conftest</kind>
        <symbol>cli_runner, configured_pipeline, config_file</symbol>
        <lines>1-223</lines>
        <reason>CLI testing fixtures with Click CliRunner, sample files, configured pipeline with extractors/processors/formatters, batch processor, YAML config generation</reason>
      </artifact>
      <artifact>
        <path>tests/test_formatters/conftest.py</path>
        <kind>conftest</kind>
        <symbol>rich_processing_result, unicode_processing_result</symbol>
        <lines>1-342</lines>
        <reason>Formatter testing fixtures with various processing result scenarios: minimal, rich (all content types), empty, unicode/i18n, deeply nested, long content, failed results</reason>
      </artifact>
      <artifact>
        <path>tests/performance/conftest.py</path>
        <kind>conftest</kind>
        <symbol>perf_measure, baseline_manager</symbol>
        <lines>1-466</lines>
        <reason>Performance testing infrastructure with measurement context manager, baseline management, system specs, benchmark result factories, performance assertion helpers</reason>
      </artifact>
      <artifact>
        <path>tests/fixtures/</path>
        <kind>directory</kind>
        <symbol>test data files</symbol>
        <lines></lines>
        <reason>Test fixture directory containing real-world files (COBIT PDFs, NIST Excel/PDF, OWASP guides), Excel samples, DOCX/PPTX with tables/images</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="pytest" version=">=8.0.0" />
        <package name="pytest-cov" version=">=5.0.0" />
        <package name="pytest-xdist" version=">=3.6.0" />
        <package name="pytest-mock" version=">=3.11.0" />
        <package name="black" version="~=24.0" />
        <package name="ruff" version="~=0.6.0" />
        <package name="mypy" version="~=1.11.0" />
        <package name="pre-commit" version="~=3.0" />
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Test structure must mirror src/ directory organization (tests/unit/, tests/integration/, tests/performance/)</constraint>
    <constraint>All quality gates must pass: pytest, ruff, mypy, black before commit (enforced by pre-commit hooks)</constraint>
    <constraint>Coverage target: >80% overall (>60% baseline acceptable for Epic 1 brownfield code)</constraint>
    <constraint>Unit test suite execution must complete in <30 seconds</constraint>
    <constraint>Pytest test discovery must complete in <2 seconds</constraint>
    <constraint>CI pipeline must run on every commit and block PRs if any check fails</constraint>
    <constraint>Test fixtures must be <100KB each and sanitized (no sensitive data)</constraint>
    <constraint>Must address 229 failing tests identified in brownfield assessment before Epic 2</constraint>
    <constraint>Pre-commit hooks cannot be bypassed (--no-verify discouraged)</constraint>
    <constraint>Test markers must be used for selective execution (unit, integration, performance, slow)</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>pytest execution</name>
      <kind>CLI command</kind>
      <signature>pytest tests/ -v --cov=src --cov-report=html --cov-report=term</signature>
      <path>pytest.ini</path>
    </interface>
    <interface>
      <name>pytest markers</name>
      <kind>test filtering</kind>
      <signature>@pytest.mark.unit, @pytest.mark.integration, @pytest.mark.performance, @pytest.mark.slow</signature>
      <path>pytest.ini</path>
    </interface>
    <interface>
      <name>pre-commit hooks</name>
      <kind>git hooks</kind>
      <signature>pre-commit install; pre-commit run --all-files</signature>
      <path>.pre-commit-config.yaml</path>
    </interface>
    <interface>
      <name>GitHub Actions CI</name>
      <kind>CI pipeline</kind>
      <signature>Workflow triggers: on push, on pull_request; Jobs: test, lint, type-check, format-check</signature>
      <path>.github/workflows/test.yml</path>
    </interface>
    <interface>
      <name>pytest fixtures</name>
      <kind>test utilities</kind>
      <signature>Global fixtures in tests/conftest.py; specialized fixtures in subdirectory conftest.py files</signature>
      <path>tests/conftest.py</path>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Testing framework: pytest 8.x with pytest-cov for coverage, pytest-xdist for parallel execution, pytest-mock for mocking. Code quality: black 24.x (formatting), ruff 0.6.x (linting), mypy 1.11.x (type checking). Pre-commit hooks enforce all quality checks before commit. CI pipeline (GitHub Actions) runs on every commit with same checks. Test organization: Unit tests in tests/unit/ (>90% coverage target for core modules), integration tests in tests/integration/ (end-to-end workflows), performance tests in tests/performance/ (baseline benchmarking, not in regular CI). Fixture strategy: Global shared fixtures in tests/conftest.py, specialized fixtures in subdirectory conftest files. Test markers for selective execution: unit, integration, performance, slow, extraction, processing, formatting, pipeline, cli, edge_case, stress, infrastructure, cross_format. Coverage strategy: Epic 1 baseline >60%, Epic 2+ target >80%.</standards>

    <locations>
      <location>tests/unit/ - Fast isolated unit tests mirroring src/ structure</location>
      <location>tests/integration/ - End-to-end pipeline integration tests</location>
      <location>tests/performance/ - Baseline performance benchmarks</location>
      <location>tests/fixtures/ - Test data files (PDFs, DOCX, Excel, images, <100KB each)</location>
      <location>tests/conftest.py - Global shared pytest fixtures</location>
      <location>tests/*/conftest.py - Specialized fixtures per test category</location>
      <location>.github/workflows/test.yml - CI pipeline configuration</location>
      <location>.pre-commit-config.yaml - Pre-commit hook configuration</location>
      <location>pytest.ini - Pytest configuration and markers</location>
    </locations>

    <ideas>
      <idea ac="AC-1.3.1">Test pytest.ini configuration: Verify test discovery patterns work, markers are registered, coverage thresholds are enforced</idea>
      <idea ac="AC-1.3.2">Test fixture accessibility: Verify all test fixtures load correctly, are <100KB, and contain expected content</idea>
      <idea ac="AC-1.3.3">Test directory structure: Verify tests/ mirrors src/, all __init__.py exist, conftest.py loads</idea>
      <idea ac="AC-1.3.4">Test integration framework: Create test_pipeline_basic.py with fixture loading, Document object creation, verify passes</idea>
      <idea ac="AC-1.3.5">Test CI pipeline: Trigger workflow, verify pytest/coverage/ruff/mypy/black all execute, check PR status reporting</idea>
      <idea ac="AC-1.3.6">Test coverage tracking: Run pytest --cov, verify HTML report generated, terminal output shows percentage, baseline documented</idea>
      <idea ac="AC-1.3.7">Test pre-commit hooks: Make intentional quality violation, verify hooks block commit, test hook bypass discouraged</idea>
      <idea ac="AC-1.3.1,AC-1.3.6">Brownfield baseline: Run pytest --cov=src on existing 1007 tests, document baseline percentage, analyze 229 failing tests</idea>
      <idea ac="AC-1.3.4">Integration test patterns: Document conftest.py fixture usage, show pipeline configuration examples, demonstrate mock usage</idea>
      <idea ac="AC-1.3.5">CI performance: Verify unit test suite <30s, pytest discovery <2s, parallel execution with pytest-xdist</idea>
    </ideas>
  </tests>
</story-context>
