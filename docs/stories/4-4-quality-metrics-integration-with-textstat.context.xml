<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>4</epicId>
    <storyId>4</storyId>
    <title>Integrate Quality Metrics using Textstat for Content Assessment</title>
    <status>drafted</status>
    <generatedAt>2025-11-20</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/4-4-quality-metrics-integration-with-textstat.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>content quality analyst</asA>
    <iWant>to assess the readability and quality of document chunks using comprehensive metrics from textstat</iWant>
    <soThat>I can filter out low-quality content (OCR gibberish, malformed text) and reduce LLM hallucinations by 20% while prioritizing high-quality chunks for processing</soThat>
    <tasks>
      <!-- Core Implementation -->
      <task>Create src/data_extract/semantic/quality.py module</task>
      <task>Implement QualityMetricsStage class with PipelineStage protocol</task>
      <task>Integrate textstat library for readability calculations</task>
      <task>Add comprehensive metric computation methods</task>
      <task>Create composite scoring algorithm</task>
      <!-- Readability Metrics -->
      <task>Implement Flesch Reading Ease calculator</task>
      <task>Add Flesch-Kincaid Grade Level metric</task>
      <task>Implement Gunning Fog Index</task>
      <task>Add SMOG (Simple Measure of Gobbledygook) Index</task>
      <task>Implement Coleman-Liau and Dale-Chall readability scores</task>
      <!-- Quality Assessment -->
      <task>Calculate lexical diversity (type-token ratio)</task>
      <task>Add syllable count and complexity metrics</task>
      <task>Implement sentence length and structure analysis</task>
      <task>Create OCR gibberish detection heuristics</task>
      <task>Add special character and formatting anomaly detection</task>
      <!-- Scoring and Flagging -->
      <task>Design weighted composite scoring formula</task>
      <task>Implement quality threshold configuration</task>
      <task>Create quality flag enum (HIGH, MEDIUM, LOW, REVIEW)</task>
      <task>Add filtering logic for quality-based selection</task>
      <task>Generate quality improvement suggestions</task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="AC-4.4-1">QualityMetricsStage implements PipelineStage protocol accepting List[Chunk] and returning enriched List[Chunk]</criterion>
    <criterion id="AC-4.4-2">Compute Flesch-Kincaid Grade Level, Gunning Fog, SMOG Index, and Coleman-Liau Index for each chunk</criterion>
    <criterion id="AC-4.4-3">Calculate lexical diversity (unique words/total words) and syllable complexity metrics</criterion>
    <criterion id="AC-4.4-4">Generate composite quality score (0.0-1.0 scale) combining readability and diversity metrics</criterion>
    <criterion id="AC-4.4-5">Flag low-quality chunks (score &lt; 0.3) for review or exclusion from semantic processing</criterion>
    <criterion id="AC-4.4-6">Performance meets NFR: &lt;10ms per chunk, &lt;10s for 1000 chunks, minimal memory overhead</criterion>
    <criterion id="AC-4.4-7">Enrich chunk.quality_score field and add readability_scores dictionary to metadata</criterion>
    <criterion id="AC-4.4-8">Support configurable metric weights for domain-specific quality assessment</criterion>
    <criterion id="AC-4.4-9">Generate quality distribution report showing score histogram and flagged chunks</criterion>
    <criterion id="AC-4.4-10">All code passes mypy with zero errors and black/ruff with zero violations</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/architecture/epic-4-knowledge-curation-architecture.md</path>
        <title>Epic 4 Knowledge Curation Architecture</title>
        <section>Story 4.4: Quality Metrics Integration</section>
        <snippet>Integrate textstat for content quality assessment, multiple readability indices for robustness, flag low-quality content for review.</snippet>
      </doc>
      <doc>
        <path>docs/implementation/epic-4-implementation-patterns.md</path>
        <title>Epic 4 Implementation Patterns</title>
        <section>3.4 Quality Metrics Integration</section>
        <snippet>QualityMetricsStage enriches chunks with comprehensive quality scores using textstat library, composite scoring with weighted metrics.</snippet>
      </doc>
      <doc>
        <path>docs/testing/epic-4-behavioral-test-strategy.md</path>
        <title>Behavioral Test Strategy</title>
        <section>Quality Filtering</section>
        <snippet>Verify quality metrics flag bad content, test OCR gibberish detection, validate readability improvement.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-4.md</path>
        <title>Epic 4 Technical Specification</title>
        <section>Quality Metrics (Story 4.4)</section>
        <snippet>Readability scores (Flesch-Kincaid, Gunning Fog), lexical diversity measurement, content quality flags for filtering.</snippet>
      </doc>
      <doc>
        <path>docs/chunk/quality.py</path>
        <title>Existing Quality Implementation</title>
        <section>QualityScore Model</section>
        <snippet>Existing QualityScore dataclass from Epic 3 that this story extends with readability metrics.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/data_extract/core/pipeline.py</path>
        <kind>protocol</kind>
        <symbol>PipelineStage</symbol>
        <lines>20</lines>
        <reason>Base protocol that QualityMetricsStage must implement</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/core/models.py</path>
        <kind>model</kind>
        <symbol>Chunk</symbol>
        <lines>359</lines>
        <reason>Model to be enriched with quality scores</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/chunk/quality.py</path>
        <kind>model</kind>
        <symbol>QualityScore</symbol>
        <lines>all</lines>
        <reason>Existing quality score model to extend</reason>
      </artifact>
      <artifact>
        <path>src/data_extract/chunk/models.py</path>
        <kind>model</kind>
        <symbol>ChunkMetadata</symbol>
        <lines>22</lines>
        <reason>Metadata model to add readability_scores field</reason>
      </artifact>
      <artifact>
        <path>tests/fixtures/semantic_corpus_264k.json</path>
        <kind>fixture</kind>
        <symbol>semantic_corpus</symbol>
        <lines>all</lines>
        <reason>Test corpus for quality metric validation</reason>
      </artifact>
      <artifact>
        <path>tests/behavioral/epic_4/test_quality_scores.py</path>
        <kind>test</kind>
        <symbol>test_quality_scores_identify_gibberish</symbol>
        <lines>to be created</lines>
        <reason>Behavioral test for OCR gibberish detection</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package>textstat</package>
        <version>>=0.7.3</version>
        <purpose>Comprehensive readability metrics library</purpose>
      </python>
      <python>
        <package>numpy</package>
        <version>>=1.24.3</version>
        <purpose>Statistical calculations for composite scoring</purpose>
      </python>
      <python>
        <package>scikit-learn</package>
        <version>>=1.3.0</version>
        <purpose>Optional: MinMaxScaler for score normalization</purpose>
      </python>
      <python>
        <package>pandas</package>
        <version>>=2.0.3</version>
        <purpose>Quality distribution analysis and reporting</purpose>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Must preserve existing chunk structure while adding quality fields</constraint>
    <constraint>Quality score must be on 0.0-1.0 scale for consistency</constraint>
    <constraint>Processing time must be under 10ms per chunk</constraint>
    <constraint>Must handle edge cases gracefully (empty text, single words, special chars)</constraint>
    <constraint>Metric weights must be configurable for different domains</constraint>
    <constraint>Must not modify original chunk text, only add metadata</constraint>
    <constraint>Quality flags must be actionable (filter, review, accept)</constraint>
    <constraint>Must integrate seamlessly with existing pipeline stages</constraint>
    <constraint>Cache repeated calculations for performance</constraint>
    <constraint>All code must pass mypy, black, and ruff with zero violations</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>QualityMetricsStage</name>
      <kind>class</kind>
      <signature>class QualityMetricsStage: def process(self, chunks: List[Chunk], context: ProcessingContext) -> List[Chunk]</signature>
      <path>src/data_extract/semantic/quality.py (to be created)</path>
    </interface>
    <interface>
      <name>textstat</name>
      <kind>module</kind>
      <signature>import textstat</signature>
      <path>textstat</path>
    </interface>
    <interface>
      <name>QualityConfig</name>
      <kind>dataclass</kind>
      <signature>@dataclass class QualityConfig: min_quality: float = 0.3; weights: Dict[str, float] = field(default_factory=dict)</signature>
      <path>src/data_extract/semantic/quality.py (to be created)</path>
    </interface>
    <interface>
      <name>ReadabilityScores</name>
      <kind>model</kind>
      <signature>flesch_reading_ease: float; flesch_kincaid_grade: float; gunning_fog: float; smog_index: float; coleman_liau: float; lexical_diversity: float</signature>
      <path>src/data_extract/semantic/quality.py (to be created)</path>
    </interface>
    <interface>
      <name>QualityFlag</name>
      <kind>enum</kind>
      <signature>class QualityFlag(Enum): HIGH = "high"; MEDIUM = "medium"; LOW = "low"; REVIEW = "review"</signature>
      <path>src/data_extract/semantic/quality.py (to be created)</path>
    </interface>
  </interfaces>

  <tests>
    <standards>The project uses pytest as the test framework. Tests must be organized in tests/unit/ mirroring the src/ structure. All greenfield code requires 80% coverage minimum, with 95% target for quality modules. Edge cases must be thoroughly tested (empty text, single words, special characters, very long text). Performance benchmarks validate the 10ms per chunk requirement.</standards>
    <locations>
      <location>tests/unit/test_semantic/test_quality_stage.py (unit tests)</location>
      <location>tests/behavioral/epic_4/test_quality_filtering.py (behavioral)</location>
      <location>tests/performance/test_quality_benchmarks.py (performance)</location>
      <location>tests/fixtures/quality/gibberish_samples.txt (OCR gibberish)</location>
      <location>tests/fixtures/quality/high_quality_samples.txt (good content)</location>
    </locations>
    <ideas>
      <idea ac="AC-4.4-1">Test QualityMetricsStage implements PipelineStage protocol correctly</idea>
      <idea ac="AC-4.4-2">Test all readability metrics return valid scores in expected ranges</idea>
      <idea ac="AC-4.4-3">Test lexical diversity calculation with various text samples</idea>
      <idea ac="AC-4.4-4">Test composite score stays within 0.0-1.0 range</idea>
      <idea ac="AC-4.4-5">Test quality flagging correctly identifies low-quality chunks</idea>
      <idea ac="AC-4.4-6">Benchmark processing time for 1000 chunks under 10 seconds</idea>
      <idea ac="AC-4.4-7">Test chunk enrichment preserves original data while adding scores</idea>
      <idea ac="AC-4.4-8">Test configurable weights produce different scores</idea>
      <idea ac="AC-4.4-9">Test quality report generation with distribution statistics</idea>
      <idea ac="AC-4.4-5">Test OCR gibberish detection with real malformed samples</idea>
      <idea ac="AC-4.4-2">Test edge cases: empty string, single character, only punctuation</idea>
    </ideas>
  </tests>
</story-context>