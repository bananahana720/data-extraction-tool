# Test Cases: 2.5-3.1 - UAT Workflow Framework

**Story**: 2.5.3.1
**Generated**: 2025-11-13
**Coverage Level**: standard
**Generated By**: create-test-cases workflow

---

## Story Summary

**As a** QA Engineer / Developer
**I want** automated UAT workflows for test case creation, context-building, AI-assisted test execution, and senior QA review
**So that** acceptance criteria can be systematically validated with AI assistance and human oversight

---

## Test Coverage Summary

| AC ID | Description | Happy Path | Edge Cases | Error Cases | Integration | Total Tests |
|-------|-------------|------------|------------|-------------|-------------|-------------|
| AC-2.5.3.1-1 | create-test-cases workflow designed | ✓ | ✓ (2) | ✓ (1) | ✓ (1) | 5 |
| AC-2.5.3.1-2 | build-test-context workflow designed | ✓ | ✓ (2) | ✓ (1) | ✓ (1) | 5 |
| AC-2.5.3.1-3 | execute-tests workflow designed | ✓ | ✓ (2) | ✓ (1) | ✓ (1) | 5 |
| AC-2.5.3.1-4 | review-uat-results workflow designed | ✓ | ✓ (2) | ✓ (1) | ✓ (1) | 5 |
| AC-2.5.3.1-5 | Documentation in tech-spec | ✓ | ✓ (1) | - | ✓ (1) | 3 |
| AC-2.5.3.1-6 | Example UAT execution on Story 2.5.3 | ✓ | ✓ (1) | ✓ (1) | ✓ (1) | 4 |

**Test Type Distribution**:
- Unit tests: 0
- Integration tests: 6
- CLI tests: 0
- Manual tests: 21
- Performance tests: 0
- **Total test cases**: 27

---

## Test Cases

### AC-2.5.3.1-1: UAT workflow `create-test-cases` designed and documented

#### Test Case TC-2.5-3.1-1-1

**Acceptance Criterion**: AC-2.5.3.1-1
**Test Type**: Manual
**Scenario**: Happy Path

**Objective**: Verify create-test-cases workflow structure and documentation are complete

**Preconditions**:
- BMAD workflow directory structure exists
- Story markdown files available for testing

**Test Steps**:
1. Navigate to `bmad/bmm/workflows/4-implementation/create-test-cases/`
2. Verify `workflow.yaml` exists and contains required fields:
   - `name`, `description`, `config_source`
   - `variables` section with `story_path`, `story_dir`, `test_coverage_level`
   - `default_output_file` pointing to `{output_folder}/uat/test-cases/{{story_key}}-test-cases.md`
3. Verify `instructions.md` exists with step-by-step workflow logic:
   - Step 1: Parse story file and extract ACs
   - Step 2: Generate test scenarios (happy path, edge cases, error cases)
   - Step 3: Map scenarios to test types
   - Step 4: Output structured test cases
4. Verify `template.md` exists with proper variable placeholders
5. Verify `checklist.md` exists for workflow validation
6. Verify `README.md` documents workflow purpose and usage

**Expected Results**:
- All 5 required files exist (workflow.yaml, instructions.md, template.md, checklist.md, README.md)
- workflow.yaml defines correct inputs (story markdown) and outputs (test cases)
- instructions.md provides clear, executable steps for AI agent
- template.md includes all required sections (Story Summary, Test Coverage, Test Cases, Testing Notes)
- Documentation is complete and unambiguous

**Test Data**:
- Input: Story file `docs/stories/2.5-3-quality-gate-automation-and-documentation.md`
- Expected Output: Test cases file following template structure

**Dependencies**:
- BMAD workflow framework
- Story markdown files in `docs/stories/`

---

#### Test Case TC-2.5-3.1-1-2

**Acceptance Criterion**: AC-2.5.3.1-1
**Test Type**: Integration
**Scenario**: Happy Path

**Objective**: Validate create-test-cases workflow executes successfully on a real story

**Preconditions**:
- create-test-cases workflow files exist and are valid
- Test story file available

**Test Steps**:
1. Execute workflow command: `workflow create-test-cases story_path=docs/stories/2.5-3-quality-gate-automation-and-documentation.md`
2. Observe workflow execution through all steps
3. Verify output file created at `docs/uat/test-cases/2.5-3-test-cases.md`
4. Verify output file contains:
   - Story summary with As a/I want/So that
   - All 8 ACs from source story covered
   - Test cases with proper structure (TC-{id}, Objective, Preconditions, Steps, Expected Results)
   - Test coverage matrix
   - Testing notes and recommendations

**Expected Results**:
- Workflow executes without errors
- Output file generated at correct path
- All ACs from source story have corresponding test cases
- Test cases follow standard structure
- Coverage matrix shows appropriate distribution (happy path, edge cases, error cases)

**Test Data**:
- Input: `docs/stories/2.5-3-quality-gate-automation-and-documentation.md` (8 ACs)
- Expected Output: `docs/uat/test-cases/2.5-3-test-cases.md` with 24+ test cases (3-4 per AC at standard coverage)

**Dependencies**:
- Workflow engine (`bmad/core/tasks/workflow.xml`)
- BMAD config (`bmad/bmm/config.yaml`)

---

#### Test Case TC-2.5-3.1-1-3

**Acceptance Criterion**: AC-2.5.3.1-1
**Test Type**: Manual
**Scenario**: Edge Case - Story with no acceptance criteria

**Objective**: Verify workflow handles story files with missing or empty AC sections gracefully

**Preconditions**:
- create-test-cases workflow operational
- Test story file with no ACs prepared

**Test Steps**:
1. Create test story file with story description but no AC section
2. Execute workflow: `workflow create-test-cases story_path=docs/stories/test-no-acs.md`
3. Observe workflow behavior

**Expected Results**:
- Workflow detects missing ACs
- Provides clear error message: "No acceptance criteria found in story file"
- Prompts user to provide story with ACs or abort
- Does not generate empty or malformed test cases file

**Test Data**:
- Input: Story markdown file without AC section
- Expected Output: Clear error message, no output file generated

**Dependencies**:
- Error handling in workflow instructions

---

#### Test Case TC-2.5-3.1-1-4

**Acceptance Criterion**: AC-2.5.3.1-1
**Test Type**: Manual
**Scenario**: Edge Case - Story with many acceptance criteria (10+)

**Objective**: Verify workflow scales to stories with large numbers of ACs

**Preconditions**:
- create-test-cases workflow operational
- Story file with 10+ ACs available or created

**Test Steps**:
1. Execute workflow on story with 10+ ACs
2. Observe workflow execution time and output quality
3. Verify all ACs covered in generated test cases
4. Verify coverage matrix remains readable
5. Verify test case numbering is consistent

**Expected Results**:
- Workflow completes successfully (may take longer for large stories)
- All 10+ ACs have corresponding test cases
- Standard coverage applies (3-4 tests per AC = 30-40+ total tests)
- Coverage matrix remains well-formatted and readable
- Test case IDs follow consistent pattern (TC-{story-key}-{ac-num}-{scenario-num})

**Test Data**:
- Input: Story with 10+ ACs
- Expected Output: 30-40+ test cases with proper structure

**Dependencies**:
- Template rendering engine
- Workflow execution timeout (should be sufficient for large stories)

---

#### Test Case TC-2.5-3.1-1-5

**Acceptance Criterion**: AC-2.5.3.1-1
**Test Type**: Manual
**Scenario**: Error Case - Invalid story file path

**Objective**: Verify workflow handles invalid file paths gracefully

**Preconditions**:
- create-test-cases workflow operational

**Test Steps**:
1. Execute workflow with non-existent file path: `workflow create-test-cases story_path=docs/stories/non-existent-story.md`
2. Observe error handling

**Expected Results**:
- Workflow detects file not found
- Provides clear error message with file path
- Suggests checking file path or listing available stories
- Does not crash or generate partial output

**Test Data**:
- Input: Invalid file path
- Expected Output: Clear error message, no output file

**Dependencies**:
- File system access validation
- Error handling in workflow

---

### AC-2.5.3.1-2: UAT workflow `build-test-context` designed and documented

#### Test Case TC-2.5-3.1-2-1

**Acceptance Criterion**: AC-2.5.3.1-2
**Test Type**: Manual
**Scenario**: Happy Path

**Objective**: Verify build-test-context workflow structure and documentation are complete

**Preconditions**:
- BMAD workflow directory structure exists
- Test cases file available from previous workflow

**Test Steps**:
1. Navigate to `bmad/bmm/workflows/4-implementation/build-test-context/`
2. Verify `workflow.yaml` exists with required fields:
   - Input variables: `story_path`, `test_cases_file`, `story_context_xml` (optional)
   - Output: `{output_folder}/uat/test-context/{{story_key}}-test-context.xml`
   - Variables: `test_fixture_paths`, `helper_module_paths`, `config_files`
3. Verify `instructions.md` exists with context gathering logic:
   - Step 1: Load test cases and identify required fixtures/helpers
   - Step 2: Discover relevant test fixtures
   - Step 3: Identify test helper modules (conftest.py)
   - Step 4: Gather pytest configuration
   - Step 5: Generate test-context.xml
4. Verify `template.xml` exists with proper XML structure
5. Verify `checklist.md` exists for validation
6. Verify `README.md` documents workflow and relationship to story-context

**Expected Results**:
- All 5 required files exist
- workflow.yaml defines correct inputs (test cases) and outputs (test context XML)
- instructions.md provides clear fixture discovery and context building steps
- template.xml provides well-structured XML format
- Documentation explains difference from story-context workflow (testing focus vs implementation focus)

**Test Data**:
- Input: Test cases file from create-test-cases workflow
- Expected Output: test-context.xml with fixtures, helpers, pytest config

**Dependencies**:
- Test fixture directory (`tests/fixtures/`)
- pytest configuration (`pytest.ini`, `pyproject.toml`)
- conftest.py files

---

#### Test Case TC-2.5-3.1-2-2

**Acceptance Criterion**: AC-2.5.3.1-2
**Test Type**: Integration
**Scenario**: Happy Path

**Objective**: Validate build-test-context workflow executes and discovers test infrastructure

**Preconditions**:
- build-test-context workflow files exist
- Test cases file from previous step exists
- Test fixtures and conftest.py exist in project

**Test Steps**:
1. Execute workflow: `workflow build-test-context test_cases_file=docs/uat/test-cases/2.5-3-test-cases.md`
2. Observe workflow execution
3. Verify output file created at `docs/uat/test-context/2.5-3-test-context.xml`
4. Verify XML contains:
   - Test fixture paths discovered from `tests/fixtures/`
   - conftest.py content and fixture definitions
   - pytest.ini configuration
   - Relevant test helper modules

**Expected Results**:
- Workflow executes successfully
- test-context.xml generated with well-formed XML
- All relevant fixtures discovered (PDF, DOCX, XLSX fixtures for Extract story)
- conftest.py fixtures included
- pytest markers and configuration captured

**Test Data**:
- Input: `docs/uat/test-cases/2.5-3-test-cases.md`
- Expected Output: `docs/uat/test-context/2.5-3-test-context.xml`

**Dependencies**:
- File system glob/search capabilities
- XML generation libraries

---

#### Test Case TC-2.5-3.1-2-3

**Acceptance Criterion**: AC-2.5.3.1-2
**Test Type**: Manual
**Scenario**: Edge Case - Test cases with no fixture requirements

**Objective**: Verify workflow handles test cases that don't require test fixtures

**Preconditions**:
- build-test-context workflow operational
- Test cases for documentation-only story (no code tests)

**Test Steps**:
1. Create test cases for documentation story (manual verification only)
2. Execute build-test-context on these test cases
3. Observe workflow behavior

**Expected Results**:
- Workflow completes successfully
- test-context.xml generated but with minimal content
- No false positive fixture discoveries
- Documentation clearly indicates "no test fixtures required for this story"

**Test Data**:
- Input: Test cases with only manual test types
- Expected Output: test-context.xml with minimal fixture section

**Dependencies**:
- Fixture discovery logic that handles empty results

---

#### Test Case TC-2.5-3.1-2-4

**Acceptance Criterion**: AC-2.5.3.1-2
**Test Type**: Manual
**Scenario**: Edge Case - Reuse story-context XML

**Objective**: Verify workflow can optionally reuse story-context.xml to avoid duplication

**Preconditions**:
- build-test-context workflow operational
- story-context.xml exists for the story
- Test cases file exists

**Test Steps**:
1. Execute workflow with optional story context: `workflow build-test-context test_cases_file=docs/uat/test-cases/2.5-3-test-cases.md include_story_context=true`
2. Verify workflow loads existing story-context.xml
3. Verify test-context.xml includes or references story context
4. Verify test-specific additions are merged (fixtures, pytest config)

**Expected Results**:
- Workflow loads story-context.xml successfully
- test-context.xml includes both story context and test-specific context
- No duplication of code references
- Clear separation between implementation context and test context

**Test Data**:
- Input: Test cases + existing story-context.xml
- Expected Output: Merged test-context.xml

**Dependencies**:
- XML merging logic
- story-context workflow (for generating input)

---

#### Test Case TC-2.5-3.1-2-5

**Acceptance Criterion**: AC-2.5.3.1-2
**Test Type**: Manual
**Scenario**: Error Case - Invalid test cases file path

**Objective**: Verify workflow handles missing test cases file gracefully

**Preconditions**:
- build-test-context workflow operational

**Test Steps**:
1. Execute workflow with invalid path: `workflow build-test-context test_cases_file=docs/uat/test-cases/non-existent.md`
2. Observe error handling

**Expected Results**:
- Workflow detects file not found
- Clear error message with file path
- Suggests running create-test-cases first
- No partial output generated

**Test Data**:
- Input: Invalid file path
- Expected Output: Clear error message

**Dependencies**:
- File validation logic

---

### AC-2.5.3.1-3: UAT workflow `execute-tests` designed and documented

#### Test Case TC-2.5-3.1-3-1

**Acceptance Criterion**: AC-2.5.3.1-3
**Test Type**: Manual
**Scenario**: Happy Path

**Objective**: Verify execute-tests workflow structure and documentation are complete

**Preconditions**:
- BMAD workflow directory structure exists
- Test cases and test context available

**Test Steps**:
1. Navigate to `bmad/bmm/workflows/4-implementation/execute-tests/`
2. Verify `workflow.yaml` exists with required fields:
   - Input: `test_cases_file`, `test_context_file`, `story_file`
   - Output: `{output_folder}/uat/test-results/{{story_key}}-test-results.md`
   - Variables: `test_execution_mode` (automated/manual/hybrid), `tmux_session_name`
3. Verify `instructions.md` exists with test execution logic:
   - Step 1: Load test cases and categorize by type
   - Step 2: Execute pytest tests via tmux-cli
   - Step 3: Execute CLI tests using tmux-cli
   - Step 4: Guide manual tests and capture results
   - Step 5: Aggregate results with evidence
4. Verify `template.md` exists for test results format
5. Verify tmux-cli integration documented with examples
6. Verify `checklist.md` exists for validation
7. Verify `README.md` documents workflow and tmux-cli requirements

**Expected Results**:
- All 5 required files exist
- workflow.yaml defines test execution modes
- instructions.md provides clear test execution steps for each test type
- tmux-cli integration patterns documented with examples
- template.md includes pass/fail/blocked status fields and evidence sections

**Test Data**:
- Input: Test cases + test context
- Expected Output: Test execution results with evidence

**Dependencies**:
- tmux-cli command availability
- pytest execution capability

---

#### Test Case TC-2.5-3.1-3-2

**Acceptance Criterion**: AC-2.5.3.1-3
**Test Type**: Integration
**Scenario**: Happy Path - Automated test execution

**Objective**: Validate execute-tests workflow runs pytest tests and captures results

**Preconditions**:
- execute-tests workflow files exist
- Test cases and test context exist
- pytest tests exist for the story

**Test Steps**:
1. Execute workflow in automated mode: `workflow execute-tests test_execution_mode=automated`
2. Observe workflow execution
3. Verify workflow categorizes tests by type (unit, integration)
4. Verify pytest executed via tmux-cli or directly
5. Verify output file created at `docs/uat/test-results/2.5-3-test-results.md`
6. Verify results include:
   - Pass/fail/blocked status for each test case
   - pytest output captured as evidence
   - Test execution timestamps
   - Failure details with stack traces

**Expected Results**:
- Workflow executes all automated tests
- Results file generated with complete status for each test
- Evidence includes pytest output, coverage reports
- Clear pass/fail ratio calculated
- Recommendations provided for failures

**Test Data**:
- Input: Test cases with unit/integration test types
- Expected Output: `docs/uat/test-results/2.5-3-test-results.md`

**Dependencies**:
- pytest execution
- tmux-cli (optional, for interactive execution)

---

#### Test Case TC-2.5-3.1-3-3

**Acceptance Criterion**: AC-2.5.3.1-3
**Test Type**: Manual
**Scenario**: Edge Case - CLI tests requiring tmux-cli

**Objective**: Verify workflow executes CLI tests using tmux-cli integration

**Preconditions**:
- execute-tests workflow operational
- tmux-cli command available
- Test cases include CLI test types
- CLI application (data-extract) available

**Test Steps**:
1. Execute workflow with CLI tests: `workflow execute-tests test_execution_mode=hybrid`
2. Observe workflow launch tmux-cli session
3. Verify workflow sends CLI commands to tmux session
4. Verify workflow captures CLI output
5. Verify results include CLI output as evidence

**Expected Results**:
- tmux-cli session launched successfully
- CLI commands executed in separate pane
- Output captured and included in results
- CLI test marked as pass/fail based on output analysis
- tmux session cleaned up after execution

**Test Data**:
- Input: Test cases with CLI test type
- Expected Output: Test results with CLI output evidence

**Dependencies**:
- tmux-cli command
- data-extract CLI application

---

#### Test Case TC-2.5-3.1-3-4

**Acceptance Criterion**: AC-2.5.3.1-3
**Test Type**: Manual
**Scenario**: Edge Case - Manual tests requiring human interaction

**Objective**: Verify workflow guides tester through manual test execution

**Preconditions**:
- execute-tests workflow operational
- Test cases include manual test types

**Test Steps**:
1. Execute workflow in manual mode: `workflow execute-tests test_execution_mode=manual`
2. Observe workflow present manual test steps to user
3. Respond to workflow prompts with test results
4. Verify workflow captures tester responses
5. Verify results file includes manual test outcomes

**Expected Results**:
- Workflow presents manual test steps clearly
- Prompts for pass/fail status after each test
- Captures tester notes and observations
- Results file includes manual test outcomes with tester comments
- Workflow continues through all manual tests

**Test Data**:
- Input: Test cases with manual test type
- Expected Output: Test results with tester observations

**Dependencies**:
- Interactive prompting capability
- User input capture

---

#### Test Case TC-2.5-3.1-3-5

**Acceptance Criterion**: AC-2.5.3.1-3
**Test Type**: Manual
**Scenario**: Error Case - tmux-cli not available

**Objective**: Verify workflow handles missing tmux-cli gracefully

**Preconditions**:
- execute-tests workflow operational
- tmux-cli not installed or not in PATH
- Test cases include CLI test types

**Test Steps**:
1. Ensure tmux-cli is not available (temporarily rename or remove from PATH)
2. Execute workflow: `workflow execute-tests test_execution_mode=automated`
3. Observe error handling when CLI tests attempted

**Expected Results**:
- Workflow detects tmux-cli unavailable
- Clear error message: "tmux-cli required for CLI tests but not found"
- Suggests installing tmux-cli or switching to manual mode
- CLI tests marked as "blocked" with reason
- Other test types (unit, integration) continue executing
- Workflow completes with partial results

**Test Data**:
- Input: Test cases with CLI tests, tmux-cli unavailable
- Expected Output: Results with CLI tests blocked, others executed

**Dependencies**:
- Dependency checking logic
- Graceful degradation

---

### AC-2.5.3.1-4: UAT workflow `review-uat-results` designed and documented

#### Test Case TC-2.5-3.1-4-1

**Acceptance Criterion**: AC-2.5.3.1-4
**Test Type**: Manual
**Scenario**: Happy Path

**Objective**: Verify review-uat-results workflow structure and documentation are complete

**Preconditions**:
- BMAD workflow directory structure exists
- Test results available from execute-tests

**Test Steps**:
1. Navigate to `bmad/bmm/workflows/4-implementation/review-uat-results/`
2. Verify `workflow.yaml` exists with required fields:
   - Input: `test_results_file`, `test_cases_file`, `story_file`
   - Output: `{output_folder}/uat/reviews/{{story_key}}-uat-review.md`
   - Variables: `reviewer_name`, `review_date`, `quality_gate_level`
3. Verify `instructions.md` exists with AI-assisted review logic:
   - Step 1: Analyze pass/fail/blocked ratio
   - Step 2: Identify gaps in coverage vs ACs
   - Step 3: Analyze edge case and error scenario coverage
   - Step 4: Check evidence quality
   - Step 5: Generate findings with severity
   - Step 6: Provide approval decision
4. Verify `template.md` exists for review report format
5. Verify `checklist.md` exists for review quality validation
6. Verify `README.md` documents senior QA role and decision criteria

**Expected Results**:
- All 5 required files exist
- workflow.yaml defines review inputs and approval output
- instructions.md provides systematic review process
- template.md includes approval/changes-requested/blocked decision
- Documentation explains quality gate levels (minimal/standard/strict)

**Test Data**:
- Input: Test results from execute-tests
- Expected Output: UAT review with approval decision

**Dependencies**:
- Test results file from execute-tests workflow

---

#### Test Case TC-2.5-3.1-4-2

**Acceptance Criterion**: AC-2.5.3.1-4
**Test Type**: Integration
**Scenario**: Happy Path - All tests pass

**Objective**: Validate review workflow generates approval for clean test results

**Preconditions**:
- review-uat-results workflow files exist
- Test results with all tests passing available

**Test Steps**:
1. Execute workflow: `workflow review-uat-results test_results_file=docs/uat/test-results/2.5-3-test-results.md`
2. Observe AI-assisted analysis
3. Verify output file created at `docs/uat/reviews/2.5-3-uat-review.md`
4. Verify review includes:
   - Pass ratio analysis (100% in this case)
   - Coverage gap analysis (should find none)
   - Edge case coverage assessment
   - Evidence quality review
   - Approval decision: APPROVED
   - Stakeholder summary

**Expected Results**:
- Workflow completes successfully
- Review report generated
- Decision: APPROVED
- No critical findings
- Stakeholder summary suitable for non-technical audience
- Clear statement: "All acceptance criteria validated"

**Test Data**:
- Input: Test results with 100% pass rate
- Expected Output: UAT review with APPROVED status

**Dependencies**:
- AI analysis capabilities
- Gap detection logic

---

#### Test Case TC-2.5-3.1-4-3

**Acceptance Criterion**: AC-2.5.3.1-4
**Test Type**: Manual
**Scenario**: Edge Case - All tests fail

**Objective**: Verify review workflow correctly identifies complete test failure

**Preconditions**:
- review-uat-results workflow operational
- Test results with all tests failing available (simulated or real)

**Test Steps**:
1. Execute workflow on failing test results
2. Observe AI analysis of failures
3. Verify review report generated

**Expected Results**:
- Review decision: CHANGES REQUESTED or BLOCKED
- Critical findings identifying systemic issues
- All ACs marked as not validated
- Recommendations for investigation (environment issues, code defects, test issues)
- Stakeholder summary explains severity clearly
- No approval granted

**Test Data**:
- Input: Test results with 0% pass rate
- Expected Output: UAT review with CHANGES REQUESTED/BLOCKED

**Dependencies**:
- Failure analysis logic

---

#### Test Case TC-2.5-3.1-4-4

**Acceptance Criterion**: AC-2.5.3.1-4
**Test Type**: Manual
**Scenario**: Edge Case - Partial failures (edge cases failing)

**Objective**: Verify review workflow makes nuanced decisions on partial failures

**Preconditions**:
- review-uat-results workflow operational
- Test results with happy paths passing, edge cases failing

**Test Steps**:
1. Execute workflow on partial test results
2. Observe AI analysis of failure patterns
3. Verify review distinguishes between critical AC failures and edge case issues

**Expected Results**:
- Review analyzes failure patterns (e.g., "All happy paths pass, 3 edge cases fail")
- Decision depends on quality gate level and severity
- Findings categorized by severity (critical/major/minor)
- If critical ACs pass: Possible approval with minor findings
- If critical ACs fail: CHANGES REQUESTED
- Clear guidance on which tests must be fixed

**Test Data**:
- Input: Test results with 70% pass rate (happy paths pass, edge cases fail)
- Expected Output: UAT review with nuanced findings

**Dependencies**:
- Pattern recognition in AI analysis
- Quality gate level configuration

---

#### Test Case TC-2.5-3.1-4-5

**Acceptance Criterion**: AC-2.5.3.1-4
**Test Type**: Manual
**Scenario**: Error Case - Missing test results file

**Objective**: Verify workflow handles missing test results gracefully

**Preconditions**:
- review-uat-results workflow operational

**Test Steps**:
1. Execute workflow with invalid path: `workflow review-uat-results test_results_file=docs/uat/test-results/non-existent.md`
2. Observe error handling

**Expected Results**:
- Workflow detects file not found
- Clear error message
- Suggests running execute-tests first
- No review report generated

**Test Data**:
- Input: Invalid file path
- Expected Output: Clear error message

**Dependencies**:
- File validation logic

---

### AC-2.5.3.1-5: Workflow integration documented in tech-spec-epic-2.5.md

#### Test Case TC-2.5-3.1-5-1

**Acceptance Criterion**: AC-2.5.3.1-5
**Test Type**: Manual
**Scenario**: Happy Path

**Objective**: Verify tech-spec-epic-2.5.md contains comprehensive UAT workflow documentation

**Preconditions**:
- tech-spec-epic-2.5.md exists
- All 4 UAT workflows designed

**Test Steps**:
1. Open `docs/tech-spec-epic-2.5.md`
2. Verify UAT workflow section exists with:
   - Section header: "UAT Testing Framework" or similar
   - End-to-end process flow diagram (mermaid or ASCII)
   - Workflow sequence: create-test-cases → build-test-context → execute-tests → review-uat-results
   - Handoff points documented (outputs of one workflow = inputs of next)
   - Integration with story development workflow explained
3. Verify workflow orchestration options documented:
   - Manual sequential execution
   - Future: Automated orchestration
   - Selective execution (skip completed steps)
4. Verify usage guidance explains when to use UAT workflows

**Expected Results**:
- tech-spec contains dedicated UAT framework section
- Process flow diagram clearly shows 4-workflow pipeline
- Handoff points and data flows documented
- Integration with existing workflows (story-context, dev-story, code-review) explained
- Clear guidance on when UAT workflows apply (systematic AC validation)

**Test Data**:
- Location: `docs/tech-spec-epic-2.5.md`
- Expected: UAT workflow section with diagrams

**Dependencies**:
- Documentation standards

---

#### Test Case TC-2.5-3.1-5-2

**Acceptance Criterion**: AC-2.5.3.1-5
**Test Type**: Manual
**Scenario**: Edge Case - CLAUDE.md updated with UAT usage

**Objective**: Verify CLAUDE.md includes UAT workflow usage under Common Tasks

**Preconditions**:
- CLAUDE.md exists
- UAT workflows operational

**Test Steps**:
1. Open `CLAUDE.md`
2. Verify "Common Tasks" section includes "Running UAT Workflows" subsection
3. Verify subsection documents:
   - Complete UAT flow example
   - Individual workflow invocation examples
   - When to use UAT vs standard testing
   - tmux-cli integration notes for CLI testing

**Expected Results**:
- CLAUDE.md updated with UAT workflow patterns
- Clear examples for AI agents to follow
- Integration patterns documented
- Usage guidance clear and actionable

**Test Data**:
- Location: `CLAUDE.md` section "Common Tasks"
- Expected: UAT workflow usage examples

**Dependencies**:
- Documentation standards

---

#### Test Case TC-2.5-3.1-5-3

**Acceptance Criterion**: AC-2.5.3.1-5
**Test Type**: Integration
**Scenario**: Workflow handoff validation

**Objective**: Verify outputs of each workflow match inputs of next workflow in pipeline

**Preconditions**:
- All 4 workflow.yaml files exist

**Test Steps**:
1. Review create-test-cases workflow.yaml default_output_file
2. Review build-test-context workflow.yaml input variables
3. Verify test cases output path matches expected input for build-test-context
4. Repeat for build-test-context → execute-tests
5. Repeat for execute-tests → review-uat-results
6. Verify file naming conventions consistent ({story-key}-test-cases.md, etc.)

**Expected Results**:
- Output paths of workflow N match input expectations of workflow N+1
- File naming conventions consistent across all workflows
- No manual file renaming or path adjustments required
- Pipeline flows smoothly from one workflow to next

**Test Data**:
- Source: workflow.yaml files for all 4 workflows
- Expected: Consistent paths and naming

**Dependencies**:
- Workflow configuration standards

---

### AC-2.5.3.1-6: Example UAT execution documented for Story 2.5.3

#### Test Case TC-2.5-3.1-6-1

**Acceptance Criterion**: AC-2.5.3.1-6
**Test Type**: Manual
**Scenario**: Happy Path - End-to-end UAT execution

**Objective**: Execute complete UAT pipeline on Story 2.5.3 and document results

**Preconditions**:
- All 4 UAT workflows operational
- Story 2.5.3 markdown file exists
- Story 2.5.3 is completed (status: done)

**Test Steps**:
1. Execute create-test-cases on Story 2.5.3:
   `workflow create-test-cases story_path=docs/stories/2.5-3-quality-gate-automation-and-documentation.md`
2. Review generated test cases, verify all 8 ACs covered
3. Execute build-test-context:
   `workflow build-test-context`
4. Review test context, verify fixtures discovered
5. Execute execute-tests in hybrid mode:
   `workflow execute-tests test_execution_mode=hybrid`
6. Review test results, note pass/fail/blocked status
7. Execute review-uat-results:
   `workflow review-uat-results`
8. Review UAT review report, note approval decision
9. Document lessons learned in story or tech spec

**Expected Results**:
- Complete UAT pipeline executes successfully
- All 4 output files generated:
  - `docs/uat/test-cases/2.5-3-test-cases.md`
  - `docs/uat/test-context/2.5-3-test-context.xml`
  - `docs/uat/test-results/2.5-3-test-results.md`
  - `docs/uat/reviews/2.5-3-uat-review.md`
- All 8 ACs from Story 2.5.3 validated
- Lessons learned documented (workflow improvements, edge cases discovered)
- Example serves as reference for future UAT executions

**Test Data**:
- Input: `docs/stories/2.5-3-quality-gate-automation-and-documentation.md`
- Expected: 4 UAT output files demonstrating complete pipeline

**Dependencies**:
- All UAT workflows
- Story 2.5.3 completion

---

#### Test Case TC-2.5-3.1-6-2

**Acceptance Criterion**: AC-2.5.3.1-6
**Test Type**: Manual
**Scenario**: Edge Case - Workflow refinement based on execution

**Objective**: Identify and document workflow improvements discovered during execution

**Preconditions**:
- Example UAT execution completed on Story 2.5.3
- Issues or improvement opportunities identified

**Test Steps**:
1. Review UAT execution results for Story 2.5.3
2. Identify workflow issues encountered:
   - Unclear instructions
   - Missing error handling
   - Template formatting issues
   - Integration gaps
3. Document refinements needed in story or tech spec
4. Update workflow templates if critical issues found

**Expected Results**:
- Lessons learned documented clearly
- Workflow refinements prioritized (critical vs nice-to-have)
- Future improvements captured for next iteration
- If critical issues found: Workflows updated immediately
- Documentation reflects real-world usage patterns

**Test Data**:
- Source: UAT execution on Story 2.5.3
- Expected: Lessons learned document with improvements

**Dependencies**:
- Retrospective analysis

---

#### Test Case TC-2.5-3.1-6-3

**Acceptance Criterion**: AC-2.5.3.1-6
**Test Type**: Manual
**Scenario**: Error Case - Test execution failures during example

**Objective**: Verify workflow handles test failures gracefully during example execution

**Preconditions**:
- UAT workflows operational
- Story 2.5.3 execution in progress

**Test Steps**:
1. During execute-tests on Story 2.5.3, observe any test failures
2. Verify failures captured in test results
3. Verify review-uat-results identifies failures correctly
4. Verify lessons learned document test failure handling

**Expected Results**:
- Test failures captured with evidence
- review-uat-results provides CHANGES REQUESTED decision
- Failure analysis identifies root causes
- Process demonstrates graceful handling of failures
- Documentation includes failure scenario patterns

**Test Data**:
- Source: Actual test execution results (may include failures)
- Expected: Documented failure handling patterns

**Dependencies**:
- Real test execution
- Failure analysis capabilities

---

#### Test Case TC-2.5-3.1-6-4

**Acceptance Criterion**: AC-2.5.3.1-6
**Test Type**: Integration
**Scenario**: Metrics capture during example execution

**Objective**: Capture and document UAT workflow performance metrics

**Preconditions**:
- UAT workflows operational
- Story 2.5.3 execution planned

**Test Steps**:
1. Record start time for complete UAT pipeline execution
2. Execute all 4 workflows sequentially
3. Record completion time for each workflow
4. Record total time for complete pipeline
5. Document metrics in story or tech spec:
   - Time per workflow
   - Total pipeline time
   - Test case count generated
   - Test execution time
   - Review generation time

**Expected Results**:
- Performance metrics captured for all workflows
- Total pipeline time documented (target: <30 min for typical story)
- Metrics baseline established for future comparison
- Bottlenecks identified (if any)
- Documentation includes timing expectations

**Test Data**:
- Source: Timed execution of UAT pipeline on Story 2.5.3
- Expected: Performance metrics document

**Dependencies**:
- Timing capabilities
- Metrics documentation standards

---

## Testing Notes

### Execution Recommendations

**Suggested Test Execution Order**:
1. **Phase 1 - Workflow Structure Validation** (Manual tests TC-*-1):
   - Verify all workflow files exist and are complete
   - Run in parallel for all 4 workflows (independent checks)
   - Est. time: 30 minutes total

2. **Phase 2 - Individual Workflow Integration Tests** (Integration tests TC-*-2):
   - Test each workflow in isolation with sample inputs
   - Run sequentially to establish baseline
   - Est. time: 1 hour total

3. **Phase 3 - Edge Case Testing** (Edge case tests TC-*-3, TC-*-4):
   - Test boundary conditions and unusual scenarios
   - Can run in parallel across workflows
   - Est. time: 1 hour total

4. **Phase 4 - Error Handling** (Error case tests TC-*-5):
   - Verify graceful degradation and error messages
   - Can run in parallel
   - Est. time: 30 minutes total

5. **Phase 5 - Documentation Validation** (AC-2.5.3.1-5 tests):
   - Verify tech spec and CLAUDE.md updates
   - Manual review
   - Est. time: 20 minutes total

6. **Phase 6 - End-to-End Example** (AC-2.5.3.1-6 tests):
   - Execute complete UAT pipeline on Story 2.5.3
   - Must run sequentially (pipeline dependencies)
   - Est. time: 2 hours total

**Total Estimated Test Execution Time**: 5-6 hours

**Critical Tests That Must Pass** (Blockers):
- TC-2.5-3.1-1-1: create-test-cases structure validation
- TC-2.5-3.1-2-1: build-test-context structure validation
- TC-2.5-3.1-3-1: execute-tests structure validation
- TC-2.5-3.1-4-1: review-uat-results structure validation
- TC-2.5-3.1-1-2: create-test-cases workflow execution
- TC-2.5-3.1-6-1: End-to-end UAT pipeline execution

**Tests That Can Run in Parallel**:
- All Phase 1 tests (TC-*-1 for all ACs)
- All Phase 3 edge case tests
- All Phase 4 error case tests

### Implementation Notes

**Suggested pytest markers**: (Note: This story is primarily manual/integration testing)
- `@pytest.mark.manual` - Manual verification tests
- `@pytest.mark.integration` - Workflow integration tests
- `@pytest.mark.uat` - UAT framework specific tests
- `@pytest.mark.workflow` - Workflow execution tests

**Fixture requirements**:
- Story markdown files for testing (existing stories in `docs/stories/`)
- Sample test cases files (generated during test execution)
- Sample test context XML files (generated during test execution)
- tmux-cli mock or actual command (for CLI test execution testing)

**Helper functions needed**:
- `load_workflow_config(workflow_path)` - Parse workflow.yaml
- `validate_workflow_structure(workflow_path)` - Check all required files exist
- `execute_workflow(workflow_name, **kwargs)` - Programmatic workflow execution
- `parse_test_cases(test_cases_file)` - Parse generated test cases
- `validate_xml_structure(xml_file, schema)` - Validate test context XML

**Integration points to mock/stub**:
- tmux-cli command (for testing execute-tests workflow without actual tmux)
- File system operations (for testing error cases without creating actual files)
- pytest execution (for testing execute-tests without running real tests)

### Risk Areas

**Complex scenarios requiring extra attention**:
1. **End-to-end pipeline orchestration** (TC-2.5-3.1-6-1):
   - Multiple workflow dependencies
   - File path consistency across workflows
   - Data format compatibility between workflow outputs/inputs
   - Highest risk of integration issues

2. **tmux-cli integration** (TC-2.5-3.1-3-3):
   - External dependency on tmux-cli command
   - Platform-specific behavior (Windows vs macOS/Linux)
   - Session management and cleanup
   - Output capture reliability

3. **AI-assisted review quality** (TC-2.5-3.1-4-2, TC-2.5-3.1-4-3, TC-2.5-3.1-4-4):
   - Review decision consistency
   - Gap detection accuracy
   - False positives/negatives in coverage analysis
   - Quality depends on AI model capabilities

**Potential failure points**:
- Workflow YAML parsing errors (syntax issues, missing required fields)
- File path inconsistencies across Windows/macOS/Linux
- tmux-cli unavailable or version incompatibility
- XML generation errors (malformed XML, encoding issues)
- Large story files causing timeout or memory issues
- Template variable resolution failures

**Performance bottlenecks**:
- create-test-cases on stories with 10+ ACs (standard coverage = 30-40+ test cases)
- build-test-context fixture discovery with large test suite (1000+ test files)
- execute-tests with many integration/CLI tests (serial execution may be slow)
- AI analysis in review-uat-results for complex failure patterns

**Security considerations**:
- File path validation to prevent directory traversal attacks
- Input sanitization for workflow variables
- Safe execution of external commands (tmux-cli, pytest)
- Proper handling of test output containing sensitive data
- Access control for UAT output files (may contain test failures with stack traces)

---

## Acceptance Criteria Coverage

### AC-2.5.3.1-1: create-test-cases workflow designed
- ✅ TC-2.5-3.1-1-1: Structure validation
- ✅ TC-2.5-3.1-1-2: Workflow execution
- ✅ TC-2.5-3.1-1-3: Edge case - No ACs
- ✅ TC-2.5-3.1-1-4: Edge case - Many ACs
- ✅ TC-2.5-3.1-1-5: Error case - Invalid path

### AC-2.5.3.1-2: build-test-context workflow designed
- ✅ TC-2.5-3.1-2-1: Structure validation
- ✅ TC-2.5-3.1-2-2: Workflow execution
- ✅ TC-2.5-3.1-2-3: Edge case - No fixtures
- ✅ TC-2.5-3.1-2-4: Edge case - Reuse story context
- ✅ TC-2.5-3.1-2-5: Error case - Invalid path

### AC-2.5.3.1-3: execute-tests workflow designed
- ✅ TC-2.5-3.1-3-1: Structure validation
- ✅ TC-2.5-3.1-3-2: Workflow execution - Automated
- ✅ TC-2.5-3.1-3-3: Edge case - CLI tests with tmux-cli
- ✅ TC-2.5-3.1-3-4: Edge case - Manual tests
- ✅ TC-2.5-3.1-3-5: Error case - tmux-cli unavailable

### AC-2.5.3.1-4: review-uat-results workflow designed
- ✅ TC-2.5-3.1-4-1: Structure validation
- ✅ TC-2.5-3.1-4-2: Workflow execution - All pass
- ✅ TC-2.5-3.1-4-3: Edge case - All fail
- ✅ TC-2.5-3.1-4-4: Edge case - Partial failures
- ✅ TC-2.5-3.1-4-5: Error case - Missing file

### AC-2.5.3.1-5: Workflow integration documented
- ✅ TC-2.5-3.1-5-1: tech-spec documentation
- ✅ TC-2.5-3.1-5-2: CLAUDE.md updated
- ✅ TC-2.5-3.1-5-3: Workflow handoff validation

### AC-2.5.3.1-6: Example UAT execution on Story 2.5.3
- ✅ TC-2.5-3.1-6-1: End-to-end execution
- ✅ TC-2.5-3.1-6-2: Workflow refinements
- ✅ TC-2.5-3.1-6-3: Failure handling
- ✅ TC-2.5-3.1-6-4: Metrics capture

**Coverage Summary**: All 6 ACs have comprehensive test coverage with 27 total test cases.

---

## Next Steps

1. **Review test cases**: Validate coverage and scenarios ✓ (this document)
2. **Run build-test-context**: Gather fixtures and helpers → `workflow build-test-context`
3. **Implement missing fixtures**: Create any required test data (none needed - all files exist)
4. **Execute tests**: Run via execute-tests workflow → `workflow execute-tests`
5. **Review results**: QA review via review-uat-results workflow → `workflow review-uat-results`

---

**Document Status**: Ready for Context Building
**Generated by**: andrew using BMAD UAT Framework
