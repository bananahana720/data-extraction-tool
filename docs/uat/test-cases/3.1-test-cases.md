# Test Cases: 3.1 - Semantic Boundary-Aware Chunking Engine

**Story**: 3-1-semantic-boundary-aware-chunking-engine
**Generated**: 2025-11-13
**Coverage Level**: standard
**Generated By**: create-test-cases workflow

---

## Story Summary

**As a** data scientist preparing enterprise documents for RAG workflows
**I want** text chunked at semantic boundaries (sentences, paragraphs, sections)
**So that** LLM retrievals maintain complete context without mid-sentence splits

---

## Test Coverage Summary

| AC ID | Description | Happy Path | Edge Cases | Error Cases | Integration | Total Tests |
|-------|-------------|------------|------------|-------------|-------------|-------------|
| AC-3.1-1 | Chunks Never Split Mid-Sentence | ✓ | ✓ (3) | ✓ (1) | ✓ (1) | 6 |
| AC-3.1-2 | Section Boundaries Respected | ✓ | ✓ (2) | ✓ (1) | ✓ (1) | 5 |
| AC-3.1-3 | Chunk Size Configurable | ✓ | ✓ (2) | ✓ (1) | - | 4 |
| AC-3.1-4 | Chunk Overlap Configurable | ✓ | ✓ (2) | ✓ (1) | - | 4 |
| AC-3.1-5 | Sentence Tokenization Uses spaCy | ✓ | ✓ (1) | - | ✓ (2) | 4 |
| AC-3.1-6 | Edge Cases Handled | ✓ | ✓ (4) | ✓ (1) | ✓ (1) | 7 |
| AC-3.1-7 | Chunking is Deterministic | ✓ | ✓ (2) | - | ✓ (1) | 4 |

**Test Type Distribution**:
- Unit tests: 24
- Integration tests: 7
- CLI tests: 0
- Manual tests: 3
- Performance tests: 0
- **Total test cases**: 34

---

## Test Cases

### AC-3.1-1: Chunks Never Split Mid-Sentence (P0 - Critical)

#### Test Case TC-3.1-1-HP-01

**Acceptance Criterion**: AC-3.1-1
**Test Type**: Unit
**Scenario**: Happy Path

**Objective**: Verify that basic chunking respects sentence boundaries in typical documents

**Preconditions**:
- ChunkingEngine initialized with default config (chunk_size=512, overlap_pct=0.15)
- ProcessingResult fixture with 5 paragraphs, ~2000 words, normal sentence structure

**Test Steps**:
1. Load ProcessingResult fixture `tests/fixtures/normalized_results/typical_policy.json`
2. Call `ChunkingEngine.chunk_document(result)`
3. Iterate through all yielded chunks
4. For each chunk, verify last character is sentence-ending punctuation (., !, ?, or :")

**Expected Results**:
- All chunks end with complete sentences
- No chunk ends mid-sentence (verified by punctuation check)
- ~4-6 chunks generated (2000 words / 512 tokens ≈ 4 chunks)

**Test Data**:
- Input: ProcessingResult with normalized policy text (5 paragraphs)
- Expected Output: 4-6 chunks, all ending with sentence boundary

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: None
- Configuration: Default ChunkingEngine config

---

#### Test Case TC-3.1-1-EC-01

**Acceptance Criterion**: AC-3.1-1
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify that very long sentences (>chunk_size) become single chunks without splitting

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with single sentence exceeding 512 tokens (~2048 characters)

**Test Steps**:
1. Create ProcessingResult with very long sentence (e.g., complex legal clause with 700 tokens)
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify exactly 1 chunk generated
5. Verify chunk contains entire sentence (no truncation)
6. Verify warning logged: "Sentence exceeds chunk_size"

**Expected Results**:
- Single chunk generated containing entire sentence
- Chunk size exceeds configured chunk_size (700 > 512)
- Warning message logged to structlog
- No errors raised

**Test Data**:
- Input: ProcessingResult with 700-token sentence (simulated legal clause)
- Expected Output: 1 chunk with 700 tokens

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/very_long_sentences.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-1-EC-02

**Acceptance Criterion**: AC-3.1-1
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify that micro-sentences (<10 chars) are combined until chunk_size reached

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with multiple micro-sentences (e.g., "Yes. No. Maybe. OK.")

**Test Steps**:
1. Create ProcessingResult with 20 micro-sentences (<10 chars each)
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify chunks combine multiple micro-sentences
5. Verify no chunk contains only a single micro-sentence

**Expected Results**:
- Multiple micro-sentences combined into chunks
- Each chunk ≥ chunk_size (or close to it)
- Sentence boundaries preserved within combined chunk
- No single-micro-sentence chunks (unless document ends)

**Test Data**:
- Input: ProcessingResult with 20 micro-sentences
- Expected Output: 1-2 chunks with combined sentences

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/micro_sentences.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-1-EC-03

**Acceptance Criterion**: AC-3.1-1
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify chunking handles text with no punctuation using spaCy statistical model

**Preconditions**:
- ChunkingEngine initialized with default config
- ProcessingResult with continuous text, no punctuation marks

**Test Steps**:
1. Create ProcessingResult with 1000-word text, no periods/commas/punctuation
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify chunks are generated (spaCy statistical model detects boundaries)
5. Verify chunks have reasonable size distribution

**Expected Results**:
- Chunks generated successfully (no errors)
- spaCy statistical model detects sentence boundaries
- Chunk sizes roughly align with chunk_size (±20% variance acceptable)
- No empty chunks

**Test Data**:
- Input: ProcessingResult with continuous unpunctuated text
- Expected Output: 2-3 chunks with statistical boundaries

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/no_punctuation.json`
- Helpers: None
- Configuration: Default

---

#### Test Case TC-3.1-1-ERR-01

**Acceptance Criterion**: AC-3.1-1
**Test Type**: Unit
**Scenario**: Error Case

**Objective**: Verify empty ProcessingResult produces zero chunks without errors

**Preconditions**:
- ChunkingEngine initialized with default config
- ProcessingResult with empty normalized text

**Test Steps**:
1. Create ProcessingResult with normalized_text=""
2. Call `ChunkingEngine.chunk_document(result)`
3. Iterate through chunks
4. Verify iterator returns immediately (no chunks yielded)
5. Verify info log: "Empty normalized document: {source_file}"

**Expected Results**:
- Zero chunks yielded
- No exceptions raised
- Info log message confirms empty document
- Iterator completes successfully

**Test Data**:
- Input: ProcessingResult with normalized_text=""
- Expected Output: Empty iterator (0 chunks)

**Dependencies**:
- Fixtures: None (inline creation)
- Helpers: None
- Configuration: Default

---

#### Test Case TC-3.1-1-INT-01

**Acceptance Criterion**: AC-3.1-1
**Test Type**: Integration
**Scenario**: Integration

**Objective**: Validate sentence boundary preservation with real audit documents (SOC2 report)

**Preconditions**:
- ChunkingEngine initialized with default config
- Real SOC2 report ProcessingResult fixture (Epic 2 output)

**Test Steps**:
1. Load `tests/fixtures/normalized_results/soc2_report.json` (Epic 2 output)
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. For each chunk:
   - Verify ends with sentence boundary
   - Verify no mid-sentence splits (regex check for incomplete syntax)
5. Manually inspect 3 random chunks for coherence

**Expected Results**:
- All chunks end with complete sentences
- No incomplete clauses at chunk boundaries
- Section headers properly handled (e.g., "Risk Assessment" becomes chunk prefix)
- Entity tags preserved from Epic 2

**Test Data**:
- Input: SOC2 report ProcessingResult from Epic 2
- Expected Output: 15-25 chunks with complete sentences

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/soc2_report.json`
- Helpers: None
- Configuration: Default

---

### AC-3.1-2: Section Boundaries Respected When Possible (P0)

#### Test Case TC-3.1-2-HP-01

**Acceptance Criterion**: AC-3.1-2
**Test Type**: Unit
**Scenario**: Happy Path

**Objective**: Verify chunking aligns with section boundaries when chunk_size permits

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with 3 sections, each ~400 tokens (fits in chunk_size)

**Test Steps**:
1. Create ProcessingResult with ContentBlocks marking section boundaries
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify 3 chunks generated (one per section)
5. Verify each chunk's metadata.section_context matches section title

**Expected Results**:
- 3 chunks generated (one per section)
- Chunk 1 metadata.section_context = "Section 1: Overview"
- Chunk 2 metadata.section_context = "Section 2: Methodology"
- Chunk 3 metadata.section_context = "Section 3: Findings"
- No section split across multiple chunks

**Test Data**:
- Input: ProcessingResult with 3 sections (400 tokens each)
- Expected Output: 3 chunks aligned with sections

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/multi_section_policy.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-2-EC-01

**Acceptance Criterion**: AC-3.1-2
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify large sections (>chunk_size) are split at sentence boundaries within section

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with single section = 2000 tokens

**Test Steps**:
1. Create ProcessingResult with large section (2000 tokens)
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify multiple chunks generated (~4 chunks)
5. Verify all chunks have same metadata.section_context (section preserved)
6. Verify splits occur at sentence boundaries (no mid-sentence splits)

**Expected Results**:
- ~4 chunks generated (2000 tokens / 512 = ~4)
- All chunks share section_context = "Risk Assessment > Identified Risks"
- All chunks end with sentence boundaries
- Overlap preserved between chunks (15% default)

**Test Data**:
- Input: ProcessingResult with 2000-token section
- Expected Output: 4 chunks with shared section_context

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/large_section.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-2-EC-02

**Acceptance Criterion**: AC-3.1-2
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify short sections (<chunk_size) become single chunks without artificial splitting

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with 5 short sections (100 tokens each)

**Test Steps**:
1. Create ProcessingResult with 5 short sections (100 tokens each)
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify section grouping behavior (multiple sections may combine if total <chunk_size)
5. Verify no section artificially split

**Expected Results**:
- Sections combined intelligently (e.g., 2-3 sections per chunk if total <512)
- No single section split across chunks
- metadata.section_context lists all sections in combined chunk (e.g., "Section A, Section B")

**Test Data**:
- Input: ProcessingResult with 5 short sections (100 tokens each)
- Expected Output: 2-3 chunks with combined sections

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/short_sections.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-2-ERR-01

**Acceptance Criterion**: AC-3.1-2
**Test Type**: Unit
**Scenario**: Error Case

**Objective**: Verify ProcessingResult without section markers chunks by sentence boundaries only

**Preconditions**:
- ChunkingEngine initialized with default config
- ProcessingResult with no ContentBlocks (plain normalized text)

**Test Steps**:
1. Create ProcessingResult with normalized_text, no ContentBlocks
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify chunks generated based on sentence boundaries only
5. Verify metadata.section_context = None or ""

**Expected Results**:
- Chunks generated successfully (no errors)
- Chunking based on sentence boundaries only (no section alignment)
- metadata.section_context empty for all chunks
- No warnings/errors logged

**Test Data**:
- Input: ProcessingResult with plain text (no sections)
- Expected Output: Chunks based on sentence boundaries

**Dependencies**:
- Fixtures: None (inline creation)
- Helpers: None
- Configuration: Default

---

#### Test Case TC-3.1-2-INT-01

**Acceptance Criterion**: AC-3.1-2
**Test Type**: Integration
**Scenario**: Integration

**Objective**: Validate section boundary preservation with real risk register document

**Preconditions**:
- ChunkingEngine initialized with default config
- Real risk register ProcessingResult fixture (Epic 2 output)

**Test Steps**:
1. Load `tests/fixtures/normalized_results/risk_register.json`
2. Call `ChunkingEngine.chunk_document(result)`
3. Collect all chunks
4. Verify section_context populated for each chunk
5. Manually verify 3 random chunks align with expected sections

**Expected Results**:
- All chunks have populated section_context metadata
- Section hierarchy preserved (e.g., "Risk Assessment > Identified Risks > Financial Risks")
- Large sections split at sentence boundaries, preserving section_context
- Short sections combined intelligently

**Test Data**:
- Input: Risk register ProcessingResult from Epic 2
- Expected Output: 10-20 chunks with section metadata

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/risk_register.json`
- Helpers: None
- Configuration: Default

---

### AC-3.1-3: Chunk Size Configurable (P1)

#### Test Case TC-3.1-3-HP-01

**Acceptance Criterion**: AC-3.1-3
**Test Type**: Unit
**Scenario**: Happy Path

**Objective**: Verify ChunkingEngine accepts and applies various chunk_size configurations

**Preconditions**:
- ProcessingResult fixture with 2000-word document

**Test Steps**:
1. Initialize ChunkingEngine with chunk_size=256
2. Chunk document, count chunks (expect ~8 chunks)
3. Initialize ChunkingEngine with chunk_size=512
4. Chunk same document, count chunks (expect ~4 chunks)
5. Initialize ChunkingEngine with chunk_size=1024
6. Chunk same document, count chunks (expect ~2 chunks)

**Expected Results**:
- chunk_size=256 → ~8 chunks
- chunk_size=512 → ~4 chunks
- chunk_size=1024 → ~2 chunks
- Chunk counts inversely proportional to chunk_size
- All configurations execute without errors

**Test Data**:
- Input: ProcessingResult with 2000-word document
- Expected Output: Varying chunk counts based on size config

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: None
- Configuration: Parameterized chunk_size (256, 512, 1024)

---

#### Test Case TC-3.1-3-EC-01

**Acceptance Criterion**: AC-3.1-3
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify chunk_size at lower boundary (128 tokens) works with warning

**Preconditions**:
- ChunkingEngine initialized with chunk_size=128
- ProcessingResult fixture with 500-word document

**Test Steps**:
1. Initialize ChunkingEngine with chunk_size=128
2. Verify warning logged: "chunk_size=128 is below recommended minimum (256)"
3. Call `chunk_document(result)`
4. Collect all chunks
5. Verify chunks average ~128 tokens (±10%)

**Expected Results**:
- Warning logged during initialization
- Chunking proceeds successfully
- ~10-15 chunks generated (500 words / 128 tokens)
- Chunks respect sentence boundaries (may exceed 128 if sentence long)

**Test Data**:
- Input: ProcessingResult with 500-word document
- Expected Output: 10-15 chunks with ~128 tokens each

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: None
- Configuration: chunk_size=128

---

#### Test Case TC-3.1-3-EC-02

**Acceptance Criterion**: AC-3.1-3
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify chunk_size at upper boundary (2048 tokens) works with warning

**Preconditions**:
- ChunkingEngine initialized with chunk_size=2048
- ProcessingResult fixture with 3000-word document

**Test Steps**:
1. Initialize ChunkingEngine with chunk_size=2048
2. Verify warning logged: "chunk_size=2048 is above recommended maximum (1024)"
3. Call `chunk_document(result)`
4. Collect all chunks
5. Verify chunks average ~2048 tokens (±10%)

**Expected Results**:
- Warning logged during initialization
- Chunking proceeds successfully
- ~3-4 chunks generated (3000 words / 2048 tokens)
- Large chunks may impact LLM retrieval (warning exists for this reason)

**Test Data**:
- Input: ProcessingResult with 3000-word document
- Expected Output: 3-4 chunks with ~2048 tokens each

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/large_document.json`
- Helpers: None
- Configuration: chunk_size=2048

---

#### Test Case TC-3.1-3-ERR-01

**Acceptance Criterion**: AC-3.1-3
**Test Type**: Unit
**Scenario**: Error Case

**Objective**: Verify invalid chunk_size (size=1) raises validation error

**Preconditions**:
- Attempt to initialize ChunkingEngine with chunk_size=1

**Test Steps**:
1. Attempt to initialize ChunkingEngine with chunk_size=1
2. Expect ValueError or ValidationError
3. Verify error message: "chunk_size must be ≥ 128 tokens"

**Expected Results**:
- ValueError raised during initialization
- Clear error message with valid range (128-2048)
- No engine instance created

**Test Data**:
- Input: chunk_size=1 (invalid)
- Expected Output: ValueError with actionable message

**Dependencies**:
- Fixtures: None
- Helpers: None
- Configuration: chunk_size=1 (invalid)

---

### AC-3.1-4: Chunk Overlap Configurable (P1)

#### Test Case TC-3.1-4-HP-01

**Acceptance Criterion**: AC-3.1-4
**Test Type**: Unit
**Scenario**: Happy Path

**Objective**: Verify ChunkingEngine applies overlap_pct correctly in sliding window

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512, overlap_pct=0.15
- ProcessingResult with 2000-word document

**Test Steps**:
1. Initialize ChunkingEngine with chunk_size=512, overlap_pct=0.15
2. Chunk document, collect chunks
3. Verify overlap_tokens = int(512 * 0.15) = 76 tokens
4. Verify last 76 tokens of chunk[i] appear in first 76 tokens of chunk[i+1]
5. Test with overlap_pct=0.0 (no overlap), verify no token overlap
6. Test with overlap_pct=0.2 (20% overlap), verify 102-token overlap

**Expected Results**:
- overlap_pct=0.15 → 76-token overlap between adjacent chunks
- overlap_pct=0.0 → 0-token overlap (chunks are contiguous)
- overlap_pct=0.2 → 102-token overlap
- Overlap calculation consistent across all chunk pairs

**Test Data**:
- Input: ProcessingResult with 2000-word document
- Expected Output: Chunks with specified overlap percentage

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: Token overlap verification function
- Configuration: Parameterized overlap_pct (0.0, 0.15, 0.2)

---

#### Test Case TC-3.1-4-EC-01

**Acceptance Criterion**: AC-3.1-4
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify overlap_pct at lower boundary (0.0) produces contiguous chunks

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512, overlap_pct=0.0
- ProcessingResult with 1000-word document

**Test Steps**:
1. Initialize ChunkingEngine with overlap_pct=0.0
2. Chunk document, collect chunks
3. Verify no token overlap between adjacent chunks
4. Verify chunks are contiguous (chunk[i] end position = chunk[i+1] start position)
5. Verify no gaps in coverage (all sentences included)

**Expected Results**:
- Zero token overlap between chunks
- Contiguous coverage (no gaps)
- No duplicate content across chunks
- All document sentences included in exactly one chunk

**Test Data**:
- Input: ProcessingResult with 1000-word document
- Expected Output: Contiguous chunks with 0% overlap

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: Contiguity verification function
- Configuration: overlap_pct=0.0

---

#### Test Case TC-3.1-4-EC-02

**Acceptance Criterion**: AC-3.1-4
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify overlap_pct at upper boundary (0.5) produces 50% overlap with warning

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512, overlap_pct=0.5
- ProcessingResult with 1000-word document

**Test Steps**:
1. Initialize ChunkingEngine with overlap_pct=0.5
2. Verify warning logged: "overlap_pct=0.5 is high and may cause excessive duplication"
3. Chunk document, collect chunks
4. Verify overlap_tokens = int(512 * 0.5) = 256 tokens
5. Verify 50% overlap between adjacent chunks

**Expected Results**:
- Warning logged during initialization
- 256-token overlap (50% of chunk_size)
- Chunking proceeds successfully
- Significant content duplication across chunks (expected with 50% overlap)

**Test Data**:
- Input: ProcessingResult with 1000-word document
- Expected Output: Chunks with 50% overlap

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: Token overlap verification function
- Configuration: overlap_pct=0.5

---

#### Test Case TC-3.1-4-ERR-01

**Acceptance Criterion**: AC-3.1-4
**Test Type**: Unit
**Scenario**: Error Case

**Objective**: Verify invalid overlap_pct (overlap=1.0) raises validation error

**Preconditions**:
- Attempt to initialize ChunkingEngine with overlap_pct=1.0 (100%)

**Test Steps**:
1. Attempt to initialize ChunkingEngine with overlap_pct=1.0
2. Expect ValueError or ValidationError
3. Verify error message: "overlap_pct must be in range 0.0-0.5"

**Expected Results**:
- ValueError raised during initialization
- Clear error message with valid range (0.0-0.5)
- No engine instance created

**Test Data**:
- Input: overlap_pct=1.0 (invalid)
- Expected Output: ValueError with actionable message

**Dependencies**:
- Fixtures: None
- Helpers: None
- Configuration: overlap_pct=1.0 (invalid)

---

### AC-3.1-5: Sentence Tokenization Uses spaCy (P0)

#### Test Case TC-3.1-5-HP-01

**Acceptance Criterion**: AC-3.1-5
**Test Type**: Unit
**Scenario**: Happy Path

**Objective**: Verify ChunkingEngine uses injected SentenceSegmenter dependency

**Preconditions**:
- Mock SentenceSegmenter for fast unit testing
- ChunkingEngine initialized with mock segmenter

**Test Steps**:
1. Create mock SentenceSegmenter that tracks method calls
2. Initialize ChunkingEngine with mock segmenter
3. Call `chunk_document(result)`
4. Verify mock segmenter's `segment()` method was called
5. Verify segmenter called once per document (not per chunk)

**Expected Results**:
- SentenceSegmenter.segment() called exactly once
- ChunkingEngine does not directly import spaCy (dependency injection pattern)
- Mock segmenter returns controlled sentence list
- Chunks generated based on mock sentences

**Test Data**:
- Input: ProcessingResult with controlled text
- Expected Output: Chunks based on mock segmenter output

**Dependencies**:
- Fixtures: None (inline mock)
- Helpers: Mock SentenceSegmenter
- Configuration: Default

---

#### Test Case TC-3.1-5-EC-01

**Acceptance Criterion**: AC-3.1-5
**Test Type**: Integration
**Scenario**: Edge Case

**Objective**: Verify spaCy model loaded lazily on first use (not during __init__)

**Preconditions**:
- Real SentenceSegmenter (not mocked)
- ChunkingEngine initialized with real segmenter

**Test Steps**:
1. Initialize SentenceSegmenter (model not loaded yet)
2. Verify spaCy model NOT loaded (check global cache)
3. Initialize ChunkingEngine with segmenter
4. Call `chunk_document(result)` for first time
5. Verify spaCy model NOW loaded (lazy loading triggered)
6. Call `chunk_document(result)` for second document
7. Verify model NOT reloaded (global cache reused)

**Expected Results**:
- spaCy model not loaded during __init__
- Model loaded on first `chunk_document()` call
- Model loading time ~1.2 seconds (acceptable one-time cost)
- Subsequent calls reuse cached model (no reload overhead)

**Test Data**:
- Input: Two ProcessingResult fixtures
- Expected Output: Model loaded once, cached for reuse

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: None
- Configuration: Default with real SentenceSegmenter

---

#### Test Case TC-3.1-5-INT-01

**Acceptance Criterion**: AC-3.1-5
**Test Type**: Integration
**Scenario**: Integration

**Objective**: Verify spaCy model version logged in ChunkMetadata.processing_version

**Preconditions**:
- ChunkingEngine initialized with real SentenceSegmenter
- spaCy model en_core_web_md installed

**Test Steps**:
1. Load ProcessingResult fixture
2. Call `chunk_document(result)`
3. Collect first chunk
4. Verify chunk.metadata.processing_version contains spaCy model version
5. Verify version string format: "en_core_web_md-3.7.2" or similar

**Expected Results**:
- processing_version field populated
- Version string includes model name (en_core_web_md) and version (3.7.x)
- Version consistent across all chunks in same document
- Version enables reproducibility tracking (same model → same results)

**Test Data**:
- Input: ProcessingResult fixture
- Expected Output: Chunks with processing_version metadata

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: None
- Configuration: Default with real SentenceSegmenter

---

#### Test Case TC-3.1-5-INT-02

**Acceptance Criterion**: AC-3.1-5
**Test Type**: Integration
**Scenario**: Integration

**Objective**: Validate spaCy sentence boundary detection accuracy with real documents

**Preconditions**:
- ChunkingEngine initialized with real SentenceSegmenter
- ProcessingResult from real audit document (Epic 2 output)

**Test Steps**:
1. Load `tests/fixtures/normalized_results/soc2_report.json`
2. Call `chunk_document(result)`
3. Collect all chunks
4. Manually review 5 random chunk boundaries
5. Verify sentence boundaries are accurate (no mid-sentence splits)
6. Compare with expected sentence count from fixture metadata

**Expected Results**:
- 95%+ sentence boundary accuracy (spaCy model accuracy from Story 2.5.2)
- All chunk boundaries align with sentence boundaries
- No mid-sentence splits observed
- Sentence count matches fixture metadata (±2% tolerance for edge cases)

**Test Data**:
- Input: SOC2 report ProcessingResult from Epic 2
- Expected Output: Chunks with accurate sentence boundaries

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/soc2_report.json`
- Helpers: None
- Configuration: Default with real SentenceSegmenter

---

### AC-3.1-6: Edge Cases Handled (P0)

#### Test Case TC-3.1-6-HP-01

**Acceptance Criterion**: AC-3.1-6
**Test Type**: Unit
**Scenario**: Happy Path

**Objective**: Verify typical document (mixed sentence lengths) chunks successfully

**Preconditions**:
- ChunkingEngine initialized with default config
- ProcessingResult with mixed sentence lengths (short, medium, long)

**Test Steps**:
1. Create ProcessingResult with varied sentence structure
2. Call `chunk_document(result)`
3. Collect all chunks
4. Verify no errors raised
5. Verify reasonable chunk count (~4-6 for 2000-word doc)

**Expected Results**:
- Chunking completes successfully
- No warnings/errors logged
- Chunks have reasonable size distribution
- All sentences included in chunks

**Test Data**:
- Input: ProcessingResult with mixed sentence structure
- Expected Output: 4-6 chunks with varied content

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: None
- Configuration: Default

---

#### Test Case TC-3.1-6-EC-01

**Acceptance Criterion**: AC-3.1-6
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify very long sentence (>chunk_size) becomes single chunk with warning

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with 700-token sentence

**Test Steps**:
1. Create ProcessingResult with single 700-token sentence
2. Call `chunk_document(result)`
3. Collect chunks
4. Verify exactly 1 chunk generated
5. Verify chunk contains entire sentence (700 tokens)
6. Verify warning logged: "Sentence exceeds chunk_size (700 > 512)"

**Expected Results**:
- Single chunk with 700 tokens (exceeds chunk_size)
- Warning logged to structlog
- No errors raised
- Sentence not truncated or split

**Test Data**:
- Input: ProcessingResult with 700-token sentence
- Expected Output: 1 chunk with 700 tokens + warning

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/very_long_sentences.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-6-EC-02

**Acceptance Criterion**: AC-3.1-6
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify micro-sentences (<10 chars) are combined until chunk_size reached

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with 20 micro-sentences

**Test Steps**:
1. Create ProcessingResult with 20 micro-sentences ("Yes.", "No.", "OK.", etc.)
2. Call `chunk_document(result)`
3. Collect chunks
4. Verify micro-sentences combined into chunks
5. Verify each chunk ≥ 400 tokens (close to chunk_size)

**Expected Results**:
- 1-2 chunks generated (all micro-sentences combined)
- Each chunk contains multiple micro-sentences
- Sentence boundaries preserved in combined chunk
- No single-micro-sentence chunks

**Test Data**:
- Input: ProcessingResult with 20 micro-sentences
- Expected Output: 1-2 chunks with combined content

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/micro_sentences.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-6-EC-03

**Acceptance Criterion**: AC-3.1-6
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify short section (<chunk_size) becomes single chunk without artificial splitting

**Preconditions**:
- ChunkingEngine initialized with chunk_size=512
- ProcessingResult with 200-token section

**Test Steps**:
1. Create ProcessingResult with single 200-token section
2. Call `chunk_document(result)`
3. Collect chunks
4. Verify exactly 1 chunk generated
5. Verify chunk contains entire section (no artificial split)
6. Verify chunk size = 200 tokens (no padding added)

**Expected Results**:
- Single chunk with 200 tokens (below chunk_size)
- No artificial splitting or padding
- Section context preserved in metadata
- No warnings/errors

**Test Data**:
- Input: ProcessingResult with 200-token section
- Expected Output: 1 chunk with 200 tokens

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/short_sections.json`
- Helpers: None
- Configuration: chunk_size=512

---

#### Test Case TC-3.1-6-EC-04

**Acceptance Criterion**: AC-3.1-6
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify empty normalized document produces zero chunks with info log

**Preconditions**:
- ChunkingEngine initialized with default config
- ProcessingResult with normalized_text=""

**Test Steps**:
1. Create ProcessingResult with empty normalized_text
2. Call `chunk_document(result)`
3. Iterate through chunks
4. Verify zero chunks yielded
5. Verify info log: "Empty normalized document: {source_file}"

**Expected Results**:
- Zero chunks generated
- Info log message confirms empty document
- No errors raised
- Iterator completes successfully

**Test Data**:
- Input: ProcessingResult with normalized_text=""
- Expected Output: Empty iterator (0 chunks)

**Dependencies**:
- Fixtures: None (inline creation)
- Helpers: None
- Configuration: Default

---

#### Test Case TC-3.1-6-ERR-01

**Acceptance Criterion**: AC-3.1-6
**Test Type**: Unit
**Scenario**: Error Case

**Objective**: Verify ProcessingResult with None normalized_text raises clear error

**Preconditions**:
- ChunkingEngine initialized with default config
- ProcessingResult with normalized_text=None (invalid)

**Test Steps**:
1. Create ProcessingResult with normalized_text=None
2. Call `chunk_document(result)`
3. Expect ValueError or TypeError
4. Verify error message: "normalized_text cannot be None"

**Expected Results**:
- ValueError or TypeError raised
- Clear error message with actionable suggestion
- No chunks generated

**Test Data**:
- Input: ProcessingResult with normalized_text=None
- Expected Output: ValueError with clear message

**Dependencies**:
- Fixtures: None (inline creation)
- Helpers: None
- Configuration: Default

---

#### Test Case TC-3.1-6-INT-01

**Acceptance Criterion**: AC-3.1-6
**Test Type**: Integration
**Scenario**: Integration

**Objective**: Validate edge case handling with real edge case documents (10,000-word single sentence)

**Preconditions**:
- ChunkingEngine initialized with default config
- Real edge case fixture (10,000-word run-on sentence)

**Test Steps**:
1. Load `tests/fixtures/normalized_results/extreme_long_sentence.json`
2. Call `chunk_document(result)`
3. Collect chunks
4. Verify single chunk generated containing entire sentence
5. Verify warning logged for extreme sentence length

**Expected Results**:
- Single chunk with ~10,000 words (extreme case)
- Warning logged: "Sentence exceeds chunk_size by 20x"
- No errors raised (graceful handling)
- Entire sentence preserved (no truncation)

**Test Data**:
- Input: ProcessingResult with 10,000-word sentence
- Expected Output: 1 extreme chunk + warning

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/extreme_long_sentence.json`
- Helpers: None
- Configuration: Default

---

### AC-3.1-7: Chunking is Deterministic (P0 - Critical)

#### Test Case TC-3.1-7-HP-01

**Acceptance Criterion**: AC-3.1-7
**Test Type**: Unit
**Scenario**: Happy Path

**Objective**: Verify same ProcessingResult produces identical chunks across 10 runs

**Preconditions**:
- ChunkingEngine initialized with fixed config (chunk_size=512, overlap_pct=0.15)
- ProcessingResult fixture with 2000-word document

**Test Steps**:
1. Load ProcessingResult fixture
2. Chunk document 10 times with same configuration
3. For each run, serialize chunks to JSON
4. Perform byte-for-byte comparison of all 10 outputs
5. Verify all outputs are identical

**Expected Results**:
- All 10 runs produce identical chunks (byte-for-byte)
- Chunk IDs identical across runs
- Chunk text identical across runs
- Chunk metadata identical across runs (except created_at if timestamp included)

**Test Data**:
- Input: ProcessingResult fixture
- Expected Output: 10 identical chunk outputs

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: JSON serialization, byte comparison
- Configuration: Fixed (chunk_size=512, overlap_pct=0.15)

---

#### Test Case TC-3.1-7-EC-01

**Acceptance Criterion**: AC-3.1-7
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify chunk_id generation is deterministic (derived from source + position)

**Preconditions**:
- ChunkingEngine initialized with default config
- ProcessingResult with source_file="test_policy.pdf"

**Test Steps**:
1. Create ProcessingResult with source_file="test_policy.pdf"
2. Chunk document, collect chunks
3. Verify chunk IDs follow pattern: `test_policy_chunk_000`, `test_policy_chunk_001`, etc.
4. Verify no timestamps in chunk IDs
5. Verify no random components in chunk IDs
6. Chunk same document again, verify identical chunk IDs

**Expected Results**:
- Chunk IDs follow deterministic pattern: `{source_file_stem}_chunk_{position:03d}`
- Position counter zero-padded (000, 001, 002)
- No timestamps or random values in IDs
- Identical IDs across multiple runs

**Test Data**:
- Input: ProcessingResult with source_file="test_policy.pdf"
- Expected Output: Chunks with deterministic IDs

**Dependencies**:
- Fixtures: None (inline creation)
- Helpers: None
- Configuration: Default

---

#### Test Case TC-3.1-7-EC-02

**Acceptance Criterion**: AC-3.1-7
**Test Type**: Unit
**Scenario**: Edge Case

**Objective**: Verify different configurations produce different chunks (sensitivity test)

**Preconditions**:
- Same ProcessingResult fixture
- Two different ChunkingEngine configurations

**Test Steps**:
1. Chunk document with chunk_size=512, overlap_pct=0.15
2. Chunk same document with chunk_size=256, overlap_pct=0.15
3. Verify outputs are DIFFERENT (configuration sensitivity)
4. Chunk document again with chunk_size=512, overlap_pct=0.15 (original config)
5. Verify output matches first run (determinism with same config)

**Expected Results**:
- Different configs → different chunks
- Same config → identical chunks (determinism)
- Configuration embedded in chunk metadata for traceability

**Test Data**:
- Input: ProcessingResult fixture
- Expected Output: Different chunks for different configs, identical for same config

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/typical_policy.json`
- Helpers: None
- Configuration: Parameterized (chunk_size=256/512)

---

#### Test Case TC-3.1-7-INT-01

**Acceptance Criterion**: AC-3.1-7
**Test Type**: Integration
**Scenario**: Integration

**Objective**: Validate determinism with real audit documents across spaCy model versions

**Preconditions**:
- ChunkingEngine initialized with real SentenceSegmenter
- Real audit document ProcessingResult fixture
- spaCy model version pinned (en_core_web_md==3.7.2)

**Test Steps**:
1. Load `tests/fixtures/normalized_results/soc2_report.json`
2. Chunk document 5 times with same config
3. Serialize chunks to JSON for each run
4. Perform byte-for-byte comparison
5. Verify spaCy model version in metadata matches pinned version (3.7.2)

**Expected Results**:
- All 5 runs produce identical chunks
- spaCy model version consistent: "en_core_web_md-3.7.2"
- Byte-for-byte output match (deterministic sentence boundaries)
- Audit trail enabled (chunk traceability via deterministic IDs)

**Test Data**:
- Input: SOC2 report ProcessingResult
- Expected Output: 5 identical chunk outputs with version metadata

**Dependencies**:
- Fixtures: `tests/fixtures/normalized_results/soc2_report.json`
- Helpers: JSON serialization, byte comparison
- Configuration: Default with pinned spaCy version

---

## Testing Notes

### Execution Recommendations

**Test Execution Order** (optimal sequence):
1. **Unit tests** (fast feedback, ~5 min):
   - Configuration tests (TC-3.1-3-*, TC-3.1-4-*)
   - Edge case tests (TC-3.1-1-EC-*, TC-3.1-6-EC-*)
   - Determinism tests (TC-3.1-7-HP-01, TC-3.1-7-EC-*)
2. **Integration tests** (~10 min):
   - spaCy integration (TC-3.1-5-INT-*)
   - Real document tests (TC-3.1-1-INT-01, TC-3.1-2-INT-01)
   - Edge case integration (TC-3.1-6-INT-01)
3. **Manual validation** (~15 min):
   - Manual review of 5 random chunk boundaries (TC-3.1-1-INT-01 step 5)
   - Section alignment verification (TC-3.1-2-INT-01 step 5)
   - spaCy accuracy spot check (TC-3.1-5-INT-02 step 4)

**Critical Tests (Must Pass - Blockers)**:
- TC-3.1-1-HP-01 (sentence boundary preservation - happy path)
- TC-3.1-1-EC-01 (very long sentence handling)
- TC-3.1-7-HP-01 (determinism - 10 runs)
- TC-3.1-7-INT-01 (determinism with real documents)
- TC-3.1-6-EC-01 (very long sentence edge case)

**Parallelizable Tests**:
- All unit tests (TC-3.1-1-*, TC-3.1-2-*, TC-3.1-3-*, TC-3.1-4-*, TC-3.1-6-*)
- Configuration tests can run in parallel
- Determinism tests (TC-3.1-7-*) can run in parallel if using separate fixtures

**Sequential Tests** (dependencies):
- spaCy integration tests (TC-3.1-5-EC-01) must run before model version tests (TC-3.1-5-INT-01)
- Real document tests depend on fixture availability

**Estimated Test Execution Time**:
- Unit tests: ~5 minutes (24 tests, fast mocks)
- Integration tests: ~10 minutes (7 tests, real spaCy model)
- Manual tests: ~15 minutes (3 tests, human verification)
- **Total**: ~30 minutes for full suite

---

### Implementation Notes

**Suggested pytest Markers**:
```python
# pytest.ini additions
[pytest]
markers =
    chunking: All chunking-related tests (Epic 3)
    sentence_boundary: Sentence boundary preservation tests (AC-3.1-1)
    determinism: Determinism and reproducibility tests (AC-3.1-7)
    edge_case: Edge case handling tests (AC-3.1-6)
    configuration: Configuration validation tests (AC-3.1-3, AC-3.1-4)
    spacy_integration: spaCy SentenceSegmenter integration tests (AC-3.1-5)
```

**Fixture Requirements**:
- `tests/fixtures/normalized_results/typical_policy.json` (Epic 2 output, 2000 words)
- `tests/fixtures/normalized_results/soc2_report.json` (Real SOC2 report, Epic 2 output)
- `tests/fixtures/normalized_results/risk_register.json` (Real risk register, Epic 2 output)
- `tests/fixtures/normalized_results/very_long_sentences.json` (700-token sentence)
- `tests/fixtures/normalized_results/micro_sentences.json` (20 micro-sentences)
- `tests/fixtures/normalized_results/no_punctuation.json` (1000-word unpunctuated text)
- `tests/fixtures/normalized_results/multi_section_policy.json` (3 sections, 400 tokens each)
- `tests/fixtures/normalized_results/large_section.json` (Single 2000-token section)
- `tests/fixtures/normalized_results/short_sections.json` (5 sections, 100 tokens each)
- `tests/fixtures/normalized_results/extreme_long_sentence.json` (10,000-word run-on)

**Helper Functions Needed**:
- `verify_token_overlap(chunk_i, chunk_i_plus_1, expected_overlap_tokens)` - Validates sliding window overlap
- `verify_sentence_boundary(chunk_text)` - Checks chunk ends with sentence punctuation
- `serialize_chunks_to_json(chunks)` - For determinism byte comparison
- `count_tokens(text)` - Token estimation (len(text) // 4)

**Integration Points to Mock/Stub**:
- **Unit tests**: Mock `SentenceSegmenter` for fast testing (controlled sentence lists)
- **Integration tests**: Use real `SentenceSegmenter` with en_core_web_md model
- **Performance tests**: No mocking (real-world conditions)

---

### Risk Areas

**Complex Scenarios Requiring Extra Attention**:
1. **Very Long Sentences (>chunk_size)**:
   - **Risk**: Trade-off between sentence coherence and chunk size uniformity
   - **Mitigation**: Comprehensive edge case testing (TC-3.1-1-EC-01, TC-3.1-6-EC-01)
   - **Validation**: Manual review of chunk size distribution with real audit documents

2. **Sliding Window Overlap Logic**:
   - **Risk**: Off-by-one errors, gaps or excessive duplication
   - **Mitigation**: Parameterized tests with overlap_pct (0.0, 0.15, 0.2, 0.5)
   - **Validation**: Token overlap verification helper function

3. **Determinism with spaCy Statistical Model**:
   - **Risk**: spaCy model updates break reproducibility
   - **Mitigation**: Pin en_core_web_md==3.7.2 in requirements, log version in metadata
   - **Validation**: Determinism tests across 10 runs (TC-3.1-7-HP-01, TC-3.1-7-INT-01)

**Potential Failure Points**:
1. **spaCy Model Loading**:
   - **Issue**: Model not installed or wrong version
   - **Detection**: Integration test TC-3.1-5-INT-01 validates version
   - **Recovery**: Clear error message with installation instructions

2. **Empty Documents**:
   - **Issue**: Edge case handling for empty normalized_text
   - **Detection**: TC-3.1-6-EC-04 validates zero chunks produced
   - **Recovery**: Info log message, no errors raised

3. **Memory Exhaustion (Large Documents)**:
   - **Issue**: Buffering all chunks exhausts memory
   - **Detection**: Not covered in this story (deferred to performance testing)
   - **Mitigation**: Streaming generator pattern (yield chunks one at a time)

**Performance Bottlenecks**:
1. **spaCy Sentence Segmentation** (~0.5 sec per 10k words):
   - **Impact**: Acceptable overhead (NFR-P3 allows <2 sec per 10k words)
   - **Optimization**: Lazy loading, global model cache (already implemented in Story 2.5.2)

2. **Token Counting** (len(text) // 4 approximation):
   - **Impact**: Negligible (<0.01 sec per document)
   - **Accuracy**: ±10% variance acceptable for chunking purposes

**Security Considerations**:
1. **Input Validation**:
   - **Risk**: Invalid chunk_size or overlap_pct crashes engine
   - **Mitigation**: Configuration validation in `__init__` with clear error messages
   - **Testing**: TC-3.1-3-ERR-01, TC-3.1-4-ERR-01

2. **Resource Exhaustion**:
   - **Risk**: Malicious very long sentences (>100k tokens) exhaust memory
   - **Mitigation**: Warning logged, graceful handling (entire sentence becomes chunk)
   - **Testing**: TC-3.1-6-INT-01 (10,000-word sentence edge case)

---

## Acceptance Criteria Coverage

### AC-3.1-1: Chunks Never Split Mid-Sentence (P0 - Critical)
- [x] **TC-3.1-1-HP-01**: Happy path sentence boundary preservation (unit)
- [x] **TC-3.1-1-EC-01**: Very long sentences >chunk_size (unit)
- [x] **TC-3.1-1-EC-02**: Micro-sentences <10 chars combined (unit)
- [x] **TC-3.1-1-EC-03**: No punctuation handled by spaCy (unit)
- [x] **TC-3.1-1-ERR-01**: Empty document produces zero chunks (unit)
- [x] **TC-3.1-1-INT-01**: Real SOC2 report sentence boundaries (integration)
- **Coverage**: 6 tests (happy path + 3 edge cases + 1 error + 1 integration)

### AC-3.1-2: Section Boundaries Respected When Possible (P0)
- [x] **TC-3.1-2-HP-01**: Section alignment when chunk_size permits (unit)
- [x] **TC-3.1-2-EC-01**: Large sections split at sentence boundaries (unit)
- [x] **TC-3.1-2-EC-02**: Short sections combined intelligently (unit)
- [x] **TC-3.1-2-ERR-01**: No section markers falls back to sentence boundaries (unit)
- [x] **TC-3.1-2-INT-01**: Real risk register section preservation (integration)
- **Coverage**: 5 tests (happy path + 2 edge cases + 1 error + 1 integration)

### AC-3.1-3: Chunk Size Configurable (P1)
- [x] **TC-3.1-3-HP-01**: Various chunk_size configurations (256, 512, 1024) (unit)
- [x] **TC-3.1-3-EC-01**: Chunk_size lower boundary (128) with warning (unit)
- [x] **TC-3.1-3-EC-02**: Chunk_size upper boundary (2048) with warning (unit)
- [x] **TC-3.1-3-ERR-01**: Invalid chunk_size (size=1) raises error (unit)
- **Coverage**: 4 tests (happy path + 2 edge cases + 1 error)

### AC-3.1-4: Chunk Overlap Configurable (P1)
- [x] **TC-3.1-4-HP-01**: Overlap_pct sliding window validation (0.0, 0.15, 0.2) (unit)
- [x] **TC-3.1-4-EC-01**: Overlap_pct lower boundary (0.0) contiguous chunks (unit)
- [x] **TC-3.1-4-EC-02**: Overlap_pct upper boundary (0.5) with warning (unit)
- [x] **TC-3.1-4-ERR-01**: Invalid overlap_pct (1.0) raises error (unit)
- **Coverage**: 4 tests (happy path + 2 edge cases + 1 error)

### AC-3.1-5: Sentence Tokenization Uses spaCy (P0)
- [x] **TC-3.1-5-HP-01**: SentenceSegmenter dependency injection (unit with mock)
- [x] **TC-3.1-5-EC-01**: spaCy model lazy loading (integration)
- [x] **TC-3.1-5-INT-01**: spaCy version in metadata (integration)
- [x] **TC-3.1-5-INT-02**: spaCy sentence boundary accuracy validation (integration)
- **Coverage**: 4 tests (happy path unit + 1 edge case + 2 integration)

### AC-3.1-6: Edge Cases Handled (P0)
- [x] **TC-3.1-6-HP-01**: Mixed sentence lengths (happy path) (unit)
- [x] **TC-3.1-6-EC-01**: Very long sentence >chunk_size (unit)
- [x] **TC-3.1-6-EC-02**: Micro-sentences combined (unit)
- [x] **TC-3.1-6-EC-03**: Short section becomes single chunk (unit)
- [x] **TC-3.1-6-EC-04**: Empty document produces zero chunks (unit)
- [x] **TC-3.1-6-ERR-01**: normalized_text=None raises error (unit)
- [x] **TC-3.1-6-INT-01**: Extreme long sentence (10,000 words) (integration)
- **Coverage**: 7 tests (happy path + 4 edge cases + 1 error + 1 integration)

### AC-3.1-7: Chunking is Deterministic (P0 - Critical)
- [x] **TC-3.1-7-HP-01**: Determinism across 10 runs (unit)
- [x] **TC-3.1-7-EC-01**: Chunk_id deterministic pattern (unit)
- [x] **TC-3.1-7-EC-02**: Configuration sensitivity test (unit)
- [x] **TC-3.1-7-INT-01**: Determinism with real documents + spaCy version (integration)
- **Coverage**: 4 tests (happy path + 2 edge cases + 1 integration)

**Total Coverage**: 34 test cases across 7 acceptance criteria
**Critical ACs**: 4 (AC-3.1-1, AC-3.1-2, AC-3.1-6, AC-3.1-7) - all have UAT coverage

---

## Next Steps

1. **Review test cases**: Validate coverage and scenarios
2. **Run build-test-context**: Gather fixtures and helpers → `workflow build-test-context`
3. **Implement missing fixtures**: Create any required test data
4. **Execute tests**: Run via execute-tests workflow → `workflow execute-tests`
5. **Review results**: QA review via review-uat-results workflow → `workflow review-uat-results`

---

**Document Status**: Ready for Context Building
**Generated by**: andrew using BMAD UAT Framework
