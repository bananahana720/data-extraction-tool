# Test Cases: 3-3 - Chunk Metadata and Quality Scoring

**Story**: 3-3
**Generated**: 2025-11-14
**Coverage Level**: standard
**Generated By**: create-test-cases workflow

---

## Story Summary

**As a** quality engineer preparing RAG workflows
**I want** each chunk enriched with comprehensive metadata and quality scores
**So that** RAG systems can filter, prioritize, and validate high-quality retrievals based on objective metrics

---

## Test Coverage Summary

### Coverage Matrix

| AC ID | Description | Happy Path | Edge Cases | Error Cases | Integration | Total Tests |
|-------|-------------|------------|------------|-------------|-------------|-------------|
| AC-3.3-4 | Readability scores calculated | ✓ | ✓ (3) | ✓ (1) | - | 5 |
| AC-3.3-5 | Composite quality score | ✓ (2) | ✓ (2) | - | ✓ (1) | 5 |
| AC-3.3-8 | Quality flags detection | ✓ | ✓ (4) | - | ✓ (1) | 6 |

### Test Type Distribution

- Unit tests: 0
- Integration tests: 3
- CLI tests: 0
- Manual tests: 13
- Performance tests: 0
- **Total test cases**: 16

---

## Test Cases

### AC-3.3-4: Readability Scores Calculated (Flesch-Kincaid, Gunning Fog)

#### Test Case UAT-3.3-4-1

**Acceptance Criterion**: AC-3.3-4
**Test Type**: Manual
**Scenario**: Happy Path

**Objective**: Validate readability scores for standard complexity text match expected ranges

**Preconditions**:
- ChunkingEngine configured with quality enrichment enabled
- Sample document with moderate complexity text available
- textstat library integrated and operational

**Test Steps**:
1. Create sample chunk with standard complexity text (8th-10th grade reading level)
2. Use MetadataEnricher to calculate readability scores
3. Extract Flesch-Kincaid Grade Level from QualityScore
4. Extract Gunning Fog Index from QualityScore
5. Compare calculated scores against expected ranges

**Expected Results**:
- Flesch-Kincaid Grade Level: 8.0-10.0 (moderate difficulty)
- Gunning Fog Index: 10.0-12.0 (high school level)
- Both scores calculated without errors
- Scores stored in QualityScore dataclass

**Test Data**:
- **Input Text**: "The risk management framework establishes clear guidelines for identifying and mitigating potential threats. Organizations must conduct regular assessments to ensure compliance with regulatory requirements. This systematic approach helps maintain operational resilience and protect stakeholder interests."
- **Expected FK Range**: 8.0-10.0
- **Expected Gunning Fog Range**: 10.0-12.0

**Dependencies**:
- textstat library (0.7.x)
- MetadataEnricher component
- QualityScore dataclass

---

#### Test Case UAT-3.3-4-2

**Acceptance Criterion**: AC-3.3-4
**Test Type**: Manual
**Scenario**: Edge Case - Simple Text

**Objective**: Validate readability scores for simple text (children's book level) are correctly calculated

**Preconditions**:
- MetadataEnricher operational
- Simple text sample prepared

**Test Steps**:
1. Create chunk with simple, easy-to-read text (3rd-5th grade level)
2. Calculate readability scores using MetadataEnricher
3. Verify FK score is low (3.0-5.0)
4. Verify Gunning Fog score is low (5.0-8.0)
5. Confirm scores align with manual assessment of simplicity

**Expected Results**:
- Flesch-Kincaid Grade Level: 3.0-5.0 (elementary school)
- Gunning Fog Index: 5.0-8.0 (easy reading)
- Scores accurately reflect simple language structure
- No errors during calculation

**Test Data**:
- **Input Text**: "The cat sat on the mat. The dog ran in the yard. They played together all day. The sun was bright and warm."
- **Expected FK Range**: 3.0-5.0
- **Expected Gunning Fog Range**: 5.0-8.0

**Dependencies**:
- textstat library
- MetadataEnricher component

---

#### Test Case UAT-3.3-4-3

**Acceptance Criterion**: AC-3.3-4
**Test Type**: Manual
**Scenario**: Edge Case - Complex Technical Text

**Objective**: Validate readability scores for highly complex technical text (PhD thesis level) are correctly calculated

**Preconditions**:
- MetadataEnricher operational
- Complex technical text sample prepared

**Test Steps**:
1. Create chunk with complex technical/academic text (post-graduate level)
2. Calculate readability scores using MetadataEnricher
3. Verify FK score is high (>=12.0)
4. Verify Gunning Fog score is high (>=15.0)
5. Confirm scores align with manual assessment of complexity

**Expected Results**:
- Flesch-Kincaid Grade Level: >=12.0 (college/post-graduate)
- Gunning Fog Index: >=15.0 (very difficult)
- Scores accurately reflect complex language structure
- high_complexity flag potentially triggered (FK >15.0)

**Test Data**:
- **Input Text**: "The implementation of a comprehensive risk mitigation methodology necessitates the establishment of multifaceted governance frameworks that systematically integrate probabilistic assessment techniques with deterministic control mechanisms, thereby facilitating the optimization of organizational resilience through iterative refinement of procedural architectures and continuous evaluation of emerging threat vectors across heterogeneous operational environments."
- **Expected FK Range**: >=12.0
- **Expected Gunning Fog Range**: >=15.0

**Dependencies**:
- textstat library
- MetadataEnricher component
- QualityScore.flags validation

---

#### Test Case UAT-3.3-4-4

**Acceptance Criterion**: AC-3.3-4
**Test Type**: Manual
**Scenario**: Edge Case - Very Short Text

**Objective**: Validate readability calculation handles very short text (<3 sentences) gracefully

**Preconditions**:
- MetadataEnricher operational
- Very short text sample prepared

**Test Steps**:
1. Create chunk with 1-2 sentences only
2. Calculate readability scores using MetadataEnricher
3. Verify calculation completes without errors
4. Verify scores are reasonable (not NaN, not infinity)
5. Check edge case handling documented in code

**Expected Results**:
- Calculation completes successfully
- Scores are valid floats (not NaN, not infinity)
- Scores within reasonable range (0.0-30.0)
- No exceptions raised during calculation

**Test Data**:
- **Input Text**: "Risk assessment is critical."
- **Expected Result**: Valid scores calculated (specific values may vary)

**Dependencies**:
- textstat library edge case handling
- MetadataEnricher error handling

---

#### Test Case UAT-3.3-4-5

**Acceptance Criterion**: AC-3.3-4
**Test Type**: Manual
**Scenario**: Error Case - Empty Text

**Objective**: Validate readability calculation handles empty text gracefully without crashing

**Preconditions**:
- MetadataEnricher operational
- Empty text test case prepared

**Test Steps**:
1. Create chunk with empty string text
2. Attempt to calculate readability scores using MetadataEnricher
3. Verify calculation completes without raising exceptions
4. Verify scores default to 0.0 or appropriate sentinel value
5. Check error handling logs/warnings

**Expected Results**:
- No exceptions raised
- Readability scores set to 0.0 (or documented default)
- Graceful degradation behavior
- System logs warning about empty text (if applicable)

**Test Data**:
- **Input Text**: ""
- **Expected FK**: 0.0
- **Expected Gunning Fog**: 0.0

**Dependencies**:
- MetadataEnricher error handling
- textstat library behavior with empty input

---

### AC-3.3-5: Composite Quality Score (Weighted Average)

#### Test Case UAT-3.3-5-1

**Acceptance Criterion**: AC-3.3-5
**Test Type**: Manual
**Scenario**: Happy Path - High Quality Chunk

**Objective**: Validate overall quality score for high-quality chunk matches expected composite calculation

**Preconditions**:
- MetadataEnricher operational
- High-quality sample chunk prepared
- Weighted formula documented: OCR 40%, Completeness 30%, Coherence 20%, Readability 10%

**Test Steps**:
1. Create high-quality chunk with known quality metrics:
   - OCR confidence: 0.99
   - Completeness: 0.98
   - Coherence: 0.85 (high lexical overlap)
   - Readability: Normalize FK score (e.g., FK 8.0 → readability component ~0.90)
2. Calculate overall quality score using MetadataEnricher
3. Manually verify weighted calculation: (0.4 * 0.99) + (0.3 * 0.98) + (0.2 * 0.85) + (0.1 * readability_norm)
4. Compare calculated overall score to expected value
5. Verify overall score >= 0.90

**Expected Results**:
- OCR confidence propagated correctly: 0.99
- Completeness calculated correctly: 0.98
- Coherence calculated correctly: ~0.85
- Overall score calculated as weighted average
- Overall score >= 0.90 (high quality threshold)
- No quality flags triggered

**Test Data**:
- **Input Chunk**: Well-formed text from high-quality OCR scan
- **Expected OCR**: 0.99
- **Expected Completeness**: 0.98
- **Expected Coherence**: ~0.85
- **Expected Overall**: ~0.90-0.95

**Dependencies**:
- MetadataEnricher weighted calculation logic
- QualityScore dataclass
- ProcessingResult with OCR metadata

---

#### Test Case UAT-3.3-5-2

**Acceptance Criterion**: AC-3.3-5
**Test Type**: Manual
**Scenario**: Happy Path - Medium Quality Chunk

**Objective**: Validate overall quality score for medium-quality chunk reflects intermediate values

**Preconditions**:
- MetadataEnricher operational
- Medium-quality sample chunk prepared

**Test Steps**:
1. Create medium-quality chunk with moderate quality metrics:
   - OCR confidence: 0.90
   - Completeness: 0.85
   - Coherence: 0.70
   - Readability: Moderate complexity (FK ~10.0)
2. Calculate overall quality score using MetadataEnricher
3. Manually verify weighted calculation
4. Verify overall score in range 0.80-0.85
5. Verify score accurately reflects medium quality

**Expected Results**:
- Overall score: 0.80-0.85 (medium quality)
- Weighted calculation accurate
- Score differentiates from high-quality chunks
- Possible quality flags depending on thresholds

**Test Data**:
- **Input Chunk**: Moderate quality text with minor issues
- **Expected OCR**: 0.90
- **Expected Completeness**: 0.85
- **Expected Coherence**: ~0.70
- **Expected Overall**: ~0.80-0.85

**Dependencies**:
- MetadataEnricher component
- QualityScore dataclass

---

#### Test Case UAT-3.3-5-3

**Acceptance Criterion**: AC-3.3-5
**Test Type**: Manual
**Scenario**: Edge Case - Perfect Quality

**Objective**: Validate perfect quality scores (all 1.0) produce overall score of 1.0

**Preconditions**:
- MetadataEnricher operational
- Perfect quality scenario prepared

**Test Steps**:
1. Create theoretical perfect-quality chunk:
   - OCR confidence: 1.0
   - Completeness: 1.0
   - Coherence: 1.0
   - Readability: Normalized to 1.0
2. Calculate overall quality score
3. Verify overall score = 1.0
4. Verify weighted calculation: (0.4 * 1.0) + (0.3 * 1.0) + (0.2 * 1.0) + (0.1 * 1.0) = 1.0

**Expected Results**:
- Overall score: 1.0 (perfect quality)
- All component scores: 1.0
- No quality flags
- Calculation mathematically correct

**Test Data**:
- **All Metrics**: 1.0
- **Expected Overall**: 1.0

**Dependencies**:
- MetadataEnricher component
- QualityScore validation (0.0-1.0 range)

---

#### Test Case UAT-3.3-5-4

**Acceptance Criterion**: AC-3.3-5
**Test Type**: Manual
**Scenario**: Edge Case - Low Quality Chunk

**Objective**: Validate overall quality score for low-quality chunk accurately reflects poor quality

**Preconditions**:
- MetadataEnricher operational
- Low-quality sample chunk prepared

**Test Steps**:
1. Create low-quality chunk with poor quality metrics:
   - OCR confidence: 0.70
   - Completeness: 0.60
   - Coherence: 0.50
   - Readability: Poor (very high FK or very low)
2. Calculate overall quality score
3. Manually verify weighted calculation
4. Verify overall score <= 0.65
5. Verify multiple quality flags triggered

**Expected Results**:
- Overall score: ~0.60-0.65 (low quality)
- Multiple quality flags present (low_ocr, incomplete_extraction, possibly gibberish)
- Weighted calculation accurate
- Score triggers quality warning threshold

**Test Data**:
- **Input Chunk**: Low-quality text with OCR errors, missing content
- **Expected OCR**: 0.70
- **Expected Completeness**: 0.60
- **Expected Coherence**: ~0.50
- **Expected Overall**: ~0.60-0.65

**Dependencies**:
- MetadataEnricher component
- QualityScore.flags detection logic

---

#### Test Case UAT-3.3-5-5

**Acceptance Criterion**: AC-3.3-5
**Test Type**: Integration
**Scenario**: Integration - Weighted Formula Validation

**Objective**: Validate weighted average formula is correctly implemented (40% OCR, 30% completeness, 20% coherence, 10% readability)

**Preconditions**:
- MetadataEnricher source code accessible
- Multiple test samples with known component scores

**Test Steps**:
1. Review MetadataEnricher._calculate_overall_score() implementation
2. Verify formula: overall = (0.4 * ocr) + (0.3 * completeness) + (0.2 * coherence) + (0.1 * readability_norm)
3. Test with multiple known value combinations
4. Validate results match manual calculation (within floating-point tolerance)
5. Verify weights sum to 1.0 (100%)

**Expected Results**:
- Formula implementation matches specification
- Weights: OCR 40%, Completeness 30%, Coherence 20%, Readability 10%
- Calculation accurate to within 0.01 tolerance
- All test cases produce correct weighted average

**Test Data**:
- **Test Case 1**: OCR 0.95, Comp 0.90, Coh 0.80, Read 0.85 → Overall ≈ 0.89
- **Test Case 2**: OCR 0.80, Comp 0.75, Coh 0.70, Read 0.65 → Overall ≈ 0.755
- **Test Case 3**: OCR 1.0, Comp 1.0, Coh 1.0, Read 1.0 → Overall = 1.0

**Dependencies**:
- MetadataEnricher source code
- Python floating-point arithmetic (pytest.approx for tolerance)

---

### AC-3.3-8: Quality Flags Detection

#### Test Case UAT-3.3-8-1

**Acceptance Criterion**: AC-3.3-8
**Test Type**: Manual
**Scenario**: Happy Path - No Quality Issues

**Objective**: Validate high-quality chunk produces empty flags list (no issues detected)

**Preconditions**:
- MetadataEnricher operational
- High-quality chunk sample prepared

**Test Steps**:
1. Create high-quality chunk:
   - OCR confidence >= 0.95
   - Completeness >= 0.90
   - FK grade level <= 15.0
   - Non-alphabetic character ratio <= 30%
2. Calculate quality scores and flags
3. Verify QualityScore.flags is empty list []
4. Verify no false-positive flag detection

**Expected Results**:
- QualityScore.flags: [] (empty list)
- No flags triggered for high-quality content
- All quality thresholds satisfied

**Test Data**:
- **Input Chunk**: Clean, well-formed text from quality source
- **Expected Flags**: [] (empty)

**Dependencies**:
- MetadataEnricher flag detection logic
- QualityScore.flags field

---

#### Test Case UAT-3.3-8-2

**Acceptance Criterion**: AC-3.3-8
**Test Type**: Manual
**Scenario**: Edge Case - Low OCR Flag

**Objective**: Validate low_ocr flag is correctly triggered when OCR confidence < 0.95

**Preconditions**:
- MetadataEnricher operational
- Sample chunk with low OCR confidence prepared

**Test Steps**:
1. Create chunk with OCR confidence = 0.90 (below 0.95 threshold)
2. Other quality metrics acceptable (completeness >= 0.90, FK <= 15, gibberish check pass)
3. Calculate quality scores and flags
4. Verify "low_ocr" flag present in QualityScore.flags
5. Verify only low_ocr flag present (no false positives)

**Expected Results**:
- QualityScore.flags: ["low_ocr"]
- OCR confidence: 0.90
- Flag triggered at correct threshold (< 0.95)
- Other flags not triggered

**Test Data**:
- **OCR Confidence**: 0.90
- **Expected Flags**: ["low_ocr"]

**Dependencies**:
- MetadataEnricher flag detection logic
- ProcessingResult OCR metadata

---

#### Test Case UAT-3.3-8-3

**Acceptance Criterion**: AC-3.3-8
**Test Type**: Manual
**Scenario**: Edge Case - Incomplete Extraction Flag

**Objective**: Validate incomplete_extraction flag is correctly triggered when completeness < 0.90

**Preconditions**:
- MetadataEnricher operational
- Sample chunk with low completeness prepared

**Test Steps**:
1. Create chunk with completeness = 0.85 (below 0.90 threshold)
2. Other quality metrics acceptable (OCR >= 0.95, FK <= 15, gibberish check pass)
3. Calculate quality scores and flags
4. Verify "incomplete_extraction" flag present in QualityScore.flags
5. Verify only incomplete_extraction flag present

**Expected Results**:
- QualityScore.flags: ["incomplete_extraction"]
- Completeness: 0.85
- Flag triggered at correct threshold (< 0.90)
- Other flags not triggered

**Test Data**:
- **Completeness**: 0.85
- **Expected Flags**: ["incomplete_extraction"]

**Dependencies**:
- MetadataEnricher flag detection logic
- Completeness calculation from entity preservation

---

#### Test Case UAT-3.3-8-4

**Acceptance Criterion**: AC-3.3-8
**Test Type**: Manual
**Scenario**: Edge Case - High Complexity Flag

**Objective**: Validate high_complexity flag is correctly triggered when Flesch-Kincaid grade level > 15.0

**Preconditions**:
- MetadataEnricher operational
- Complex technical text sample prepared

**Test Steps**:
1. Create chunk with very complex text (FK grade level 18.0)
2. Other quality metrics acceptable (OCR >= 0.95, completeness >= 0.90, gibberish check pass)
3. Calculate readability scores and flags
4. Verify "high_complexity" flag present in QualityScore.flags
5. Verify FK score > 15.0

**Expected Results**:
- QualityScore.flags: ["high_complexity"]
- Flesch-Kincaid: 18.0 (or similar high value)
- Flag triggered at correct threshold (> 15.0)
- Other flags not triggered

**Test Data**:
- **Input Text**: Highly complex academic/technical text (see UAT-3.3-4-3 for sample)
- **Expected FK**: > 15.0
- **Expected Flags**: ["high_complexity"]

**Dependencies**:
- MetadataEnricher flag detection logic
- textstat readability calculation

---

#### Test Case UAT-3.3-8-5

**Acceptance Criterion**: AC-3.3-8
**Test Type**: Manual
**Scenario**: Edge Case - Gibberish Flag

**Objective**: Validate gibberish flag is correctly triggered when non-alphabetic character ratio > 30%

**Preconditions**:
- MetadataEnricher operational
- Gibberish text sample prepared

**Test Steps**:
1. Create chunk with excessive non-alphabetic characters (symbols, numbers, special chars)
2. Calculate character statistics
3. Verify non-alphabetic ratio > 30%
4. Calculate quality scores and flags
5. Verify "gibberish" flag present in QualityScore.flags

**Expected Results**:
- QualityScore.flags: ["gibberish"]
- Non-alphabetic character ratio > 30%
- Flag triggered at correct threshold
- Text identified as low-quality gibberish

**Test Data**:
- **Input Text**: "R!$k-2024-001: ###CRITICAL### @@@ATTENTION@@@ 99% c0mpl!@nc3 r3qu!r3d $$$ 123-456-7890 %%% *** ~~~"
- **Non-Alpha Ratio**: > 30%
- **Expected Flags**: ["gibberish"]

**Dependencies**:
- MetadataEnricher gibberish detection logic
- Character classification algorithm

---

#### Test Case UAT-3.3-8-6

**Acceptance Criterion**: AC-3.3-8
**Test Type**: Manual
**Scenario**: Edge Case - Multiple Flags

**Objective**: Validate multiple quality flags can be detected simultaneously for a single chunk

**Preconditions**:
- MetadataEnricher operational
- Low-quality chunk with multiple issues prepared

**Test Steps**:
1. Create chunk with multiple quality issues:
   - OCR confidence: 0.85 (triggers low_ocr)
   - Completeness: 0.80 (triggers incomplete_extraction)
   - Contains gibberish characters (triggers gibberish)
2. Calculate quality scores and flags
3. Verify QualityScore.flags contains all applicable flags
4. Verify flags list includes: ["low_ocr", "incomplete_extraction", "gibberish"]
5. Verify overall quality score is low

**Expected Results**:
- QualityScore.flags: ["low_ocr", "incomplete_extraction", "gibberish"] (order may vary)
- Multiple flags correctly detected
- Overall score reflects cumulative quality issues (<0.70)
- Each flag corresponds to specific threshold violation

**Test Data**:
- **OCR Confidence**: 0.85
- **Completeness**: 0.80
- **Text Contains**: Gibberish characters
- **Expected Flags**: ["low_ocr", "incomplete_extraction", "gibberish"]

**Dependencies**:
- MetadataEnricher flag detection logic
- QualityScore.flags list handling

---

#### Test Case UAT-3.3-8-7

**Acceptance Criterion**: AC-3.3-8
**Test Type**: Integration
**Scenario**: Integration - End-to-End Flag Detection

**Objective**: Validate flag detection works correctly in full pipeline from ProcessingResult to enriched Chunk

**Preconditions**:
- Complete pipeline operational (ProcessingResult → ChunkingEngine → MetadataEnricher)
- Varied quality test documents prepared

**Test Steps**:
1. Create ProcessingResult with varied quality chunks
2. Run complete chunking pipeline with quality enrichment
3. Verify flags correctly propagated to each chunk's metadata
4. Verify flag accuracy across all chunks
5. Verify filtering chunks by flags works correctly

**Expected Results**:
- All chunks have QualityScore.flags populated
- High-quality chunks: empty flags list
- Low-quality chunks: appropriate flags present
- Flag distribution matches manual quality assessment
- Filtering by flags isolates problematic chunks

**Test Data**:
- **Test Corpus**: Mix of high, medium, and low-quality documents
- **Expected Behavior**: Flags accurately identify quality issues

**Dependencies**:
- Complete Epic 3 pipeline
- ProcessingResult fixtures
- MetadataEnricher integration with ChunkingEngine

---

## Testing Notes

### Execution Recommendations

**Suggested Test Execution Order**:
1. **Phase 1: Readability Tests** (UAT-3.3-4-1 through UAT-3.3-4-5)
   - Start with manual validation of textstat integration
   - Verify edge cases (simple, complex, short, empty text)
   - Estimated time: 30-45 minutes

2. **Phase 2: Quality Score Tests** (UAT-3.3-5-1 through UAT-3.3-5-5)
   - Validate weighted average calculation
   - Test quality spectrum (high, medium, low, perfect)
   - Verify formula implementation (integration test)
   - Estimated time: 45-60 minutes

3. **Phase 3: Flag Detection Tests** (UAT-3.3-8-1 through UAT-3.3-8-7)
   - Validate individual flag triggers
   - Test multiple flags scenario
   - End-to-end integration validation
   - Estimated time: 60-75 minutes

**Critical Tests (Must Pass)**:
- UAT-3.3-4-1: Standard readability calculation (baseline validation)
- UAT-3.3-5-1: High-quality composite score (happy path)
- UAT-3.3-5-5: Weighted formula validation (correctness proof)
- UAT-3.3-8-1: No false-positive flags (accuracy baseline)
- UAT-3.3-8-6: Multiple flags detection (comprehensive validation)
- UAT-3.3-8-7: End-to-end integration (system validation)

**Parallel Execution Opportunities**:
- All AC-3.3-4 tests can run in parallel (independent readability validation)
- AC-3.3-5 edge cases (UAT-3.3-5-3, UAT-3.3-5-4) can run in parallel
- Individual flag tests (UAT-3.3-8-2 through UAT-3.3-8-5) can run in parallel

**Estimated Total Execution Time**: 2.5-3.0 hours

---

### Implementation Notes

**pytest Markers to Use**:
- `@pytest.mark.uat` - All UAT test cases
- `@pytest.mark.quality` - Quality scoring specific tests
- `@pytest.mark.chunking` - Chunking-related tests
- `@pytest.mark.integration` - End-to-end integration tests

**Fixture Requirements**:
- `quality_test_documents/` - Varied quality sample texts
  - `simple_text.txt` - Elementary school level
  - `standard_text.txt` - 8th-10th grade level
  - `complex_text.txt` - Post-graduate level
  - `gibberish_text.txt` - High non-alpha ratio
  - `short_text.txt` - 1-2 sentences
  - `empty_text.txt` - Empty file
- `processing_result_fixtures/` - ProcessingResult objects with varied OCR/completeness
  - `high_quality_result.json` - OCR 0.99, completeness 0.98
  - `medium_quality_result.json` - OCR 0.90, completeness 0.85
  - `low_quality_result.json` - OCR 0.70, completeness 0.60

**Helper Functions Needed**:
- `create_test_chunk(text, ocr_conf, completeness)` - Generate test Chunk with specified quality
- `verify_readability_range(score, min_val, max_val)` - Validate score within range
- `calculate_expected_overall(ocr, comp, coh, read)` - Manual weighted calculation
- `count_non_alpha_chars(text)` - Calculate gibberish ratio

**Integration Points to Mock/Stub**:
- None required - tests use actual MetadataEnricher and textstat library
- ProcessingResult can be mocked for controlled OCR/completeness values

---

### Risk Areas

**Complex Scenarios Requiring Extra Attention**:
1. **Readability Edge Cases** (UAT-3.3-4-4, UAT-3.3-4-5)
   - textstat library behavior with very short or empty text
   - Potential for NaN or infinity values
   - Error handling validation critical

2. **Weighted Calculation Precision** (UAT-3.3-5-5)
   - Floating-point arithmetic may introduce rounding errors
   - Use pytest.approx with tolerance (0.01) for comparisons
   - Verify weights sum exactly to 1.0

3. **Multiple Flags Detection** (UAT-3.3-8-6)
   - Ensure flags list can contain multiple values
   - Verify no duplicate flags in list
   - Test flag ordering (if deterministic)

**Potential Failure Points**:
- **textstat library integration**: Verify version compatibility (0.7.x)
- **OCR confidence extraction**: Ensure per-page dict correctly averaged
- **Coherence calculation**: Lexical overlap heuristic may produce unexpected values
- **Gibberish detection**: 30% threshold may need tuning based on real-world data

**Performance Bottlenecks**:
- textstat calculations may be slow for very long chunks (>2000 words)
- Coherence calculation is O(n*m) for sentence pairs
- Overall enrichment overhead should be <0.1s per 1,000 words

**Security Considerations**:
- None identified for quality scoring (no external API calls, no file system writes)
- Input text sanitization already handled in Epic 2

---

## Acceptance Criteria Coverage

### AC-3.3-4: Readability Scores Calculated ✓
- [x] Happy path validation (UAT-3.3-4-1)
- [x] Simple text edge case (UAT-3.3-4-2)
- [x] Complex text edge case (UAT-3.3-4-3)
- [x] Short text edge case (UAT-3.3-4-4)
- [x] Empty text error case (UAT-3.3-4-5)
- **Coverage**: 5/5 test cases (100%)

### AC-3.3-5: Composite Quality Score ✓
- [x] High-quality happy path (UAT-3.3-5-1)
- [x] Medium-quality happy path (UAT-3.3-5-2)
- [x] Perfect quality edge case (UAT-3.3-5-3)
- [x] Low-quality edge case (UAT-3.3-5-4)
- [x] Weighted formula validation (UAT-3.3-5-5)
- **Coverage**: 5/5 test cases (100%)

### AC-3.3-8: Quality Flags Detection ✓
- [x] No flags happy path (UAT-3.3-8-1)
- [x] Low OCR flag (UAT-3.3-8-2)
- [x] Incomplete extraction flag (UAT-3.3-8-3)
- [x] High complexity flag (UAT-3.3-8-4)
- [x] Gibberish flag (UAT-3.3-8-5)
- [x] Multiple flags (UAT-3.3-8-6)
- [x] End-to-end integration (UAT-3.3-8-7)
- **Coverage**: 7/7 test cases (100%)

**Overall UAT Coverage**: 16/16 test cases (100% of UAT-required ACs)

---

## Next Steps

1. **Review test cases**: Validate coverage and scenarios with stakeholders
2. **Run build-test-context**: Gather fixtures and helpers → `/bmad:bmm:workflows:build-test-context`
3. **Implement missing fixtures**: Create quality_test_documents/ directory with sample texts
4. **Execute tests**: Run via execute-tests workflow → `/bmad:bmm:workflows:execute-tests`
5. **Review results**: QA review via review-uat-results workflow → `/bmad:bmm:workflows:review-uat-results`

---

**Document Status**: Ready for Context Building
**Generated by**: andrew using BMAD UAT Framework
