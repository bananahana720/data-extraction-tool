# Execute Tests Workflow

**Purpose**: AI-driven test execution with pass/fail/blocked status, tmux-cli integration for CLI tests, and structured result capture.

**Type**: Document workflow (generates test-results.md)

**Part of**: BMad UAT Workflow Framework (Story 2.5.3.1)

---

## Overview

The `execute-tests` workflow executes test cases generated by create-test-cases, using test infrastructure discovered by build-test-context. It supports automated pytest execution, CLI testing via tmux-cli, and guided manual testing with comprehensive result capture and evidence collection.

## When to Use

- **After test context building**: When test cases and context are ready
- **For UAT validation**: To systematically validate acceptance criteria
- **During development**: To verify implementation against test cases
- **Before QA review**: To gather test results for review-uat-results workflow

## Input Requirements

**Required**:
- Test cases file (from create-test-cases workflow)
- Test context XML (from build-test-context workflow)

**Optional**:
- Story markdown file (derived from test cases if not provided)
- Execution mode preference (automated/manual/hybrid)
- Screenshot capture setting for CLI tests

## Output

**File**: `docs/uat/test-results/{story_key}-test-results.md`

**Contents**:
- Execution summary with pass/fail/blocked counts
- Acceptance criteria validation status
- Detailed results for all test types (pytest, CLI, manual)
- Evidence for failed and blocked tests (logs, output, screenshots)
- Performance observations
- Recommendations for next steps

## Usage

### Basic Usage (Hybrid Mode)

```bash
workflow execute-tests
```

The workflow will:
1. Find latest test cases and test context
2. Run automated tests (pytest)
3. Run CLI tests (tmux-cli)
4. Guide through manual tests
5. Generate comprehensive results report

### Automated Tests Only

```bash
workflow execute-tests test_execution_mode=automated
```

Runs only pytest tests (unit, integration, performance).

### Manual Tests Only

```bash
workflow execute-tests test_execution_mode=manual
```

Guides through manual test execution with user input.

### With Screenshot Capture

```bash
workflow execute-tests capture_screenshots=true
```

Captures screenshots during CLI test execution (requires screenshot capability).

### Stop on First Failure

```bash
workflow execute-tests continue_on_failure=false
```

Stops execution immediately when a test fails (default: continue).

## Workflow Pipeline Position

This is **Step 3** in the UAT workflow pipeline:

```
┌─────────────────────┐
│ create-test-cases   │
│ (Story → Test Cases)│
└──────────┬──────────┘
           ↓
┌─────────────────────┐
│ build-test-context  │
│ (Gather fixtures)   │
└──────────┬──────────┘
           ↓
┌─────────────────────┐
│ execute-tests       │ ← YOU ARE HERE
│ (Run tests)         │
└──────────┬──────────┘
           ↓
┌─────────────────────┐
│ review-uat-results  │
│ (QA approval)       │
└─────────────────────┘
```

## Test Execution Types

### 1. Automated Tests (pytest)

**Test Types**: Unit, Integration, Performance

**Process**:
1. Build pytest command with markers and timeout
2. Execute via pytest
3. Capture output and parse results
4. Map pytest tests to test case IDs
5. Record PASS/FAIL/ERROR with evidence

**Example**:
```bash
pytest -v --tb=short -m "unit or integration" --timeout=300
```

### 2. CLI Tests (tmux-cli)

**Test Types**: CLI command tests, interactive application tests

**Process**:
1. Launch shell in tmux via `tmux-cli launch "zsh"`
2. For each CLI test:
   - Send command via `tmux-cli send`
   - Wait for completion via `tmux-cli wait_idle`
   - Capture output via `tmux-cli capture`
   - Verify expected results
   - Record PASS/FAIL with output evidence
3. Clean up tmux session

**Example**:
```bash
# Launch shell
tmux-cli launch "zsh"  # Returns pane ID

# Run test command
tmux-cli send "data-extract process test.pdf" --pane=2
tmux-cli wait_idle --pane=2 --idle-time=2.0 --timeout=60

# Capture and verify
tmux-cli capture --pane=2
```

**tmux-cli Integration Pattern**:
- Always launch shell first (prevents losing output)
- Use wait_idle to ensure command completion
- Capture output for verification
- Handle interactive prompts with send
- Clean up pane after tests

**⚠️ Windows Users**: tmux-cli requires tmux, which is Unix/Linux only. On Windows:
- **Option 1 (Recommended)**: Run workflow from WSL
  ```bash
  wsl
  cd /mnt/c/Users/{username}/projects/{project-name}
  workflow execute-tests test_execution_mode=hybrid
  ```
- **Option 2**: Run with test_execution_mode=manual (skip CLI tests)
- **Option 3**: CLI tests will be marked as BLOCKED with Windows limitation noted

See `docs/tmux-cli-instructions.md` for full tmux-cli reference and `docs/uat/tmux-cli-windows-setup.md` for detailed Windows setup.

### 3. Manual Tests

**Test Types**: Tests requiring human verification

**Process**:
1. Display test case with instructions to user
2. User executes test manually
3. Prompt for result (pass/fail/blocked)
4. Capture failure description or notes
5. Record status with user-provided evidence

## Execution Modes

### Automated Mode
- Runs only pytest tests
- No user interaction required
- Fast execution
- Good for CI/CD pipelines

### Manual Mode
- Guides through manual tests only
- Requires user interaction
- Good for exploratory testing
- User provides pass/fail status

### Hybrid Mode (Default)
- Runs automated tests first
- Then guides through manual tests
- Comprehensive coverage
- Recommended for UAT

## Test Result Status Values

- **PASS**: Test executed successfully, met expected results
- **FAIL**: Test executed but did not meet expected results
- **BLOCKED**: Test could not execute due to missing prerequisite
- **SKIPPED**: Test was intentionally skipped (pytest only)
- **ERROR**: Test execution encountered an error (pytest only)

## Evidence Capture

### Automated Tests
- Failure messages and tracebacks
- pytest output snippets
- Execution time
- Coverage data (if available)

### CLI Tests
- Command output (stdout/stderr)
- Screenshots (if capture_screenshots enabled)
- Execution time
- Interactive session transcript

### Manual Tests
- User-provided failure descriptions
- User observations and notes
- Pass/fail rationale

## Environment Verification

Before execution, the workflow verifies:
- Required fixtures exist
- Helper functions available
- pytest configuration valid
- Environment variables set
- External dependencies accessible

Tests with missing prerequisites are marked as **BLOCKED** with setup guidance.

## Configuration

The workflow uses BMM config values from `bmad/bmm/config.yaml`:

- `dev_story_location`: Where to find story files
- `output_folder`: Where to save test results (docs/)
- `user_name`: Test executor name
- `communication_language`: Workflow interaction language

Execution settings from workflow.yaml:
- `pytest_args`: Default pytest arguments (-v --tb=short)
- `pytest_timeout`: Test timeout in seconds (300)
- `tmux_idle_time`: Seconds to wait for CLI idle (2.0)
- `tmux_timeout`: Timeout for tmux operations (60)

## Examples

### Example 1: Standard Hybrid Execution

```bash
workflow execute-tests
```

Output: `docs/uat/test-results/2.5-3.1-test-results.md`
- Runs all pytest tests
- Runs all CLI tests via tmux-cli
- Guides through manual tests
- Comprehensive results report

### Example 2: Automated Tests Only (CI/CD)

```bash
workflow execute-tests test_execution_mode=automated
```

Output: Test results with only pytest tests executed
- No user interaction required
- Good for continuous integration

### Example 3: Specific Test Files with Screenshots

```bash
workflow execute-tests \
  test_cases_file=docs/uat/test-cases/2.5-2-test-cases.md \
  test_context_file=docs/uat/test-context/2.5-2-test-context.xml \
  capture_screenshots=true
```

Output: Test results with CLI screenshots captured

### Example 4: Stop on First Failure

```bash
workflow execute-tests continue_on_failure=false
```

Output: Execution stops at first failing test, results saved

## Troubleshooting

**Issue**: "Test cases file not found"
**Solution**: Run create-test-cases workflow first or provide explicit path

**Issue**: "Test context file not found"
**Solution**: Run build-test-context workflow first

**Issue**: "Fixtures missing - tests blocked"
**Solution**: Review test context, generate missing fixtures, re-run

**Issue**: "tmux-cli command not found"
**Solution**: Install tmux-cli via `uv tool install tmux-cli`

**Issue**: "pytest tests failing"
**Solution**: Review failure messages, fix code, re-run execute-tests

**Issue**: "CLI tests timing out"
**Solution**: Increase tmux_timeout setting or investigate command hang

## Next Steps After Execution

### All Tests Passed
1. **Review results**: Verify test evidence and coverage
2. **Run UAT review**: Execute review-uat-results workflow
   ```bash
   workflow review-uat-results
   ```
3. **Story completion**: Mark story as done if UAT approved

### Tests Failed
1. **Review failures**: Analyze failure messages and evidence
2. **Fix issues**: Address root causes in code
3. **Re-run tests**: Execute workflow again
4. **Iterate**: Repeat until all tests pass

### Tests Blocked
1. **Review blockers**: Check test context for missing prerequisites
2. **Resolve blockers**: Generate fixtures, fix setup issues
3. **Re-run tests**: Execute workflow after resolution

## Acceptance Criteria Mapping

Results are grouped by acceptance criterion:

```markdown
## AC-2.5.3.1-1: UAT workflow create-test-cases designed

**Status**: PASS

**Test Results**:
- TC-2.5-3.1-1-1 (Happy Path): PASS ✓
- TC-2.5-3.1-1-2 (Edge Case): PASS ✓
- TC-2.5-3.1-1-3 (Error Case): PASS ✓

**Evidence**: All tests passed with expected output
```

AC is marked FAIL if any test fails, BLOCKED if any test blocked.

## Related Workflows

- **create-test-cases**: Generates test cases that this workflow executes
- **build-test-context**: Prepares test infrastructure for execution
- **review-uat-results**: Reviews execution results for QA approval
- **story-context**: Provides development context (complementary)

## Integration with Development Workflow

```
Development (dev-story) ← Code implementation
    ↓
Test Creation (create-test-cases) ← Define validation
    ↓
Test Context (build-test-context) ← Prepare infrastructure
    ↓
Test Execution (execute-tests) ← Validate implementation ← YOU ARE HERE
    ↓
Code Review (code-review) + UAT Review (review-uat-results)
    ↓
Story Done (story-done)
```

## Author

Created as part of Epic 2.5 - Testing Infrastructure (Story 2.5.3.1)

## Version

1.0.0 - Initial release
