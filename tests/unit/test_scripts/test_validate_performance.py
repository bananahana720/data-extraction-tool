"""
Unit tests for validate_performance.py script.

Tests performance baseline parsing, test execution, regression detection,
NFR violation checking, and report generation.
"""

import sys
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

# Add scripts directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "scripts"))

from validate_performance import PerformanceValidator


@pytest.fixture
def mock_project_root(tmp_path):
    """Create a mock project structure with performance baselines."""
    project = tmp_path / "test_project"
    project.mkdir()

    # Create test directories
    (project / "tests" / "performance").mkdir(parents=True)
    (project / "docs").mkdir()
    (project / "scripts").mkdir()

    # Create mock baseline document
    baseline_file = project / "docs" / "performance-baselines-epic-3.md"
    baseline_file.write_text(
        """
# Performance Baselines - Epic 3

## Baseline Measurements

- **Chunking Time:** 3.0 seconds (10k words)
- **Memory Delta:** 255.5 MB
- **Throughput:** 1200 words per second

| Metric | Baseline | Actual | Status |
|--------|----------|--------|--------|
| chunk_time | 3.0s | 3.1s | ✅ |
| memory_peak | 255 MB | 260 MB | ✅ |

NFR-P1: Throughput must be >= 1000 words/sec
NFR-P2: Memory must be <= 500 MB
NFR-P3: Latency must be <= 5s for 10k words
"""
    )

    return project


@pytest.fixture
def mock_test_results():
    """Mock test results for comparison."""
    return {
        "test_chunking_time": {"value": 3.2, "unit": "seconds", "test": "test_chunking"},
        "test_memory_peak": {"value": 260, "unit": "MB", "test": "test_memory"},
        "test_throughput": {"value": 1100, "unit": "words/second", "test": "test_throughput"},
    }


class TestPerformanceValidator:
    """Test cases for PerformanceValidator class."""

    @pytest.mark.unit
    def test_initialization(self):
        """Test PerformanceValidator initialization."""
        validator = PerformanceValidator(
            component="chunk", update_baseline=True, ci_mode=True, verbose=True
        )

        assert validator.component == "chunk"
        assert validator.update_baseline is True
        assert validator.ci_mode is True
        assert validator.verbose is True
        assert validator.test_results == {}
        assert validator.baseline_data == {}
        assert validator.regressions == []
        assert validator.nfr_violations == []

    @pytest.mark.unit
    @patch("subprocess.run")
    def test_detect_changed_files(self, mock_run):
        """Test changed file detection."""
        validator = PerformanceValidator()

        # Mock git diff output
        mock_run.return_value = MagicMock(
            returncode=0,
            stdout="src/data_extract/chunk/chunking_engine.py\ntests/test_chunk.py\n",
            stderr="",
        )

        test_modules = validator.detect_changed_files()

        assert "test_chunking_performance" in test_modules
        assert "test_entity_chunking_performance" in test_modules
        mock_run.assert_called()

    @pytest.mark.unit
    @patch("subprocess.run")
    def test_detect_changed_files_no_changes(self, mock_run):
        """Test when no files have changed."""
        validator = PerformanceValidator()

        # Mock empty git diff
        mock_run.return_value = MagicMock(returncode=0, stdout="", stderr="")

        test_modules = validator.detect_changed_files()

        # Should return all tests when no changes detected
        assert len(test_modules) > 0
        assert any("extract" in module for module in test_modules)
        assert any("chunk" in module for module in test_modules)

    @pytest.mark.unit
    def test_parse_baseline_documents(self, mock_project_root):
        """Test baseline document parsing."""
        with patch("validate_performance.PROJECT_ROOT", mock_project_root):
            with patch("validate_performance.DOCS_DIR", mock_project_root / "docs"):
                validator = PerformanceValidator()

                result = validator.parse_baseline_documents()

                assert result is True
                assert len(validator.baseline_data) > 0

                # Check parsed metrics
                assert any("chunk" in key or "time" in key for key in validator.baseline_data)
                assert any("memory" in key for key in validator.baseline_data)

    @pytest.mark.unit
    def test_extract_metric_from_line(self):
        """Test metric extraction from markdown line."""
        validator = PerformanceValidator()

        # Test seconds extraction
        validator._extract_metric_from_line("Chunking Time: 3.5 seconds", "test_source")
        assert "test_source_chunking_time" in validator.baseline_data
        assert validator.baseline_data["test_source_chunking_time"]["value"] == 3.5

        # Test ms extraction
        validator._extract_metric_from_line("Response Time: 150 ms", "test_source")
        assert "test_source_response_time" in validator.baseline_data
        assert validator.baseline_data["test_source_response_time"]["value"] == 0.15

    @pytest.mark.unit
    def test_extract_memory_metric(self):
        """Test memory metric extraction."""
        validator = PerformanceValidator()

        validator._extract_memory_metric("Peak memory: 255.5 MB")
        assert "memory_peak_memory" in validator.baseline_data
        assert validator.baseline_data["memory_peak_memory"]["value"] == 255.5

        validator._extract_memory_metric("Memory delta: 100 MB")
        assert "memory_memory_delta" in validator.baseline_data
        assert validator.baseline_data["memory_memory_delta"]["value"] == 100

    @pytest.mark.unit
    def test_extract_throughput_metric(self):
        """Test throughput metric extraction."""
        validator = PerformanceValidator()

        validator._extract_throughput_metric("Throughput: 1200 words per second")
        assert "throughput" in validator.baseline_data
        assert validator.baseline_data["throughput"]["value"] == 1200

    @pytest.mark.unit
    @patch("subprocess.run")
    def test_run_performance_tests(self, mock_run, mock_project_root):
        """Test running performance tests."""
        with patch("validate_performance.PROJECT_ROOT", mock_project_root):
            with patch(
                "validate_performance.PERFORMANCE_TESTS_DIR",
                mock_project_root / "tests" / "performance",
            ):
                validator = PerformanceValidator()

                # Create mock test file
                test_file = (
                    mock_project_root / "tests" / "performance" / "test_chunking_performance.py"
                )
                test_file.parent.mkdir(parents=True, exist_ok=True)
                test_file.touch()

                # Mock successful test run
                mock_run.return_value = MagicMock(
                    returncode=0, stdout="3.5s call\ntest_chunking_performance PASSED", stderr=""
                )

                result = validator.run_performance_tests(["test_chunking_performance"])

                assert result is True
                mock_run.assert_called()

    @pytest.mark.unit
    def test_compare_with_baselines(self, mock_test_results):
        """Test baseline comparison."""
        validator = PerformanceValidator()

        # Set up baseline and test data
        validator.baseline_data = {
            "chunking_time": {"value": 3.0, "unit": "seconds"},
            "memory_peak": {"value": 255, "unit": "MB"},
            "throughput": {"value": 1200, "unit": "words/second"},
        }

        validator.test_results = mock_test_results

        result = validator.compare_with_baselines()

        # Should detect small regression in chunking time (3.2 > 3.0 by 6.7%)
        # But within 10% threshold, so no regression logged
        assert len(validator.regressions) == 0
        assert result is True

    @pytest.mark.unit
    def test_compare_with_baselines_regression(self):
        """Test baseline comparison with regression."""
        validator = PerformanceValidator()

        # Set up baseline and test data with significant regression
        validator.baseline_data = {
            "chunking_time": {"value": 3.0, "unit": "seconds"},
        }

        validator.test_results = {
            "test_chunking_time": {"value": 4.0, "unit": "seconds", "test": "test"},
        }

        result = validator.compare_with_baselines()

        # Should detect regression (33% worse)
        assert len(validator.regressions) == 1
        assert validator.regressions[0]["delta_pct"] > 10
        assert result is False

    @pytest.mark.unit
    def test_check_nfr_violations_pass(self, mock_test_results):
        """Test NFR violation checking - all pass."""
        validator = PerformanceValidator()
        validator.test_results = mock_test_results

        result = validator.check_nfr_violations()

        assert result is True
        assert len(validator.nfr_violations) == 0

    @pytest.mark.unit
    def test_check_nfr_violations_fail(self):
        """Test NFR violation checking - violations detected."""
        validator = PerformanceValidator()

        # Set up test data that violates NFRs
        validator.test_results = {
            "test_throughput": {"value": 800, "unit": "words/second", "test": "test"},  # Below 1000
            "test_memory": {"value": 600, "unit": "MB", "test": "test"},  # Above 500
            "test_chunk_latency": {"value": 6.0, "unit": "seconds", "test": "test"},  # Above 5s
        }

        result = validator.check_nfr_violations()

        assert result is False
        assert len(validator.nfr_violations) == 3

        # Check specific violations
        nfr_types = [v["nfr"] for v in validator.nfr_violations]
        assert "NFR-P1" in nfr_types  # Throughput
        assert "NFR-P2" in nfr_types  # Memory
        assert "NFR-P3" in nfr_types  # Latency

    @pytest.mark.unit
    def test_generate_regression_report(self, tmp_path):
        """Test regression report generation."""
        with patch("validate_performance.PROJECT_ROOT", tmp_path):
            validator = PerformanceValidator()

            # Add test data
            validator.regressions = [
                {
                    "metric": "test_metric",
                    "expected": 1.0,
                    "actual": 1.2,
                    "delta_pct": 20,
                    "unit": "seconds",
                }
            ]
            validator.nfr_violations = [
                {
                    "nfr": "NFR-P1",
                    "requirement": ">=1000 words/sec",
                    "actual": "900 words/sec",
                    "metric": "throughput",
                }
            ]

            validator.generate_regression_report()

            # Check report was created
            report_file = tmp_path / "performance_regression_report.md"
            assert report_file.exists()

            # Verify content
            content = report_file.read_text()
            assert "Performance Regression Report" in content
            assert "test_metric" in content
            assert "NFR-P1" in content

    @pytest.mark.unit
    def test_update_baseline_documentation(self, tmp_path):
        """Test baseline documentation update."""
        with patch("validate_performance.PROJECT_ROOT", tmp_path):
            with patch("validate_performance.DOCS_DIR", tmp_path):
                validator = PerformanceValidator(update_baseline=True)

                # Add test results
                validator.test_results = {
                    "test_metric": {"value": 1.5, "unit": "seconds", "test": "test"}
                }

                result = validator.update_baseline_documentation()

                assert result is True

                # Check file was created
                baseline_file = tmp_path / "performance-baselines-epic-current.md"
                assert baseline_file.exists()

                content = baseline_file.read_text()
                assert "test_metric" in content
                assert "1.500" in content

    @pytest.mark.unit
    def test_generate_summary_pass(self):
        """Test summary generation - all pass."""
        validator = PerformanceValidator()

        # No regressions or violations
        validator.test_results = {"test": {"value": 1.0}}
        validator.regressions = []
        validator.nfr_violations = []

        result = validator.generate_summary()

        assert result is True

    @pytest.mark.unit
    def test_generate_summary_fail_ci_mode(self):
        """Test summary generation - failures in CI mode."""
        validator = PerformanceValidator(ci_mode=True)

        # Add regression
        validator.regressions = [{"metric": "test", "delta_pct": 20}]

        result = validator.generate_summary()

        assert result is False  # Should fail in CI mode with regressions

    @pytest.mark.unit
    def test_component_specific_tests(self):
        """Test component-specific test selection."""
        from validate_performance import COMPONENT_MAPPING

        # Test each component has appropriate tests
        assert "chunk" in COMPONENT_MAPPING
        assert "test_chunking_performance" in COMPONENT_MAPPING["chunk"]

        assert "semantic" in COMPONENT_MAPPING
        assert "test_tfidf_performance" in COMPONENT_MAPPING["semantic"]

        assert "output" in COMPONENT_MAPPING
        assert "test_json_performance" in COMPONENT_MAPPING["output"]


@pytest.mark.integration
class TestPerformanceValidatorIntegration:
    """Integration tests for performance validator."""

    @patch("subprocess.run")
    def test_full_validation_workflow(self, mock_run, mock_project_root):
        """Test complete validation workflow."""
        with patch("validate_performance.PROJECT_ROOT", mock_project_root):
            with patch("validate_performance.DOCS_DIR", mock_project_root / "docs"):
                with patch(
                    "validate_performance.PERFORMANCE_TESTS_DIR",
                    mock_project_root / "tests" / "performance",
                ):

                    validator = PerformanceValidator()

                    # Mock git diff to detect changes
                    mock_run.return_value = MagicMock(
                        returncode=0, stdout="src/data_extract/chunk/test.py", stderr=""
                    )

                    # Create test file
                    test_file = (
                        mock_project_root / "tests" / "performance" / "test_chunking_performance.py"
                    )
                    test_file.parent.mkdir(parents=True, exist_ok=True)
                    test_file.touch()

                    # Run validation (will fail due to missing pytest, but structure is tested)
                    result = validator.run()

                    # Should have attempted to parse baselines
                    assert validator.baseline_data is not None

    @patch("subprocess.run")
    def test_ci_mode_behavior(self, mock_run):
        """Test CI mode specific behavior."""
        validator = PerformanceValidator(ci_mode=True)

        # Add violations to trigger CI failure
        validator.nfr_violations = [{"nfr": "NFR-P1"}]

        result = validator.generate_summary()

        assert result is False  # Should fail in CI mode

    @patch("subprocess.run")
    def test_component_filtering(self, mock_run):
        """Test component-specific filtering."""
        validator = PerformanceValidator(component="chunk")

        # Mock successful git operations
        mock_run.return_value = MagicMock(returncode=0, stdout="", stderr="")

        # Component should be used for test selection
        assert validator.component == "chunk"
