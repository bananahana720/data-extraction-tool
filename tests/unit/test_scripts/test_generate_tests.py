"""
Test suite for the test generator script.

Tests story parsing, AC extraction, test stub generation,
fixture creation, and marker assignment.
"""

import sys
from unittest.mock import MagicMock, patch

import pytest

# Mock structlog before importing the script
sys.modules["structlog"] = MagicMock()

from scripts.generate_tests import main


class TestTestGenerator:
    """Test cases for the TestGenerator class."""

    @pytest.fixture
    def generator(self):
        """Create TestGenerator instance."""
        from scripts.generate_tests import StoryTestGenerator

        return StoryTestGenerator()

    @pytest.fixture
    def sample_story_content(self):
        """Sample story markdown content."""
        return """
# Story 4.1: TF-IDF Vectorization Engine

Status: ready-for-dev

## Story

As a data scientist,
I want TF-IDF vectorization capabilities,
So that I can analyze document similarity.

### Story Header

- **Story Key:** `4-1-tf-idf-vectorization`
- **Epic:** 4

## Acceptance Criteria

1. **Import scanning:** Script scans all test files for imports.
2. **Cross-reference validation:** Compares imports with pyproject.toml.
3. **Performance optimization:** Process completes within 5 seconds.
4. **Integration testing:** End-to-end pipeline validation works.

## Tasks / Subtasks

- [ ] **Task 1: Core implementation**
  - [ ] Create vectorizer class
  - [ ] Implement TF-IDF algorithm

- [ ] **Task 2: Testing**
  - [ ] Write unit tests
  - [ ] Add integration tests

## Dev Notes

NFR-P1: Performance requirement - must handle 1000 documents.
"""

    @pytest.fixture
    def story_file(self, tmp_path, sample_story_content):
        """Create temporary story file."""
        story_path = tmp_path / "4-1-tf-idf-vectorization.md"
        story_path.write_text(sample_story_content)
        return story_path

    @pytest.mark.unit
    def test_init(self, generator):
        """Test TestGenerator initialization."""
        assert generator.story_content == ""
        assert generator.story_key == ""
        assert generator.epic_number == ""
        assert generator.story_number == ""
        assert generator.acceptance_criteria == []
        assert generator.tasks == []
        assert generator.has_performance_requirements is False

    @pytest.mark.unit
    def test_parse_story_file_success(self, generator, story_file):
        """Test successful story file parsing."""
        result = generator.parse_story_file(story_file)

        assert result is True
        assert generator.story_key == "4-1-tf-idf-vectorization"
        assert generator.epic_number == "4"
        assert generator.story_number == "1"
        assert len(generator.acceptance_criteria) == 4
        assert len(generator.tasks) == 2
        assert generator.has_performance_requirements is True

    @pytest.mark.unit
    def test_parse_story_file_not_found(self, generator, tmp_path):
        """Test parsing non-existent story file."""
        non_existent = tmp_path / "non_existent.md"
        result = generator.parse_story_file(non_existent)

        assert result is False

    @pytest.mark.unit
    def test_extract_story_key_from_filename(self, generator, tmp_path):
        """Test story key extraction from filename."""
        # Standard format
        story_path = tmp_path / "4-1-tf-idf-vectorization.md"
        generator._extract_story_key(story_path)

        assert generator.story_key == "4-1-tf-idf-vectorization"
        assert generator.epic_number == "4"
        assert generator.story_number == "1"

        # Epic 3.5 format
        story_path = tmp_path / "3.5-8-dependency-test-automation.md"
        generator._extract_story_key(story_path)

        assert generator.story_key == "3.5-8-dependency-test-automation"
        assert generator.epic_number == "3.5"
        assert generator.story_number == "8"

    @pytest.mark.unit
    def test_extract_story_key_from_content(self, generator, tmp_path):
        """Test story key extraction from content."""
        generator.story_content = "Story Key: `5-2-config-management`\nOther content"
        story_path = tmp_path / "some_file.md"

        generator._extract_story_key(story_path)

        assert generator.story_key == "5-2-config-management"
        assert generator.epic_number == "5"
        assert generator.story_number == "2"

    @pytest.mark.unit
    def test_extract_acceptance_criteria(self, generator, sample_story_content):
        """Test acceptance criteria extraction."""
        generator.story_content = sample_story_content
        generator._extract_acceptance_criteria()

        assert len(generator.acceptance_criteria) == 4

        # Check first AC
        ac1 = generator.acceptance_criteria[0]
        assert ac1["number"] == "1"
        assert ac1["title"] == "Import scanning"
        assert "Script scans all test files" in ac1["description"]
        assert ac1["id"] == "AC-1"

        # Check performance AC
        ac3 = generator.acceptance_criteria[2]
        assert ac3["number"] == "3"
        assert ac3["title"] == "Performance optimization"

    @pytest.mark.unit
    def test_extract_tasks(self, generator, sample_story_content):
        """Test task extraction."""
        generator.story_content = sample_story_content
        generator._extract_tasks()

        assert len(generator.tasks) == 2
        assert generator.tasks[0] == "Task 1: Core implementation"
        assert generator.tasks[1] == "Task 2: Testing"

    @pytest.mark.unit
    def test_check_performance_requirements(self, generator):
        """Test performance requirement detection."""
        # With performance keywords
        generator.story_content = "NFR requirements, performance testing, throughput optimization"
        generator._check_performance_requirements()
        assert generator.has_performance_requirements is True

        # Without performance keywords
        generator.story_content = "Simple feature implementation"
        generator._check_performance_requirements()
        assert generator.has_performance_requirements is False

    @pytest.mark.unit
    def test_get_module_name(self, generator):
        """Test module name determination."""
        test_cases = [
            ("1", "infrastructure"),
            ("2", "normalize"),
            ("2.5", "infrastructure"),
            ("3", "chunk"),
            ("3.5", "scripts"),
            ("4", "semantic"),
            ("5", "cli"),
            ("99", "misc"),
        ]

        for epic, expected in test_cases:
            generator.epic_number = epic
            assert generator._get_module_name() == expected

    @pytest.mark.unit
    def test_determine_markers_auto(self, generator):
        """Test automatic marker determination."""
        # Unit test AC
        ac_unit = {"title": "Parse function", "description": "Validate input parsing method"}
        markers = generator._determine_markers(ac_unit, "auto")
        assert "unit" in markers

        # Integration test AC
        ac_integration = {
            "title": "Pipeline validation",
            "description": "End-to-end workflow testing",
        }
        markers = generator._determine_markers(ac_integration, "auto")
        assert "integration" in markers

        # Performance test AC
        ac_performance = {"title": "Throughput test", "description": "NFR performance benchmarking"}
        markers = generator._determine_markers(ac_performance, "auto")
        assert "performance" in markers

    @pytest.mark.unit
    def test_determine_markers_fixed(self, generator):
        """Test fixed marker assignment."""
        ac = {"title": "Any", "description": "Any"}

        assert generator._determine_markers(ac, "unit") == ["unit"]
        assert generator._determine_markers(ac, "integration") == ["integration"]
        assert generator._determine_markers(ac, "both") == ["unit", "integration"]

    @pytest.mark.unit
    def test_slugify(self, generator):
        """Test text slugification."""
        assert generator._slugify("Simple Text") == "simple_text"
        assert generator._slugify("Cross-reference validation") == "crossreference_validation"
        assert generator._slugify("123 Numbers First") == "n_123_numbers_first"
        assert generator._slugify("Special!@#$%Characters") == "specialcharacters"

        # Test length limit
        long_text = "a" * 100
        assert len(generator._slugify(long_text)) == 50

    @pytest.mark.unit
    def test_generate_class_name(self, generator):
        """Test test class name generation."""
        generator.story_key = "4-1-tf-idf-vectorization"
        assert generator._generate_class_name() == "TestStory4_1TfIdfVectorization"

        generator.story_key = "3.5-8-dependency-test"
        assert generator._generate_class_name() == "TestStory3_5_8DependencyTest"

    @pytest.mark.integration
    def test_generate_test_file(self, generator, story_file, tmp_path):
        """Test complete test file generation."""
        generator.parse_story_file(story_file)

        output_dir = tmp_path / "tests"
        test_path, content = generator.generate_test_file(output_dir)

        # Verify file created
        assert test_path.exists()
        assert test_path.name == "test_4_1_tf_idf_vectorization.py"

        # Verify content structure
        assert "class TestStory4_1TfIdfVectorization:" in content
        assert "@pytest.mark" in content
        assert "def test_ac_1_import_scanning" in content
        assert "def test_ac_2_cross_reference_validation" in content
        assert "def test_ac_3_performance_optimization" in content
        assert "def test_ac_4_integration_testing" in content

        # Performance template should be included
        assert "def test_performance_requirements" in content

    @pytest.mark.integration
    def test_generate_fixture_file(self, generator, story_file, tmp_path):
        """Test fixture file generation."""
        generator.parse_story_file(story_file)

        output_dir = tmp_path / "fixtures"
        fixture_path, content = generator.generate_fixture_file(output_dir)

        # Verify file created
        assert fixture_path.exists()
        assert fixture_path.name == "4_1_tf_idf_vectorization_fixtures.py"

        # Verify content structure
        assert "@pytest.fixture" in content
        assert "def sample_data()" in content
        assert "def test_config()" in content
        assert "def mock_dependencies(monkeypatch)" in content

        # Should have data for each AC
        assert "'ac_1_input'" in content
        assert "'ac_2_input'" in content
        assert "'performance_input'" in content

        # Should have helper functions for tasks
        assert "def setup_task_1_core_implementation" in content

    @pytest.mark.unit
    def test_generate_test_content_with_semantic_module(self, generator):
        """Test test content generation for semantic module."""
        generator.story_key = "4-1-tf-idf"
        generator.epic_number = "4"
        generator.acceptance_criteria = [
            {
                "number": "1",
                "title": "TF-IDF",
                "description": "Implement vectorization",
                "id": "AC-1",
            }
        ]

        content = generator._generate_test_content("auto")

        # Should include semantic-specific imports
        assert "import numpy as np" in content
        assert "from sklearn.feature_extraction.text import TfidfVectorizer" in content

    @pytest.mark.unit
    def test_generate_test_content_with_cli_module(self, generator):
        """Test test content generation for CLI module."""
        generator.story_key = "5-1-cli-commands"
        generator.epic_number = "5"
        generator.acceptance_criteria = [
            {"number": "1", "title": "CLI", "description": "CLI command", "id": "AC-1"}
        ]

        content = generator._generate_test_content("auto")

        # Should include CLI-specific imports
        assert "from typer.testing import CliRunner" in content
        assert "from data_extract.cli import app" in content

    @pytest.mark.integration
    @patch("sys.argv", ["generate_tests.py", "--story", "test.md"])
    def test_main_success(self, tmp_path, monkeypatch):
        """Test main function with successful execution."""
        monkeypatch.chdir(tmp_path)

        # Create story file
        story_content = """
# Story 1-1: Test Story

## Acceptance Criteria

1. **Test AC:** Test description.

## Tasks / Subtasks

- [ ] Test task
"""
        story_file = tmp_path / "test.md"
        story_file.write_text(story_content)

        # Mock sys.argv for argparse
        with patch("sys.argv", ["generate_tests.py", "--story", str(story_file)]):
            with patch("sys.exit"):
                main()

        # Verify files created
        assert (tmp_path / "tests" / "fixtures" / "1_1_test_story_fixtures.py").exists()
        assert (
            tmp_path / "tests" / "unit" / "test_infrastructure" / "test_1_1_test_story.py"
        ).exists()

    @pytest.mark.integration
    @patch("sys.argv", ["generate_tests.py", "--story", "test.md", "--fixtures-only"])
    def test_main_fixtures_only(self, tmp_path, monkeypatch):
        """Test main function with fixtures-only mode."""
        monkeypatch.chdir(tmp_path)

        # Create story file
        story_content = """
# Story 1-1: Test

## Acceptance Criteria

1. **Test:** Test.
"""
        story_file = tmp_path / "test.md"
        story_file.write_text(story_content)

        with patch(
            "sys.argv", ["generate_tests.py", "--story", str(story_file), "--fixtures-only"]
        ):
            with patch("sys.exit"):
                main()

        # Only fixture file should be created
        assert (tmp_path / "tests" / "fixtures" / "1_1_test_fixtures.py").exists()
        assert not (
            tmp_path / "tests" / "unit" / "test_infrastructure" / "test_1_1_test.py"
        ).exists()

    @pytest.mark.unit
    def test_generate_performance_test_template(self, generator):
        """Test performance test template generation."""
        template = generator._generate_performance_test_template()

        # Verify template structure
        template_str = "\n".join(template)
        assert "@pytest.mark.performance" in template_str
        assert "@pytest.mark.slow" in template_str
        assert "def test_performance_requirements" in template_str
        assert "import time" in template_str
        assert "import psutil" in template_str
        assert "elapsed_time < 1.0" in template_str
        assert "memory_used < 100" in template_str
