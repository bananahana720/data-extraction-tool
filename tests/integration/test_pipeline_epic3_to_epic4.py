"""
Integration tests for Epic 3 â†’ Epic 4 handoff (Chunk â†’ Semantic).

CRITICAL: These tests validate that chunked output from Epic 3 is compatible
with semantic analysis requirements in Epic 4.

Tests cover:
- Data format compatibility
- Metadata preservation
- Serialization/deserialization
- Performance within NFR limits
- Error handling and recovery

Test IDs:
- E34-001: Chunk format compatible with TF-IDF
- E34-002: ChunkMetadata fully JSON serializable
- E34-003: Batch chunking memory stable
- E34-004: Empty chunk handling
- E34-005: Chunk IDs unique and valid
- E34-006: Chunks vectorizable with TF-IDF
- E34-007: Required metadata fields present
- E34-008: Entity boundaries preserved
- E34-009: Vectorization performance within NFR
- E34-010: Chunk serialization performance
- E34-011: Special character handling
- E34-012: Large corpus processing

Author: Wave 2.2 Agent (Winston)
Date: 2025-11-18
Sprint: Epic 4 Preparation (Wave 2)
"""

import gc
import json
import time
import tracemalloc
from pathlib import Path
from typing import List

import numpy as np
import pytest
from sklearn.feature_extraction.text import TfidfVectorizer
from src.data_extract.extract.pdf_extractor import PdfExtractor
from src.data_extract.normalize.text_normalizer import TextNormalizer

# Import Epic 3 components
from src.data_extract.chunk.engine import ChunkingEngine
from src.data_extract.chunk.models import Chunk
from src.data_extract.core.models import ContentBlock, ContentType, Document, Metadata, Position

# Markers for test organization
pytestmark = [
    pytest.mark.integration,
    pytest.mark.p0,  # Critical for Epic 4
    pytest.mark.chunking,
    pytest.mark.semantic,
]


class TestChunkToSemanticCompatibility:
    """Test compatibility between Chunk output and Semantic input."""

    @pytest.fixture
    def sample_chunks(self, tmp_path) -> List[Chunk]:
        """
        Create realistic chunks using actual pipeline.

        IMPORTANT: Uses real extractor â†’ normalizer â†’ chunker pipeline
        to ensure chunks match production format.
        """
        # First try to use existing test fixture
        pdf_path = Path("tests/fixtures/greenfield/simple.pdf")
        if not pdf_path.exists():
            # Fallback to another fixture
            pdf_path = Path("tests/fixtures/sample_audit_report.pdf")

        if not pdf_path.exists():
            # Create a minimal test document if no fixtures exist
            from src.data_extract.core.models import ValidationReport

            test_doc = Document(
                id="test_doc_001",
                text="This is a test document for Epic 3 to Epic 4 handoff testing. " * 20
                + "\n\nSection 2: Risk Assessment\n"
                + "The organization faces several key risks including RISK-001: Data breach risk "
                + "which is mitigated by CTRL-042: Access control policy. " * 10
                + "\n\nSection 3: Control Framework\n"
                + "The control framework implements multiple layers of security. " * 10,
                metadata=Metadata(
                    source_file=pdf_path,
                    document_type="report",
                    extraction_method="test",
                    processing_timestamp="2025-11-18T12:00:00Z",
                ),
                structure={
                    "page_count": 3,
                    "word_count": 500,
                    "has_images": False,
                    "has_tables": False,
                    "sections": ["Introduction", "Risk Assessment", "Control Framework"],
                },
                validation=ValidationReport(is_valid=True, confidence_score=0.98, flags=[]),
                entities=[],
                content_blocks=[
                    ContentBlock(
                        block_type=ContentType.HEADING,
                        content="Section 1: Introduction",
                        position=Position(page=1, sequence_index=0),
                        metadata={},
                    ),
                    ContentBlock(
                        block_type=ContentType.PARAGRAPH,
                        content="This is a test document for Epic 3 to Epic 4 handoff testing. "
                        * 20,
                        position=Position(page=1, sequence_index=1),
                        metadata={},
                    ),
                ],
            )
        else:
            # Use real extractor if fixture exists
            extractor = PdfExtractor()
            extraction_result = extractor.extract(pdf_path)

            if extraction_result.success:
                test_doc = extraction_result.document
            else:
                pytest.skip(f"Failed to extract test document: {extraction_result.error}")

        # Normalize the document
        normalizer = TextNormalizer()
        normalized_result = normalizer.normalize(test_doc)

        if normalized_result.success:
            normalized_doc = normalized_result.document
        else:
            normalized_doc = test_doc  # Use unnormalized if normalization fails

        # Chunk the document
        chunker = ChunkingEngine()
        chunk_result = chunker.process(normalized_doc)

        if not chunk_result.success:
            pytest.skip(f"Failed to chunk document: {chunk_result.error}")

        return chunk_result.chunks

    @pytest.mark.p0
    def test_chunks_have_text_content(self, sample_chunks):
        """
        Test ID: E34-001 - Ensure all chunks have non-empty text.

        CRITICAL: Empty text causes TF-IDF to crash with:
        ValueError: empty vocabulary; perhaps the documents only contain stop words
        """
        assert len(sample_chunks) > 0, "Should produce at least one chunk"

        empty_chunks = []
        for i, chunk in enumerate(sample_chunks):
            if not chunk.text or not chunk.text.strip():
                empty_chunks.append((i, chunk.id))

        assert len(empty_chunks) == 0, f"Found {len(empty_chunks)} empty chunks: {empty_chunks}"

        # Additional validation
        for chunk in sample_chunks:
            words = chunk.text.split()
            assert len(words) > 0, f"Chunk {chunk.id} has no words"
            assert chunk.word_count > 0, f"Chunk {chunk.id} reports 0 word_count"

    @pytest.mark.p0
    def test_chunk_metadata_serializable(self, sample_chunks):
        """
        Test ID: E34-002 - Ensure chunk metadata can be serialized to JSON.

        CRITICAL: Non-serializable metadata breaks semantic caching:
        TypeError: Object of type numpy.ndarray is not JSON serializable
        """
        serialization_errors = []

        for i, chunk in enumerate(sample_chunks):
            try:
                # Get metadata as dict
                if hasattr(chunk, "metadata") and hasattr(chunk.metadata, "to_dict"):
                    metadata_dict = chunk.metadata.to_dict()
                elif hasattr(chunk, "metadata") and isinstance(chunk.metadata, dict):
                    metadata_dict = chunk.metadata
                else:
                    metadata_dict = {}

                # Attempt JSON serialization
                json_str = json.dumps(
                    {
                        "chunk_id": chunk.id,
                        "text": chunk.text,
                        "document_id": chunk.document_id,
                        "position_index": chunk.position_index,
                        "word_count": chunk.word_count,
                        "token_count": chunk.token_count,
                        "metadata": metadata_dict,
                    }
                )

                # Verify round-trip
                deserialized = json.loads(json_str)
                assert deserialized["chunk_id"] == chunk.id
                assert deserialized["text"] == chunk.text

            except (TypeError, ValueError) as e:
                serialization_errors.append((chunk.id, str(e)))

        assert (
            len(serialization_errors) == 0
        ), f"Serialization failed for chunks: {serialization_errors}"

    @pytest.mark.p0
    def test_chunk_ids_unique_and_valid(self, sample_chunks):
        """
        Test ID: E34-005 - Validate chunk IDs are unique and properly formatted.

        CRITICAL: Duplicate IDs break similarity analysis tracking.
        Expected format: {source}_{index:03d}
        """
        chunk_ids = [chunk.id for chunk in sample_chunks]

        # Check uniqueness
        duplicates = [id for id in chunk_ids if chunk_ids.count(id) > 1]
        assert len(duplicates) == 0, f"Duplicate chunk IDs found: {set(duplicates)}"

        # Check format
        format_errors = []
        for chunk_id in chunk_ids:
            # Must be string
            assert isinstance(chunk_id, str), f"Chunk ID must be string, got {type(chunk_id)}"

            # Must not be empty
            assert chunk_id, "Chunk ID cannot be empty"

            # Should follow pattern {source}_{index}
            if "_" in chunk_id:
                parts = chunk_id.split("_")
                if len(parts) == 2:
                    source, index_str = parts
                    # Index should be numeric
                    if not index_str.isdigit():
                        format_errors.append(f"{chunk_id}: index '{index_str}' not numeric")
                else:
                    format_errors.append(f"{chunk_id}: wrong number of parts ({len(parts)})")
            else:
                # ID without underscore is acceptable but note it
                pass  # Some implementations might use different format

        if format_errors:
            pytest.fail(f"Chunk ID format errors: {format_errors}")

    @pytest.mark.p0
    def test_chunks_vectorizable_with_tfidf(self, sample_chunks):
        """
        Test ID: E34-006 - Validate chunks can be vectorized with TF-IDF.

        This is the CORE Epic 3â†’4 compatibility test.
        """
        # Extract text from chunks
        corpus = [chunk.text for chunk in sample_chunks]

        assert len(corpus) > 0, "No chunks to vectorize"

        # Configure TF-IDF vectorizer with Epic 4 settings
        vectorizer = TfidfVectorizer(
            max_features=1000,  # Reasonable limit for testing
            min_df=1,  # Accept single occurrence
            max_df=0.95,  # Remove very common terms
            ngram_range=(1, 2),  # Unigrams and bigrams
            sublinear_tf=True,  # Use log normalization
            use_idf=True,  # Use IDF weighting
        )

        try:
            # Attempt vectorization
            vectors = vectorizer.fit_transform(corpus)

            # Validate output shape
            assert vectors.shape[0] == len(corpus), "One vector per chunk"
            assert vectors.shape[1] > 0, "Should have vocabulary"

            # Check for NaN or Inf values
            assert np.isfinite(vectors.data).all(), "Vectors contain NaN or Inf"

            # Verify vocabulary was learned
            vocab = vectorizer.vocabulary_
            assert len(vocab) > 0, "Empty vocabulary"

            # Test transform on new data (simulates Epic 4 usage)
            new_text = ["This is a test chunk for semantic analysis"]
            new_vector = vectorizer.transform(new_text)
            assert new_vector.shape[0] == 1

        except ValueError as e:
            pytest.fail(f"TF-IDF vectorization failed: {e}")

    def test_chunk_metadata_includes_required_fields(self, sample_chunks):
        """
        Test ID: E34-007 - Ensure required metadata fields exist for semantic analysis.

        Required fields for Epic 4:
        - quality_score or quality: For filtering low-quality chunks
        - section_context: For understanding chunk location
        - entity_tags or entities: For entity-aware semantic analysis
        - word_count/token_count: For size-based processing
        """
        missing_fields = []

        for i, chunk in enumerate(sample_chunks):
            errors = []

            # Check for quality score (various possible field names)
            has_quality = (
                hasattr(chunk, "quality_score")
                or hasattr(chunk, "quality")
                or (
                    hasattr(chunk, "metadata")
                    and isinstance(chunk.metadata, dict)
                    and "quality" in chunk.metadata
                )
            )
            if not has_quality:
                errors.append("quality_score")

            # Check for section context
            has_context = hasattr(chunk, "section_context") or (
                hasattr(chunk, "metadata") and hasattr(chunk.metadata, "section_context")
            )
            if not has_context:
                errors.append("section_context")

            # Check for word/token counts
            if not hasattr(chunk, "word_count") or chunk.word_count == 0:
                errors.append("word_count")
            if not hasattr(chunk, "token_count") or chunk.token_count == 0:
                errors.append("token_count")

            if errors:
                missing_fields.append((chunk.id, errors))

        # We'll be lenient here - warn but don't fail if some fields are missing
        if missing_fields:
            print(f"Warning: Some chunks missing metadata fields: {missing_fields[:3]}")

    def test_chunks_preserve_entity_boundaries(self, sample_chunks):
        """
        Test ID: E34-008 - Validate that entities are not split across chunks.

        CRITICAL: Split entities break entity-aware semantic analysis.
        """
        split_entities = []

        for i, chunk in enumerate(sample_chunks):
            # Check if chunk has entity information
            if hasattr(chunk, "entities") and chunk.entities:
                # Entities are in the chunk
                for entity in chunk.entities:
                    if hasattr(entity, "text"):
                        entity_text = entity.text
                        # Verify entity text is fully contained in chunk
                        if entity_text not in chunk.text:
                            split_entities.append((chunk.id, entity_text))

            # Also check metadata for entity references
            if hasattr(chunk, "metadata"):
                if hasattr(chunk.metadata, "entity_tags"):
                    entity_refs = chunk.metadata.entity_tags
                elif isinstance(chunk.metadata, dict) and "entity_tags" in chunk.metadata:
                    entity_refs = chunk.metadata["entity_tags"]
                else:
                    entity_refs = []

                for entity_ref in entity_refs:
                    if isinstance(entity_ref, dict) and "text" in entity_ref:
                        entity_text = entity_ref["text"]
                        if entity_text and entity_text not in chunk.text:
                            split_entities.append((chunk.id, entity_text))

        assert (
            len(split_entities) == 0
        ), f"Found {len(split_entities)} split entities: {split_entities[:5]}"


class TestChunkToSemanticPerformance:
    """Test performance NFRs for Epic 3â†’4 handoff."""

    @pytest.fixture
    def large_corpus(self) -> List[Chunk]:
        """Generate a large corpus for performance testing."""
        chunks = []

        # Generate 100 chunks with varying sizes
        for i in range(100):
            text = f"This is chunk number {i}. " * (50 + i % 20)
            text += f" It contains entities like RISK-{i:03d} and CTRL-{i:03d}."

            chunk = Chunk(
                id=f"perf_test_{i:03d}",
                text=text,
                document_id="perf_doc_001",
                position_index=i,
                word_count=len(text.split()),
                token_count=len(text) // 4,
                entities=[],
                section_context=f"Section {i // 10}",
                quality_score=0.9,
                readability_scores={"flesch_reading_ease": 60.0},
                metadata=Metadata(
                    source_file=Path("test.pdf"),
                    document_type="report",
                    extraction_method="test",
                    processing_timestamp="2025-11-18T12:00:00Z",
                ),
            )
            chunks.append(chunk)

        return chunks

    @pytest.mark.performance
    def test_vectorization_within_nfr_limits(self, large_corpus):
        """
        Test ID: E34-009 - Validate TF-IDF vectorization meets performance baselines.

        NFR-P1: TF-IDF fit/transform <100ms per 1k-word document
        """
        corpus_text = [chunk.text for chunk in large_corpus]
        total_words = sum(len(text.split()) for text in corpus_text)

        # Configure vectorizer
        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))

        # Measure vectorization time
        start = time.perf_counter()
        vectors = vectorizer.fit_transform(corpus_text)
        duration_ms = (time.perf_counter() - start) * 1000

        # Calculate per-1k-word time
        time_per_1k_words = (duration_ms / total_words) * 1000

        print(f"Performance: {duration_ms:.2f}ms for {total_words} words")
        print(f"Per 1k words: {time_per_1k_words:.2f}ms")

        # Allow some tolerance (2x the target for test environment)
        assert (
            time_per_1k_words < 200
        ), f"TF-IDF too slow: {time_per_1k_words:.2f}ms per 1k words (limit: 100ms, tolerance: 200ms)"

    @pytest.mark.performance
    def test_chunk_serialization_performance(self, large_corpus):
        """
        Test ID: E34-010 - Validate chunk serialization is fast enough for caching.

        Target: <50ms for typical chunk set (100 chunks)
        """
        start = time.perf_counter()

        serialized_chunks = []
        for chunk in large_corpus:
            # Serialize each chunk
            chunk_dict = {
                "chunk_id": chunk.id,
                "text": chunk.text,
                "document_id": chunk.document_id,
                "position_index": chunk.position_index,
                "word_count": chunk.word_count,
                "token_count": chunk.token_count,
                "section_context": chunk.section_context,
                "quality_score": chunk.quality_score,
            }
            serialized_chunks.append(chunk_dict)

        # Convert to JSON
        json_str = json.dumps({"chunks": serialized_chunks})

        duration_ms = (time.perf_counter() - start) * 1000

        print(f"Serialization: {duration_ms:.2f}ms for {len(large_corpus)} chunks")
        print(f"JSON size: {len(json_str) / 1024:.2f} KB")

        # Allow some tolerance (2x the target for test environment)
        assert (
            duration_ms < 100
        ), f"Serialization too slow: {duration_ms:.2f}ms (limit: 50ms, tolerance: 100ms)"

    @pytest.mark.performance
    def test_memory_stability_batch_processing(self):
        """
        Test ID: E34-003 - Batch chunking memory stable.

        Process multiple batches and verify memory doesn't grow unbounded.
        """
        tracemalloc.start()
        baseline = tracemalloc.take_snapshot()

        # Process 10 batches of 100 chunks each
        for batch in range(10):
            chunks = []
            for i in range(100):
                chunk = Chunk(
                    id=f"batch{batch}_chunk{i:03d}",
                    text=f"Batch {batch} chunk {i} content " * 50,
                    document_id=f"batch{batch}_doc",
                    position_index=i,
                    word_count=52,
                    token_count=200,
                    entities=[],
                    section_context="Test",
                    quality_score=0.9,
                    readability_scores={},
                    metadata=Metadata(
                        source_file=Path("test.pdf"),
                        document_type="report",
                        extraction_method="test",
                        processing_timestamp="2025-11-18T12:00:00Z",
                    ),
                )
                chunks.append(chunk)

            # Simulate processing (serialization)
            for chunk in chunks:
                _ = json.dumps({"id": chunk.id, "text": chunk.text})

            # Clear references
            chunks.clear()

            # Force garbage collection every few batches
            if batch % 3 == 0:
                gc.collect()

        # Measure final memory
        peak = tracemalloc.take_snapshot()
        top_stats = peak.compare_to(baseline, "lineno")

        # Calculate total memory growth
        total_diff = sum(stat.size_diff for stat in top_stats[:20] if stat.size_diff > 0)
        total_diff_mb = total_diff / (1024 * 1024)

        print(f"Memory growth after 10 batches: {total_diff_mb:.2f} MB")

        # Memory should not grow more than 50MB for 1000 chunks
        assert total_diff_mb < 50, f"Memory grew by {total_diff_mb:.2f}MB (limit: 50MB)"

        tracemalloc.stop()


class TestChunkToSemanticErrorRecovery:
    """Test error handling for Epic 3â†’4 handoff."""

    def test_empty_chunk_handling(self):
        """
        Test ID: E34-004 - Validate graceful handling of empty chunks.

        Edge case: What if chunker produces empty chunk?
        """
        # Create chunks with edge cases
        edge_chunks = [
            Chunk(
                id="empty_001",
                text="",  # Empty!
                document_id="test_doc",
                position_index=0,
                word_count=0,
                token_count=0,
                entities=[],
                section_context="",
                quality_score=0.0,
                readability_scores={},
                metadata=Metadata(
                    source_file=Path("test.pdf"),
                    document_type="report",
                    extraction_method="test",
                    processing_timestamp="2025-11-18T12:00:00Z",
                ),
            ),
            Chunk(
                id="normal_001",
                text="This is normal text content.",
                document_id="test_doc",
                position_index=1,
                word_count=5,
                token_count=20,
                entities=[],
                section_context="Section 1",
                quality_score=0.9,
                readability_scores={},
                metadata=Metadata(
                    source_file=Path("test.pdf"),
                    document_type="report",
                    extraction_method="test",
                    processing_timestamp="2025-11-18T12:00:00Z",
                ),
            ),
        ]

        # TF-IDF should handle mixed empty/non-empty gracefully
        corpus = [chunk.text for chunk in edge_chunks]

        # Filter out empty chunks (Epic 4 should do this)
        non_empty_corpus = [text for text in corpus if text.strip()]

        if non_empty_corpus:
            vectorizer = TfidfVectorizer()
            vectors = vectorizer.fit_transform(non_empty_corpus)
            assert vectors.shape[0] == len(non_empty_corpus)
        else:
            # All chunks empty - this should be caught earlier in pipeline
            pass

    def test_special_character_handling(self):
        """
        Test ID: E34-011 - Validate TF-IDF handles chunks with special characters.

        Edge case: Unicode, emojis, control characters
        """
        special_chunks = [
            Chunk(
                id=f"special_{i:03d}",
                text=text,
                document_id="special_doc",
                position_index=i,
                word_count=len(text.split()),
                token_count=len(text) // 4,
                entities=[],
                section_context="Special",
                quality_score=0.9,
                readability_scores={},
                metadata=Metadata(
                    source_file=Path("test.pdf"),
                    document_type="report",
                    extraction_method="test",
                    processing_timestamp="2025-11-18T12:00:00Z",
                ),
            )
            for i, text in enumerate(
                [
                    "Normal ASCII text content",
                    "Text with Ã©mojis ðŸŽ‰ and Ã¼nÃ¯cÃ¶dÃ© characters",
                    "Text\twith\ttabs\nand\nnewlines",
                    "ALLCAPS TEXT AND NUMBERS 123456",
                    "Special symbols: @#$% & *()[]{}",
                    "Mixed ä¸­æ–‡ and English ãƒ†ã‚­ã‚¹ãƒˆ",
                ]
            )
        ]

        corpus = [chunk.text for chunk in special_chunks]

        # TF-IDF should handle all these without crashing
        vectorizer = TfidfVectorizer(
            strip_accents="unicode",  # Handle unicode properly
            lowercase=True,
            token_pattern=r"\b\w+\b",  # More flexible token pattern
        )

        try:
            vectors = vectorizer.fit_transform(corpus)
            assert vectors.shape[0] == len(corpus)

            # Check that we got reasonable vocabulary
            vocab = vectorizer.vocabulary_
            assert len(vocab) > 10, f"Vocabulary too small: {len(vocab)}"

        except Exception as e:
            pytest.fail(f"Failed to handle special characters: {e}")

    def test_large_corpus_processing(self):
        """
        Test ID: E34-012 - Validate processing of large corpus.

        Stress test with 1000+ chunks to catch memory/performance issues.
        """
        # Generate large corpus
        large_chunks = []
        for doc_id in range(10):  # 10 documents
            for chunk_id in range(100):  # 100 chunks per document
                text = f"Document {doc_id} chunk {chunk_id}. " * 30
                chunk = Chunk(
                    id=f"doc{doc_id:02d}_chunk{chunk_id:03d}",
                    text=text,
                    document_id=f"large_doc_{doc_id:02d}",
                    position_index=chunk_id,
                    word_count=len(text.split()),
                    token_count=len(text) // 4,
                    entities=[],
                    section_context=f"Section {chunk_id // 10}",
                    quality_score=0.85 + (chunk_id % 10) * 0.01,
                    readability_scores={"flesch_reading_ease": 50.0 + chunk_id % 30},
                    metadata=Metadata(
                        source_file=Path(f"doc_{doc_id}.pdf"),
                        document_type="report",
                        extraction_method="test",
                        processing_timestamp="2025-11-18T12:00:00Z",
                    ),
                )
                large_chunks.append(chunk)

        assert len(large_chunks) == 1000

        # Process in batches (simulating real pipeline)
        batch_size = 100
        all_vectors = []

        vectorizer = TfidfVectorizer(max_features=5000)

        for i in range(0, len(large_chunks), batch_size):
            batch = large_chunks[i : i + batch_size]
            corpus = [chunk.text for chunk in batch]

            if i == 0:
                # Fit on first batch
                batch_vectors = vectorizer.fit_transform(corpus)
            else:
                # Transform subsequent batches
                batch_vectors = vectorizer.transform(corpus)

            all_vectors.append(batch_vectors)

        # Verify we processed everything
        from scipy.sparse import vstack

        final_vectors = vstack(all_vectors)
        assert final_vectors.shape[0] == 1000


class TestChunkOutputContract:
    """Validate the contract that Epic 3 chunks must fulfill for Epic 4."""

    def test_chunk_model_contract(self):
        """
        Test the Chunk model contract that Epic 4 depends on.

        This test documents the exact fields and types Epic 4 expects.
        """
        # Create a minimal valid chunk
        chunk = Chunk(
            id="contract_001",
            text="Sample text",
            document_id="doc_001",
            position_index=0,
            word_count=2,
            token_count=8,
            entities=[],
            section_context="Test Section",
            quality_score=0.9,
            readability_scores={"flesch_reading_ease": 60.0},
            metadata=Metadata(
                source_file=Path("test.pdf"),
                document_type="report",
                extraction_method="test",
                processing_timestamp="2025-11-18T12:00:00Z",
            ),
        )

        # Verify required fields exist and have correct types
        assert isinstance(chunk.id, str)
        assert isinstance(chunk.text, str)
        assert isinstance(chunk.document_id, str)
        assert isinstance(chunk.position_index, int)
        assert chunk.position_index >= 0

        # Numeric fields
        assert isinstance(chunk.word_count, int)
        assert chunk.word_count >= 0
        assert isinstance(chunk.token_count, int)
        assert chunk.token_count >= 0

        # Quality score
        assert isinstance(chunk.quality_score, (int, float))
        assert 0.0 <= chunk.quality_score <= 1.0

        # These fields can be accessed for Epic 4
        assert hasattr(chunk, "id")
        assert hasattr(chunk, "text")
        assert hasattr(chunk, "document_id")
        assert hasattr(chunk, "word_count")
        assert hasattr(chunk, "token_count")
        assert hasattr(chunk, "quality_score")

    def test_chunk_to_dict_contract(self):
        """
        Test that chunks can be converted to dictionary for serialization.

        Epic 4 will need to serialize chunks for caching and storage.
        """
        chunk = Chunk(
            id="dict_test_001",
            text="Test content for dictionary conversion",
            document_id="doc_001",
            position_index=0,
            word_count=6,
            token_count=24,
            entities=[],
            section_context="Section 1",
            quality_score=0.95,
            readability_scores={"flesch_reading_ease": 65.0},
            metadata=Metadata(
                source_file=Path("test.pdf"),
                document_type="report",
                extraction_method="test",
                processing_timestamp="2025-11-18T12:00:00Z",
            ),
        )

        # Convert to dictionary (Epic 4 will do this)
        if hasattr(chunk, "model_dump"):
            # Pydantic v2 model
            chunk_dict = chunk.model_dump()
        elif hasattr(chunk, "dict"):
            # Pydantic v1 model
            chunk_dict = chunk.dict()
        else:
            # Manual conversion
            chunk_dict = {
                "id": chunk.id,
                "text": chunk.text,
                "document_id": chunk.document_id,
                "position_index": chunk.position_index,
                "word_count": chunk.word_count,
                "token_count": chunk.token_count,
                "quality_score": chunk.quality_score,
                "section_context": chunk.section_context,
            }

        # Verify dictionary is JSON serializable
        json_str = json.dumps(chunk_dict, default=str)
        assert json_str

        # Verify we can deserialize
        recovered = json.loads(json_str)
        assert recovered["id"] == chunk.id
        assert recovered["text"] == chunk.text
