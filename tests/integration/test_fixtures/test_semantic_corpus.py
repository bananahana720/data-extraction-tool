"""
QA Fixtures Validation Tests for Semantic Corpus

Comprehensive validation of QA fixtures infrastructure to ensure Epic 4
has reliable, validated test data with quality gates and PII compliance.
"""

import json
import re
import time
from pathlib import Path

import pytest
import textstat

# Import greenfield fixtures framework
# Import semantic corpus fixtures
from tests.fixtures.semantic_corpus import (
    CORPUS_CHARACTERISTICS,
    generate_large_corpus,
    get_business_corpus,
    get_edge_case_corpus,
    get_mixed_corpus,
    get_similarity_test_pairs,
    get_technical_corpus,
)


class TestCorpusSize:
    """Test Case 1: Corpus Size Validation - verify corpora meet Epic 4 requirements."""

    def test_technical_corpus_size(self):
        """Verify technical corpus meets minimum size requirements."""
        corpus = get_technical_corpus()

        # Minimum 5 documents for meaningful testing
        assert (
            len(corpus) >= 5
        ), f"Technical corpus has only {len(corpus)} documents, need at least 5"

        # Check word counts
        total_words = sum(len(doc.split()) for doc in corpus)
        assert (
            total_words >= 200
        ), f"Technical corpus has only {total_words} words, need at least 200"

        # Verify documented characteristics match actual
        assert len(corpus) == CORPUS_CHARACTERISTICS["technical"]["size"]

        # Each document should have meaningful content
        for doc in corpus:
            assert len(doc.split()) >= 20, "Document too short for meaningful testing"

    def test_business_corpus_size(self):
        """Verify business corpus has sufficient diversity."""
        corpus = get_business_corpus()

        assert (
            len(corpus) >= 5
        ), f"Business corpus has only {len(corpus)} documents, need at least 5"

        total_words = sum(len(doc.split()) for doc in corpus)
        assert (
            total_words >= 200
        ), f"Business corpus has only {total_words} words, need at least 200"

        assert len(corpus) == CORPUS_CHARACTERISTICS["business"]["size"]

        for doc in corpus:
            assert len(doc.split()) >= 20, "Document too short for business context testing"

    def test_mixed_corpus_balance(self):
        """Verify mixed corpus has balanced content types."""
        corpus = get_mixed_corpus()

        # Should have more documents than individual corpora
        assert len(corpus) >= 10, f"Mixed corpus has only {len(corpus)} documents"

        # Should contain both technical and business content
        technical_terms = ["algorithm", "architecture", "processing", "system"]
        business_terms = ["revenue", "customer", "compliance", "strategic"]

        full_text = " ".join(corpus).lower()

        # Check for presence of both content types
        tech_count = sum(1 for term in technical_terms if term in full_text)
        biz_count = sum(1 for term in business_terms if term in full_text)

        assert tech_count > 0, "Mixed corpus missing technical content"
        assert biz_count > 0, "Mixed corpus missing business content"

        # Verify size matches documentation
        assert len(corpus) == CORPUS_CHARACTERISTICS["mixed"]["size"]

    def test_edge_case_corpus_coverage(self):
        """Verify edge case corpus has comprehensive coverage."""
        corpus = get_edge_case_corpus()

        assert len(corpus) >= 7, f"Edge case corpus has only {len(corpus)} cases"

        # Check for specific edge case types
        edge_types_found = {
            "very_short": False,
            "very_long": False,
            "special_chars": False,
            "unicode": False,
            "numbers": False,
            "empty_like": False,
            "repeated": False,
        }

        for doc in corpus:
            if len(doc.split()) <= 2:
                edge_types_found["very_short"] = True
            if len(doc.split()) > 50:
                edge_types_found["very_long"] = True
            if re.search(r"[^\x00-\x7F]", doc):
                edge_types_found["unicode"] = True
            if re.search(r"\$[\d,]+\.\d+", doc):
                edge_types_found["numbers"] = True
            if doc.strip() == "" or len(doc.strip()) < 5:
                edge_types_found["empty_like"] = True
            if re.search(r"(\w+)\s+\1", doc.lower()):
                edge_types_found["repeated"] = True
            if re.search(r"[!@#$%^&*()]", doc):
                edge_types_found["special_chars"] = True

        # At least 5 different edge case types should be covered
        covered_types = sum(1 for v in edge_types_found.values() if v)
        assert covered_types >= 5, f"Only {covered_types} edge case types covered, need at least 5"


class TestCorpusQuality:
    """Test Case 2: Corpus Quality Metrics - validate quality characteristics."""

    def test_vocabulary_diversity(self):
        """Verify vocabulary diversity meets thresholds."""
        corpora = {
            "technical": get_technical_corpus(),
            "business": get_business_corpus(),
            "mixed": get_mixed_corpus(),
        }

        diversity_threshold = 0.3  # Unique words / total words

        for name, corpus in corpora.items():
            full_text = " ".join(corpus).lower()
            words = re.findall(r"\b\w+\b", full_text)
            unique_words = set(words)

            diversity = len(unique_words) / len(words) if words else 0

            assert (
                diversity >= diversity_threshold
            ), f"{name} corpus diversity {diversity:.2f} below threshold {diversity_threshold}"

            # Log metrics for documentation
            print(
                f"{name} corpus: {len(unique_words)} unique words, "
                f"{len(words)} total words, diversity={diversity:.2f}"
            )

    def test_readability_scores(self):
        """Verify readability scores are in expected ranges."""
        corpora = {
            "technical": get_technical_corpus(),
            "business": get_business_corpus(),
        }

        for name, corpus in corpora.items():
            scores = []
            for doc in corpus:
                if len(doc.split()) > 10:  # Need sufficient text for readability metrics
                    flesch_score = textstat.flesch_reading_ease(doc)
                    scores.append(flesch_score)

            if scores:
                avg_score = sum(scores) / len(scores)

                # Technical should be more complex (lower score)
                if name == "technical":
                    assert 0 <= avg_score <= 60, f"Technical corpus too simple: {avg_score:.1f}"
                else:
                    assert (
                        20 <= avg_score <= 80
                    ), f"Business corpus readability out of range: {avg_score:.1f}"

                print(f"{name} corpus avg Flesch score: {avg_score:.1f}")

    def test_entity_distribution(self):
        """Verify entity types are well-represented."""
        corpus = get_mixed_corpus()

        # Define entity patterns
        entity_patterns = {
            "technical": r"\b(API|ML|AI|TF-IDF|LSA|NLP|JSON|XML|HTTP)\b",
            "process": r"\b(extraction|processing|analysis|validation|optimization)\b",
            "business": r"\b(revenue|compliance|customer|strategic|risk)\b",
            "numeric": r"\$[\d,]+\.?\d*|\d+%|\d{4}",
        }

        entity_counts = {}
        full_text = " ".join(corpus)

        for entity_type, pattern in entity_patterns.items():
            matches = re.findall(pattern, full_text, re.IGNORECASE)
            entity_counts[entity_type] = len(matches)

        # Each entity type should appear at least once
        for entity_type, count in entity_counts.items():
            assert count > 0, f"Entity type '{entity_type}' not found in corpus"

        print(f"Entity distribution: {entity_counts}")

    def test_semantic_coherence(self):
        """Verify within-corpus semantic coherence."""
        technical_corpus = get_technical_corpus()

        # Simple coherence check: common technical terms should appear across documents
        technical_terms = ["system", "data", "processing", "architecture", "implementation"]

        doc_term_presence = []
        for doc in technical_corpus:
            doc_lower = doc.lower()
            term_count = sum(1 for term in technical_terms if term in doc_lower)
            doc_term_presence.append(term_count / len(technical_terms))

        avg_coherence = sum(doc_term_presence) / len(doc_term_presence)

        # At least 40% of key terms should appear in documents on average
        assert avg_coherence >= 0.4, f"Low semantic coherence: {avg_coherence:.2f}"

        print(f"Technical corpus coherence: {avg_coherence:.2f}")


class TestPIICompliance:
    """Test Case 3: PII and Compliance Validation - ensure no sensitive data."""

    # PII patterns for detection
    PII_PATTERNS = {
        "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
        "phone": r"\b(?:\+?1[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b",
        "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
        "credit_card": r"\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b",
        "real_name": r"\b(?:John\s+Doe|Jane\s+Smith|Bob\s+Johnson)\b",  # Common test names to avoid
        "address": r"\b\d+\s+[A-Z][a-z]+\s+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd)\b",
    }

    def test_no_pii_in_corpora(self):
        """Verify no PII present in any corpus."""
        all_corpora = {
            "technical": get_technical_corpus(),
            "business": get_business_corpus(),
            "mixed": get_mixed_corpus(),
            "edge_case": get_edge_case_corpus(),
        }

        violations = []

        for corpus_name, corpus in all_corpora.items():
            for doc_idx, doc in enumerate(corpus):
                for pii_type, pattern in self.PII_PATTERNS.items():
                    matches = re.findall(pattern, doc, re.IGNORECASE)
                    if matches:
                        violations.append(
                            {
                                "corpus": corpus_name,
                                "doc_index": doc_idx,
                                "pii_type": pii_type,
                                "matches": matches[:3],  # Limit to first 3 matches
                            }
                        )

        assert len(violations) == 0, f"PII found in corpora: {violations}"
        print("✓ All corpora PII-free")

    def test_compliance_scan(self):
        """Verify compliance with data safety requirements."""
        all_corpora = [
            get_technical_corpus(),
            get_business_corpus(),
            get_mixed_corpus(),
            get_edge_case_corpus(),
        ]

        sensitive_patterns = {
            "password": r"password\s*[:=]\s*\S+",
            "token": r"(?:api[_-]?key|token|secret)\s*[:=]\s*\S+",
            "internal_ip": r"\b(?:10|172\.(?:1[6-9]|2[0-9]|3[01])|192\.168)\.\d{1,3}\.\d{1,3}\b",
        }

        for corpus in all_corpora:
            for doc in corpus:
                for pattern_name, pattern in sensitive_patterns.items():
                    assert not re.search(
                        pattern, doc, re.IGNORECASE
                    ), f"Sensitive pattern '{pattern_name}' found in document"

        print("✓ Compliance scan passed - no sensitive data")

    def test_synthetic_content_only(self):
        """Verify all content is synthetic or public domain."""
        # Check that corpus generators don't reference real entities
        all_text = " ".join(
            [
                " ".join(get_technical_corpus()),
                " ".join(get_business_corpus()),
                " ".join(get_mixed_corpus()),
            ]
        )

        # Real company names that shouldn't appear
        real_entities = [
            "Microsoft",
            "Google",
            "Amazon",
            "Facebook",
            "Apple",
            "Goldman Sachs",
            "JPMorgan",
            "Wells Fargo",
        ]

        for entity in real_entities:
            assert entity not in all_text, f"Real entity '{entity}' found in corpus"

        print("✓ All content confirmed synthetic")

    def test_repository_safety(self):
        """Verify fixtures are safe for public repository."""
        # Generate a moderately sized corpus to test
        test_corpus = generate_large_corpus(num_docs=100, words_per_doc=50)

        # Should not contain any hardcoded secrets
        for doc in test_corpus:
            assert "BEGIN PRIVATE KEY" not in doc
            assert "-----BEGIN RSA" not in doc
            assert "Bearer " not in doc
            assert "Basic " not in doc  # Auth headers

        print("✓ Repository safety verified")


class TestCrossFormatConsistency:
    """Test Case 4: Cross-Format Consistency - validate fixture portability."""

    def test_fixture_loading(self):
        """Verify fixtures load successfully in all test contexts."""
        # Test direct function calls
        technical = get_technical_corpus()
        business = get_business_corpus()
        mixed = get_mixed_corpus()
        edge = get_edge_case_corpus()

        assert technical is not None and len(technical) > 0
        assert business is not None and len(business) > 0
        assert mixed is not None and len(mixed) > 0
        assert edge is not None and len(edge) > 0

        # Test they return consistent data on multiple calls
        assert get_technical_corpus() == technical, "Fixture not deterministic"
        assert get_business_corpus() == business, "Fixture not deterministic"

        print("✓ Fixtures load consistently")

    def test_serialization(self, temp_workspace):
        """Verify fixtures are serializable to different formats."""
        import pickle

        corpus = get_technical_corpus()

        # JSON serialization
        json_path = temp_workspace / "corpus.json"
        json_path.write_text(json.dumps(corpus, indent=2))
        loaded_json = json.loads(json_path.read_text())
        assert loaded_json == corpus, "JSON serialization failed"

        # Pickle serialization
        pickle_path = temp_workspace / "corpus.pkl"
        with open(pickle_path, "wb") as f:
            pickle.dump(corpus, f)
        with open(pickle_path, "rb") as f:
            loaded_pickle = pickle.load(f)
        assert loaded_pickle == corpus, "Pickle serialization failed"

        print("✓ Serialization working for JSON and pickle")

    def test_no_path_dependencies(self):
        """Verify fixtures work regardless of working directory."""
        import os

        original_cwd = os.getcwd()

        try:
            # Change to temp directory
            os.chdir("/tmp")

            # Should still work
            corpus = get_technical_corpus()
            assert len(corpus) > 0, "Fixture failed when CWD changed"

        finally:
            os.chdir(original_cwd)

        print("✓ No path dependencies found")

    def test_deterministic_behavior(self):
        """Verify fixtures produce identical output on repeated calls."""
        # Call multiple times
        results = []
        for _ in range(5):
            corpus = get_mixed_corpus()
            results.append(corpus)

        # All should be identical
        for i in range(1, len(results)):
            assert results[i] == results[0], f"Call {i} produced different results"

        # Test with parameterized generator
        gen_results = []
        for _ in range(3):
            large = generate_large_corpus(num_docs=10, words_per_doc=20)
            gen_results.append(large)

        for i in range(1, len(gen_results)):
            assert gen_results[i] == gen_results[0], "Generator not deterministic"

        print("✓ Fixtures are deterministic")


class TestEdgeCaseCoverage:
    """Test Case 5: Edge Case Coverage - verify comprehensive edge cases."""

    def test_edge_cases_present(self):
        """Verify all required edge cases are present."""
        corpus = get_edge_case_corpus()

        required_cases = {
            "empty": lambda d: len(d.strip()) < 5,
            "very_short": lambda d: len(d.split()) <= 3,
            "very_long": lambda d: len(d.split()) > 50,
            "unicode": lambda d: bool(re.search(r"[^\x00-\x7F]", d)),
            "numbers": lambda d: bool(re.search(r"\d", d)),
            "special_chars": lambda d: bool(re.search(r"[!@#$%^&*()]", d)),
            "repeated": lambda d: bool(re.search(r"(\w+).*\1", d.lower())),
        }

        found_cases = {case: False for case in required_cases}

        for doc in corpus:
            for case_name, checker in required_cases.items():
                if checker(doc):
                    found_cases[case_name] = True

        missing = [case for case, found in found_cases.items() if not found]
        assert len(missing) == 0, f"Missing edge cases: {missing}"

        print(f"✓ All {len(required_cases)} edge case types present")

    def test_edge_case_variety(self):
        """Verify edge cases cover different dimensions."""
        corpus = get_edge_case_corpus()

        dimensions = {
            "length": [],
            "charset": [],
            "structure": [],
        }

        for doc in corpus:
            # Length dimension
            dimensions["length"].append(len(doc))

            # Character set dimension
            if re.search(r"[^\x00-\x7F]", doc):
                dimensions["charset"].append("unicode")
            elif re.search(r"[^a-zA-Z0-9\s]", doc):
                dimensions["charset"].append("special")
            else:
                dimensions["charset"].append("alphanumeric")

            # Structure dimension
            if doc.strip() == "":
                dimensions["structure"].append("empty")
            elif len(doc.split()) == 1:
                dimensions["structure"].append("single_word")
            elif "\n" in doc:
                dimensions["structure"].append("multiline")
            else:
                dimensions["structure"].append("normal")

        # Should have variety in each dimension
        assert len(set(dimensions["length"])) >= 3, "Insufficient length variety"
        assert len(set(dimensions["charset"])) >= 2, "Insufficient charset variety"
        assert len(set(dimensions["structure"])) >= 2, "Insufficient structure variety"

        print("✓ Edge cases cover multiple dimensions")

    def test_edge_case_boundaries(self):
        """Verify edge cases test actual boundaries."""
        corpus = get_edge_case_corpus()

        # Check for boundary values
        lengths = [len(doc) for doc in corpus]

        # Should include very short and very long
        assert min(lengths) < 20, "No very short edge case"
        assert max(lengths) > 100, "No very long edge case"

        # Check for empty-like content
        has_empty_like = any(len(doc.strip()) < 5 for doc in corpus)
        assert has_empty_like, "No empty-like edge case"

        print("✓ Edge cases test boundaries effectively")

    def test_edge_case_documentation(self):
        """Verify edge cases are properly documented."""
        # Check that characteristics are documented
        assert "edge_cases" in CORPUS_CHARACTERISTICS

        edge_chars = CORPUS_CHARACTERISTICS["edge_cases"]
        assert "size" in edge_chars
        assert "diversity" in edge_chars
        assert edge_chars["diversity"] == "extreme", "Edge cases should have extreme diversity"

        print("✓ Edge cases properly documented")


class TestPerformanceCharacteristics:
    """Test Case 6: Performance Characteristics - validate performance requirements."""

    def test_load_time(self):
        """Verify fixture loading time < 100ms."""

        # Warm up
        _ = get_technical_corpus()

        # Measure load time
        start = time.perf_counter()
        corpus = get_technical_corpus()
        business = get_business_corpus()
        mixed = get_mixed_corpus()
        edge = get_edge_case_corpus()
        end = time.perf_counter()

        load_time_ms = (end - start) * 1000

        assert load_time_ms < 100, f"Load time {load_time_ms:.1f}ms exceeds 100ms limit"
        print(f"✓ Load time: {load_time_ms:.1f}ms (within 100ms limit)")

    def test_memory_footprint(self):
        """Verify memory usage is reasonable."""
        import sys

        # Get memory size of each corpus
        corpus_sizes = {
            "technical": sys.getsizeof(get_technical_corpus()),
            "business": sys.getsizeof(get_business_corpus()),
            "mixed": sys.getsizeof(get_mixed_corpus()),
            "edge": sys.getsizeof(get_edge_case_corpus()),
        }

        # Each corpus should be under 1MB
        max_size_bytes = 1024 * 1024  # 1MB

        for name, size in corpus_sizes.items():
            assert size < max_size_bytes, f"{name} corpus too large: {size/1024:.1f}KB"
            print(f"{name} corpus: {size/1024:.1f}KB")

        # Total should be under 10MB
        total_size = sum(corpus_sizes.values())
        assert (
            total_size < 10 * max_size_bytes
        ), f"Total size {total_size/1024/1024:.1f}MB exceeds 10MB"
        print(f"✓ Total memory: {total_size/1024:.1f}KB (well under 10MB limit)")

    def test_generation_reproducibility(self):
        """Verify fixture generation is reproducible."""
        # Generate multiple times with same parameters
        results = []
        for _ in range(3):
            corpus = generate_large_corpus(num_docs=50, words_per_doc=30)
            results.append(corpus)

        # All should be identical
        for i in range(1, len(results)):
            assert results[i] == results[0], f"Generation {i} differs from first"

        print("✓ Generation is reproducible")

    def test_caching_performance(self):
        """Verify caching improves performance if implemented."""

        # First call (potentially uncached)
        start1 = time.perf_counter()
        corpus1 = get_mixed_corpus()
        time1 = time.perf_counter() - start1

        # Second call (should be faster if cached)
        start2 = time.perf_counter()
        corpus2 = get_mixed_corpus()
        time2 = time.perf_counter() - start2

        # Verify data is same
        assert corpus1 == corpus2, "Cached data differs from original"

        # Second call should be at least as fast (allowing for timing variance)
        if time2 < time1 * 1.5:  # Allow 50% variance
            print(f"✓ Caching effective: {time2*1000:.2f}ms vs {time1*1000:.2f}ms")
        else:
            print(f"✓ No caching detected (acceptable): {time2*1000:.2f}ms vs {time1*1000:.2f}ms")


class TestGoldStandardData:
    """Test Case 7: Gold Standard Reference Data - validate reference data."""

    def test_reference_labels(self):
        """Verify reference labels are accurate."""
        similarity_pairs = get_similarity_test_pairs()

        assert len(similarity_pairs) >= 4, "Insufficient test pairs"

        for pair in similarity_pairs:
            # Required fields
            assert "doc1" in pair, "Missing doc1"
            assert "doc2" in pair, "Missing doc2"
            assert "expected_similarity" in pair, "Missing expected_similarity"
            assert "description" in pair, "Missing description"

            # Validate expected_similarity is a tuple of (min, max)
            sim_range = pair["expected_similarity"]
            assert isinstance(sim_range, tuple), "expected_similarity should be tuple"
            assert len(sim_range) == 2, "expected_similarity should have 2 values"
            assert 0 <= sim_range[0] <= sim_range[1] <= 1, "Invalid similarity range"

            # Validate description is meaningful
            assert len(pair["description"]) > 5, "Description too short"

        print(f"✓ {len(similarity_pairs)} reference pairs validated")

    def test_expected_outputs_defined(self):
        """Verify expected outputs are well-defined."""
        # Check similarity test pairs cover different ranges
        pairs = get_similarity_test_pairs()

        ranges_covered = {
            "identical": False,  # 0.99-1.0
            "high": False,  # 0.7-1.0
            "moderate": False,  # 0.3-0.7
            "low": False,  # 0.0-0.3
        }

        for pair in pairs:
            min_sim, max_sim = pair["expected_similarity"]

            if min_sim >= 0.99:
                ranges_covered["identical"] = True
            if min_sim >= 0.7:
                ranges_covered["high"] = True
            if 0.3 <= min_sim <= 0.7:
                ranges_covered["moderate"] = True
            if max_sim <= 0.3:
                ranges_covered["low"] = True

        missing_ranges = [r for r, covered in ranges_covered.items() if not covered]
        assert len(missing_ranges) == 0, f"Missing similarity ranges: {missing_ranges}"

        print("✓ All similarity ranges covered")

    def test_baseline_documentation(self):
        """Verify baseline results are documented."""
        # Check that corpus characteristics serve as baselines
        assert len(CORPUS_CHARACTERISTICS) >= 4, "Insufficient baseline documentation"

        for corpus_name, chars in CORPUS_CHARACTERISTICS.items():
            # Each should have basic metrics
            assert "size" in chars, f"{corpus_name} missing size baseline"
            assert "vocabulary" in chars, f"{corpus_name} missing vocabulary info"
            assert "diversity" in chars, f"{corpus_name} missing diversity metric"

        print(f"✓ Baselines documented for {len(CORPUS_CHARACTERISTICS)} corpora")

    def test_quality_metrics_established(self):
        """Verify quality metrics are established."""
        # Test that we can calculate quality metrics
        corpus = get_technical_corpus()

        metrics = {
            "document_count": len(corpus),
            "total_words": sum(len(doc.split()) for doc in corpus),
            "avg_words": sum(len(doc.split()) for doc in corpus) / len(corpus),
            "unique_words": len(set(" ".join(corpus).lower().split())),
        }

        # Metrics should be reasonable
        assert metrics["document_count"] >= 5
        assert metrics["total_words"] >= 200
        assert metrics["avg_words"] >= 20
        assert metrics["unique_words"] >= 50

        print(f"✓ Quality metrics: {metrics}")


class TestFixtureMaintenance:
    """Test Case 8: Fixture Maintenance and Updates - ensure maintainability."""

    def test_documentation_complete(self):
        """Verify fixture documentation is complete."""
        import inspect

        import tests.fixtures.semantic_corpus as corpus_module

        # Get all public functions
        functions = [
            name
            for name, obj in inspect.getmembers(corpus_module)
            if inspect.isfunction(obj) and not name.startswith("_")
        ]

        documented_functions = []
        for func_name in functions:
            func = getattr(corpus_module, func_name)
            if func.__doc__:
                documented_functions.append(func_name)
                # Check docstring quality
                docstring = func.__doc__.strip()
                assert len(docstring) > 20, f"{func_name} has insufficient documentation"
                assert (
                    "Returns:" in docstring or "Yields:" in docstring
                ), f"{func_name} missing return documentation"

        # At least 80% of functions should be documented
        doc_rate = len(documented_functions) / len(functions) if functions else 0
        assert doc_rate >= 0.8, f"Only {doc_rate*100:.0f}% of functions documented"

        print(f"✓ {len(documented_functions)}/{len(functions)} functions documented")

    def test_versioning_strategy(self):
        """Verify versioning strategy is in place."""
        # Check for version indicators
        corpus_module_path = Path("tests/fixtures/semantic_corpus.py")
        content = corpus_module_path.read_text()

        # Should have module docstring mentioning Epic 4
        assert "Epic 4" in content or "epic 4" in content.lower(), "Module should reference Epic 4"

        # CORPUS_CHARACTERISTICS serves as version baseline
        assert "CORPUS_CHARACTERISTICS" in content, "Missing corpus characteristics baseline"

        print("✓ Versioning strategy via Epic tracking and baselines")

    def test_backward_compatibility(self):
        """Verify old test patterns still work."""
        # Test that basic function calls work without parameters
        technical = get_technical_corpus()
        business = get_business_corpus()

        assert technical is not None
        assert business is not None

        # Test that generator function works with default parameters
        large_corpus = generate_large_corpus()
        assert len(large_corpus) == 1000, "Default parameters changed"

        print("✓ Backward compatibility maintained")

    def test_extension_patterns(self):
        """Verify fixtures can be easily extended."""
        # Test that generate_large_corpus accepts custom parameters
        custom_corpus = generate_large_corpus(num_docs=25, words_per_doc=50)

        assert len(custom_corpus) == 25, "Custom num_docs not working"

        # Check average word count
        avg_words = sum(len(doc.split()) for doc in custom_corpus) / len(custom_corpus)
        assert 40 <= avg_words <= 60, f"Custom words_per_doc not working: {avg_words:.0f}"

        # Test that corpus functions return lists (allowing extension)
        corpus = get_technical_corpus()
        extended = corpus + ["Additional document for testing."]
        assert len(extended) == len(corpus) + 1, "Cannot extend corpus"

        print("✓ Fixtures support extension patterns")


# Performance benchmark test
@pytest.mark.performance
def test_overall_fixture_performance():
    """Comprehensive performance test of all fixtures."""

    start = time.perf_counter()

    # Load all fixtures
    get_technical_corpus()
    get_business_corpus()
    get_mixed_corpus()
    get_edge_case_corpus()
    get_similarity_test_pairs()
    generate_large_corpus(num_docs=100, words_per_doc=50)

    elapsed = time.perf_counter() - start

    # Should complete within 1 second
    assert elapsed < 1.0, f"Fixture loading too slow: {elapsed:.2f}s"
    print(f"✓ All fixtures loaded in {elapsed*1000:.0f}ms")


if __name__ == "__main__":
    # Run all tests with verbose output
    pytest.main([__file__, "-v", "--tb=short"])
