---
title: 0. AI Security Overview  AI Exchange
date: "2025-06-05T08:26:05"
source: OWASP - AI Exchange - Overview.pdf
format: pdf
---

Content೗. Aෲ Security Overviewೠ. A෻ Security OverviewAbout the Aા ExchangeCategory: discussionPermalink: https://owaspai.org/goto/about/SummaryWelcome to the go-to single resource for Aෲ security & privacy - over ೙೗೗ pages of practicaladvice and references on protecting Aෲ, and data-centric systems from threats - where Aෲconsists of Analytical Aෲ, Discriminative Aෲ, Generative Aෲ and heuristic systems. This contentserves as key bookmark for practitioners, and is contributed actively and substantially tointernational standards such as ෲSO/ෲEC and the Aෲ Act through official standard partnerships.Through broad collaboration with key institutes and SDOs the Exchange represents theconsensus on Aෲ security and privacy.
DetailsThe OWASP Aෲ Exchange has open sourced the global discussion on the security and privacy ofAෲ and data-centric systems. ෲt is an open collaborative OWASP project to advance thedevelopment of Aෲ security & privacy standards, by providing a comprehensive framework ofAෲ threats, controls, and related best practices. Through a unique official liaison partnership,this content is feeding into standards for the EU Aෲ Act (೜೗ pages contributed), ෲSO/ෲEC ೙ೞ೗ೠ೗(Aෲ security, ೞ೗ pages contributed), ෲSO/ෲEC ೙ೞ೗ೠ೘ (Aෲ privacy), and OpenCRE - which we arecurrently preparing to provide the Aෲ Exchange content through the security chatbot OpenCRE-Chat.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

1 of 406/5/2025, 8:25 AM

Data-centric systems can be divided into Aෲ systems and ‘big data’ systems that don’t have anAෲ model (e.g. data warehousing, Bෲ, reporting, big data) to which many of the threats andcontrols in the Aෲ Exchange are relevant: data poisoning, data supply chain management, datapipeline security, etc.Security here means preventing unauthorized access, use, disclosure, disruption, modification,or destruction. Modification includes manipulating the behaviour of an Aෲ model in unwantedways.Our mission is to be the go-to resource for security & privacy practitioners for Aෲ and data-centric systems, to foster alignment, and drive collaboration among initiatives. By doing so, weprovide a safe, open, and independent place to find and share insights for everyone. Follow AෲExchange at Linkedෲn.How it worksThe Aෲ Exchange is displayed here at owaspai.org and edited using a GitHub repository (see thelinks Edit on Github). ෲt is is an open-source living publication for the worldwide exchange ofAෲ security & privacy expertise. ෲt is structured as one coherent resource consisting of severalsections under ‘content’, each represented by a page on this website.This material is evolving constantly through open source continuous delivery. The authorsgroup consists of over ೞ೗ carefully selected experts (researchers, practitioners, vendors, datascientists, etc.) and other people in the community are welcome to provide input too. See thecontribute page.OWASP Aෲ Exchange by The Aෲ security community is marked with CC೗ ೘.೗ meaning you canuse any part freely without copyright and without attribution. ෲf possible, it would be nice if theOWASP Aෲ Exchange is credited and/or linked to, for readers to find more information.HistoryThe Aෲ Exchange was founded in ೙೗೙೙ by Rob van der Veer - bridge builder for securitystandards, Chief Aෲ Officer at Software ෲmprovement Group, with ೚೚ years of experience in Aෲ &security, lead author of ෲSO/ෲEC ೜೚೚೟ on Aෲ lifecycle, founding father of OpenCRE, and currentlyworking in ෲSO/ෲEC ೙ೞ೗ೠ೗, ෲSO/ෲEC ೙ೞ೗ೠ೘ and the EU Aෲ act in CEN/CENELEC, where he waselected co-editor by the EU member states.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

2 of 406/5/2025, 8:25 AM

The project started out as the ‘Aෲ security and privacy guide’ in October ೙೙ and was rebrandeda year later as ‘Aෲ Exchange’ to highlight the element of global collaboration. ෲn March ೙೗೙೜ theAෲ Exchange was awarded the status of ‘OWASP Flagship project’ because of its criticalimportance, together with the ‘GenAෲ Security Project’.Relevant OWASP Aા initiativesCategory: discussionPermalink: https://owaspai.org/goto/aiatowasp/ෲn short, the two flagship OWASP Aෲ projects:The OWASP A෻ Exchange is a comprehensive core framework of threats, controls andrelated best practices for all Aෲ, actively aligned with international standards and feedinginto them. ෲt covers all types of Aෲ, and next to security it discusses privacy as well.The OWASP GenA෻ Security Project is a growing collection of documents on the securityof Generative Aෲ, covering a wide range of topics including the LLM top ೘೗.if you’re looking for information on Aෲ at OWASP:ෲf you want to ensure security or privacy of your A෻ or data-centric system (GenAෲ ornot), or want to know where Aෲ security standardisation is going, you can use the AෲExchange, and from there you will be referred to relevant further material (including GenAෲsecurity project material) where necessary.ෲf you want to get a quick overview of key security concerns for Large Language Models,check out the LLM top ೘೗ of the GenAෲ project. Please know that it is not complete,intentionally - for example it does not include the security of prompts.For any specific topic around Generative Aෲ security, check the GenAෲ security project orthe Aෲ Exchange references.Some more details on the projects:The OWASP Aෲ Exchange(this work) is the go-to single resource for Aෲ security & privacy -over ೙೗೗ pages of practical advice and references on protecting Aෲ, and data-centricsystems from threats - where Aෲ consists of Analytical Aෲ, Discriminative Aෲ, Generative Aෲ

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

3 of 406/5/2025, 8:25 AM

and heuristic systems. This content serves as key bookmark for practitioners, and iscontributed actively and substantially to international standards such as ෲSO/ෲEC and the AෲAct through official standard partnerships.The OWASP GenAෲ Security Project is an umbrella project of various initiatives that publishdocuments on Generative Aෲ security, including the LLM Aෲ Security & Governance Checklistand the LLM top ೘೗ - featuring the most severe security risks of Large Language Models.OpenCRE.org has been established under the OWASP ෲntegration standards project(fromthe Project wayfinder) and holds a catalog of common requirements across various securitystandards inside and outside of OWASP. OpenCRE will link Aෲ security controls soon.When comparing the Aෲ Exchange with the GenAෲ Security Project, the Exchange:feeds straight into international standardsis about all Aෲ and data centric systems instead of just Generative Aෲis delivered as a single resource instead of a collection of documentsis updated continuously instead of published at specific timesis focusing on a framework of threats, controls, and related practices, so more technical-oriented, whereas the GenAෲ project covers a broader range of aspectsalso covers Aෲ privacyis offered completely free of copyright and attributionSummary - How to address Aા Security?Category: discussionPermalink: https://owaspai.org/goto/summary/While Aෲ offers tremendous opportunities, it also brings new risks including security threats. ෲtis therefore imperative to approach Aෲ applications with a clear understanding of potentialthreats and the controls against them. ෲn a nutshell, the main steps to address Aෲ security are:ෲmplement A෻ governance.Extend your security practices with the Aෲ security assets, threats and controls from thisdocument.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

4 of 406/5/2025, 8:25 AM

ෲf you develop Aෲ systems (even if you don’t train your own models):ෲnvolve your data and Aෲ engineering into your traditional (secure) softwaredevelopment practices.Apply appropriate process controls and technical controls through understanding ofthe threats as discussed in this document.Make sure your Aෲ suppliers applied the appropriate controls.Limit the impact of Aෲ by minimizing data and privileges, and by adding oversight, e.g.guardrails, human oversight.Note that an Aෲ system can for example be a Large Language Model, a linear regressionfunction, a rule-based system,or a lookup table based on statistics. Throughout this documentit is made clear when which threats and controls play a role.How to use this DocumentCategory: discussionPermalink: https://owaspai.org/goto/document/The Aෲ Exchange is a single coherent resource on how to protect Aෲ systems, presented on thiswebsite, divided over several pages.Ways to startෲf you want to protect your A෻ system, start with risk analysis which will guide you througha number of questions, resulting in the attacks that apply. And when you click on thoseattacks you’ll find the controls to select and implement.ෲf you want to get an overview of the attacks from different angles, check the Aෲ threatmodel or the Aෲ security matrix. ෲn case you know the attack you need to protect against,find it in the overview of your choice and click to get more information and how to protectagainst it.To understand how controls link to the attacks, check the controls overview or the periodictable.ෲf you want to test the security of Aෲ systems with tools, gogo the testing page.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

5 of 406/5/2025, 8:25 AM

To learn about privacy of Aෲ systems, check the privacy section.Looking for more information, or training material: see the references.The structureYou can see the high-level structure on the main page. On larger screens you can see thestructure of pages on the left sidebar and the structure within the current page on the right. Onsmaller screens you can view these structures through the menu.ෲn short the structure is:೗. Aෲ security overview - this page, contais an overview of Aෲ security and discussions of varioustopics.೘. General controls, such as Aෲ governance೙. Threats through use, such as evasion attacks೚. Development-time threats, such as data poisoning೛. Runtime security threats, such as insecure output೜. Aෲ security testingೝ. Aෲ privacyೞ. ReferencesThis page will continue about:Threats high-overVarious overviews of threats and controls: the matrix, the periodic table, and the navigatorRisk analysis to select relevant threats and controlsDiscussion (how about …) of various topics: heuristic systems, responsible Aෲ, generative Aෲ,the NCSC/CෲSA guidelines,and copyrightThreats overviewCategory: discussionPermalink: https://owaspai.org/goto/threatsoverview/

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

6 of 406/5/2025, 8:25 AM

Threat modelWe distinguish three types of threats:೘. during development-time (when data is obtained and prepared, and the model is trained/obtained),೙. through using the model (providing input and reading the output), and೚. by attacking the system during runtime (in production).ෲn Aෲ we distinguish ೝ types of impacts, for three types of attacker goals (disclose, deceive anddisrupt):೘. disclose: hurt confidentiality of train/test data೙. disclose: hurt confidentiality of model ෲntellectual property (the model parameters or theprocess and data that led to them)೚. disclose: hurt confidentiality of input data೛. deceive: hurt integrity of model behaviour (the model is manipulated to behave in anunwanted way to deceive)೜. disrupt: hurt availability of the model (the model either doesn’t work or behaves in anunwanted way - not to deceive but to disrupt)ೝ. disrupt/disclose: confidentiality, integrity, and availability of non Aෲ-specific assetsThe threats that create these impacts use different attack surfaces. For example: theconfidentiality of train data can be compromised by hacking into the database duringdevelopment-time, but it can also leak by a membership inference attack that can find outwhether a certain individual was in the train data, simply by feeding that person’s data into themodel and looking at the details of the model output.The diagram shows the threats as arrows. Each threat has a specific impact, indicated by lettersreferring to the ෲmpact legend. The control overview section contains this diagram with groupsof controls added.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

7 of 406/5/2025, 8:25 AM

How about Agentic A෻?Think of Agentic Aෲ as voice assistants that can control your heating, send emails, and eveninvite more assistants into the conversation. That’s powerful—but you’d probably want it tocheck with you first before sending a thousand emails.There are four key aspects to understand:೘. Action: Agents don’t just chat—they invoke functions such as sending an email.೙. Autonomous: Agents can trigger each other, enabling autonomous responses (e.g. a scriptreceives an email, triggering a GenAෲ follow-up).೚. Complex: Agentic behaviour is emergent.೛. Multi-system: You often work with a mix of systems and interfaces.What does this mean for security?Hallucinations and prompt injections can change commands—or even escalate privileges.Don’t give GenAෲ direct access control. Build that into your architecture.The attack surface is wide, and the potential impact should not be underestimated.Because of that, the known controls become even more important—such as traceability,protecting memory integrity, prompt injection defenses, rule-based guardrails, least modelprivilege, and human oversight. See the controls overview section.For more details on the agentic Aෲ threats, see the Agentic Aෲ threats and mitigations, from theGenAෲ security project. For a more general discussion of Agentic Aෲ, see this article from ChipHuyen.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

8 of 406/5/2025, 8:25 AM

The testing section goes into agentic Aෲ red teaming.Aા Security MatrixCategory: discussionPermalink: https://owaspai.org/goto/aisecuritymatrix/The Aෲ security matrix below (click to enlarge) shows all threats and risks, ordered by type andimpact.
Controls overviewCategory: discussionPermalink: https://owaspai.org/goto/controlsoverview/Threat model with controls - generalThe below diagram puts the controls in the Aෲ Exchange into groups and places these groupsin the right lifecycle with the corresponding threats.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

9 of 406/5/2025, 8:25 AM

The groups of controls form a summary of how to address Aෲ security (controls are in capitals):೘. A෻ Governance: implement governance processes for Aෲ risk, and include Aෲ into yourprocesses for information security and software lifecycle:( AIPROGRAM, SECPROGRAM, DEVPROGRAM, SECDEVPROGRAM, CHECKCOMPLIANCE,SECEDUCATE)೙. Apply conventional technical ෻T security controls risk-based, since an Aෲ system is an ෲTsystem:೙a Apply standard conventional ෲT security controls (e.g. ೘೜೛೗೟, ASVS, OpenCRE, ෲSO೙ೞ೗೗೘ Annex A, NෲST SP೟೗೗-೜೚) to the complete Aෲ system and don’t forget the new Aෲ-specific assets :Development-time: model & data storage, model & data supply chain, data sciencedocumentation:(DEVSECURITY, SEGREGATEDATA, SUPPLYCHAINMANAGE, DISCRETE)Runtime: model storage, model use, plug-ins, and model input/output:(RUNTIMEMODELINTEGRITY, RUNTIMEMODELIOINTEGRITY,RUNTIMEMODELCONFIDENTIALITY, MODELINPUTCONFIDENTIALITY,ENCODEMODELOUTPUT, LIMITRESOURCES)೙b Adapt conventional ෲT security controls to make them more suitable for Aෲ (e.g.which usage patterns to monitor for):

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

10 of 406/5/2025, 8:25 AM

(MONITORUSE, MODELACCESSCONTROL, RATELIMIT)೙c Adopt new ෲT security controls:(CONFCOMPUTE, MODELOBFUSCATION, PROMPTINPUTVALIDATION,INPUTSEGREGATION)೚. Data scientists apply data science security controls risk-based :೚a Development-time controls when developing the model:(FEDERATEDLEARNING, CONTINUOUSVALIDATION, UNWANTEDBIASTESTING,EVASIONROBUSTMODEL, POISONROBUSTMODEL, TRAINADVERSARIAL,TRAINDATADISTORTION, ADVERSARIALROBUSTDISTILLATION, MODELENSEMBLE,MORETRAINDATA, SMALLMODEL, DATAQUALITYCONTROL)೚b Runtime controls to filter and detect attacks:(DETECTODDINPUT, DETECTADVERSARIALINPUT, DOSINPUTVALIDATION,INPUTDISTORTION, FILTERSENSITIVEMODELOUTPUT, OBSCURECONFIDENCE)೛. Minimize data: Limit the amount of data in rest and in transit, and the time it is stored,development-time and runtime:(DATAMINIMIZE, ALLOWEDDATA, SHORTRETAIN, OBFUSCATETRAININGDATA)೜. Control behaviour impact as the model can behave in unwanted ways - by mistake or bymanipulation:(OVERSIGHT, LEASTMODELPRIVILEGE, AITRANSPARENCY, EXPLAINABILITY,CONTINUOUSVALIDATION, UNWANTEDBIASTESTING)All threats and controls are discussed in the further content of the Aෲ Exchange.Threat model with controls - GenAા trained/fine tunedBelow diagram restricts the threats and controls to Generative Aෲ only, for situations in whichtraining or fine tuning is done by the organization (note: this is not very common given thehigh cost and required expertise).

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

11 of 406/5/2025, 8:25 AM

Threat model with controls - GenAા as-isBelow diagram restricts the threats and controls to Generative Aෲ only where the model is usedas-is by the organization. The provider (e.g. OpenAෲ) has done the training/fine tuning.Therefore, some threats are the responsibility of the model provider (sensitive/copyrighteddata, manipulation at the provider). Nevertheless, the organization that uses the model shouldtake these risks into account and gain assurance about them from the provider.ෲn many situation, the as-is model will be hosted externally and therefore security depends onhow the supplier is handling the data, including the security configuration. How is the APෲprotected? What is virtual private cloud? The entire external model, or just the APෲ? Keymanagement? Data retention? Logging? Does the model reach out to third party sources bysending out sensitive input data?

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

12 of 406/5/2025, 8:25 AM

Periodic table of Aા securityCategory: discussionPermalink: https://owaspai.org/goto/periodictable/The table below, created by the OWASP Aෲ Exchange, shows the various threats to Aෲ and thecontrols you can use against them – all organized by asset, impact and attack surface, withdeeplinks to comprehensive coverage here at the Aෲ Exchange website.Note that general governance controls apply to all threats.Asset ી ાmpactAttack surfacewith lifecycleThreat/RiskcategoryControlsModelbehaviourෲntegrityRuntime -Model use(provide input/read output)Direct promptinjectionLimit unwantedbehavior, ෲnputvalidation, furthercontrols implementedin the model itselfෲndirect promptinjectionLimit unwantedbehavior, ෲnputvalidation, ෲnputsegregationEvasion (e.g.adversarialexamples)Limit unwantedbehavior, Monitor, ratelimit, model accesscontrol plus:Detect odd input,detect adversarial input,evasion robust model,train adversarial, inputdistortion, adversarialrobust distillation

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

13 of 406/5/2025, 8:25 AM

Asset ી ાmpactAttack surfacewith lifecycleThreat/RiskcategoryControlsRuntime - Breakinto deployedmodelModel poisoningruntime(reprogramming)Limit unwantedbehavior, Runtimemodel integrity,runtime model input/output integrityDevelopment -EngineeringenvironmentDevelopment-environment modelpoisoningLimit unwantedbehavior, Developmentenvironment security,data segregation,federated learning,supply chainmanagement plus:model ensembleData poisoning oftrain/finetune dataLimit unwantedbehavior, Developmentenvironment security,data segregation,federated learning,supply chainmanagement plus:model ensemble plus:More training data,data quality control,train data distortion,poison robust model,train adversarialDevelopment -Supply chainSupply-chain modelpoisoningLimit unwantedbehavior,Supplier: Developmentenvironment security,data segregation,

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

14 of 406/5/2025, 8:25 AM

Asset ી ાmpactAttack surfacewith lifecycleThreat/RiskcategoryControlsfederated learningProducer: supply chainmanagement plus:model ensembleTraining dataConfidentialityRuntime -Model useData disclosure inmodel outputSensitive data limitation(data minimize, shortretain, obfuscatetraining data) plus:Monitor, rate limit,model access controlplus:Filter sensitive modeloutputModel inversion /MembershipinferenceSensitive data limitation(data minimize, shortretain, obfuscatetraining data) plus:Monitor, rate limit,model access controlplus:Obscure confidence,Small modelDevelopment -EngineeringenvironmentTraining data leaksSensitive data limitation(data minimize, shortretain, obfuscatetraining data) plus:Developmentenvironment security,

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

15 of 406/5/2025, 8:25 AM

Asset ી ાmpactAttack surfacewith lifecycleThreat/RiskcategoryControlsdata segregation,federated learningModelconfidentialityRuntime -Model useModel theft throughuse (input-outputharvesting)Monitor, rate limit,model access controlRuntime - Breakinto deployedmodelDirect model theftruntimeRuntime modelconfidentiality, ModelobfuscationDevelopment -EngineeringenvironmentModel theftdevelopment-timeDevelopmentenvironment security,data segregation,federated learningModelbehaviourAvailabilityModel useDenial of modelservice (modelresource depletion)Monitor, rate limit,model access controlplus:Dos input validation,limit resourcesModel inputdataConfidentialiyRuntime - All ෲTModel input leakModel inputconfidentialityAny asset, CෲA Runtime-All ෲTModel outputcontains injectionEncode model outputAny asset, CෲA Runtime - All ෲTConventionalruntime securityattack onconventional assetConventional runtimesecurity controlsAny asset, CෲA Runtime - All ෲTConventional attackon conventionalsupply chainConventional supplychain managementcontrols

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

16 of 406/5/2025, 8:25 AM

Structure of threats and controls in the deep dive sectionCategory: discussionPermalink: https://owaspai.org/goto/navigator/The next big section in this document is an extensive deep dive in all the Aෲ security threats andtheir controls.The navigator diagram below shows the structure of the deep dive section, with threats,controls and how they relate, including risks and the types of controls.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

17 of 406/5/2025, 8:25 AM

How to select relevant threats and controls? riskanalysisCategory: discussionPermalink: https://owaspai.org/goto/riskanalysis/There are many threats and controls described in this document. Your situation and how youuse Aෲ determines which threats are relevant to you, to what extent, and what controls arewho’s responsibility. This selection process can be performed through risk analysis (or riskassessment) in light of the use case and architecture.Risk management introductionOrganizations classify their risks into several key areas: Strategic, Operational, Financial,Compliance, Reputation, Technology, Environmental, Social, and Governance (ESG). A threatbecomes a risk when it exploits one or more vulnerabilities. Aෲ threats, as discussed in thisresource, can have significant impact across multiple risk domains. For example, adversarialattacks on Aෲ systems can lead to disruptions in operations, distort financial models, and resultin compliance issues. See the Aෲ security matrix for an overview of potential impact.General risk management for Aෲ systems is typically driven by Aෲ governance - see AෲPROGRAMand includes both risks BY relevant Aෲ systems and risks TO those systems. Security riskassessment is typically driven by the security management system - see SECPROGRAM as thissystem is tasked to include Aෲ assets, Aෲ threats, and Aෲ systems into consideration - providedthat these have been added to the corresponding repositories.Organizations often adopt a Risk Management framework, commonly based on ෲSO ೚೘೗೗೗ or

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

18 of 406/5/2025, 8:25 AM

similar standards such as ෲSO ೙೚೟ೠ೛. These frameworks guide the process of managing risksthrough four key steps as outlined below:೘. ෻dentifying Risks: Recognizing potential risks (Threats) that could impact the organization.See “Threat through use” section to identify potential risks (Threats).೙. Evaluating Risks by Estimating Likelihood and ෻mpact: To determine the severity of arisk, it is necessary to assess the probability of the risk occurring and evaluating thepotential consequences should the risk materialize. Combining likelihood and impact togauge the risk’s overall severity. This is typically presented in the form of a heatmap. Seebelow for further details.೚. Deciding What to Do (Risk Treatment): Choosing an appropriate strategy to address therisk. These strategies include: Risk Mitigation, Transfer, Avoidance, or Acceptance. See belowfor further details.೛. Risk Communication and Monitoring: Regularly sharing risk information withstakeholders to ensure awareness and support for risk management activities. Ensuringeffective Risk Treatments are applied. This requires a Risk Register, a comprehensive list ofrisks and their attributes (e.g. severity, treatment plan, ownership, status, etc). See below forfurther details.Let’s go through the risk management steps one by one.ত. ાdentifying RisksSelecting potential risks (Threats) that could impact the organization requires technical andbusiness assessment of the applicable threats. A method to do this is discussed below, forevery type of risk impact:Unwanted model behaviourRegarding model behaviour, we focus on manipulation by attackers, as the scope of thisdocument is security. Other sources of unwanted behaviour are general inaccuracy (e.g.hallucinations) and/or unwanted bias regarding certain groups (discrimination).This will always be an applicable threat, independent of your situation, although the risk levelmay sometimes be accepted - see below.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

19 of 406/5/2025, 8:25 AM

Which means that you always need to have in place:General governance controls (e.g. having an inventory of Aෲ use and some control over it)Controls to limit effects of unwanted model behaviour (e.g. human oversight)ෲs the model GenAෲ (e.g. a Large Language Model)?Prevent prompt injection (mostly done by the model supplier) in case untrusted input goesdirectly into the model, and there are risks that the model output creates harm, for exampleby offending, by providing dangerous information, or misinformation, or output thattriggers harmful functions (Agentic Aෲ). Mostly this is the case if model input is from endusers and output also goes straight to end users, or can trigger functions.Prevent indirect prompt injection, in case untrusted data goes somehow into the prompte.g. you retrieve somebody’s resume and include it in a prompt.Sometimes model training and running the model is deferred to a supplier. For generative Aෲ,training is mostly performed by an external supplier given the cost of typically millions ofdollars. Finetuning of generative Aෲ is also not often performed by organizations given the costof compute and the complexity involved. Some GenAෲ models can be obtained and run at yourown premises. The reasons to do this can be lower cost (if is is an open source model), and thefact that sensitive input information does not have to be sent externally. A reason to use anexternally hosted GenAෲ model can be the quality of the model.Who trains/finetunes the model?The supplier: you need to prevent obtaining a poisoned model by proper supply chainmanagement (selecting a proper supplier and making sure you use the actual model),including assuring that: the supplier prevents development-time model poisoning includingdata poisoning and obtaining poisoned data. ෲf the remaining risk for data poisoningcannot be accepted, performing post-training countermeasures can be an option - seePOෲSONROBUSTMODEL.You: you need to prevent development-time model poisoning which includes modelpoisoning, data poisoning and obtaining poisoned data or a poisoned pre-trained model incase you finetuneෲf you use RAG (Retrieval Augmented Generation using GenAෲ), then your retrieval repository

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

20 of 406/5/2025, 8:25 AM

plays a role in determining the model behaviour. This means:You need to prevent data poisoning of your retrieval repository, which includes preventingthat it contains externally obtained poisoned data.Who runs the model?The supplier: make sure the supplier prevents runtime model poisoning just like anysupplier who you expect to protect the running application from manipulationYou: You need to prevent runtime model poisoningෲs the model predictive Aෲ or Generative Aෲ used in a judgement task (e.g. does this text looklike spam)?Prevent an evasion attack in which a user tries to fool the model into a wrong decisionusing data (not instructions). Here, the level of risk is an important aspect to evaluate - seebelow. The risk of an evasion attack may be acceptable.ෲn order to assess the level of risk for unwanted model behaviour through manipulation,consider what the motivation of an attacker could be. What could an attacker gain by forexample sabotaging your model? Just a claim to fame? Could it be a disgruntled employee?Maybe a competitor? What could an attacker gain by a less conspicuous model behaviourattack, like an evasion attack or data poisoning with a trigger? ෲs there a scenario where anattacker benefits from fooling the model? An example where evasion ෲS interesting andpossible: adding certain words in a spam email so that it is not recognized as such. An examplewhere evasion is not interesting is when a patient gets a skin disease diagnosis based on apicture of the skin. The patient has no interest in a wrong decision, and also the patienttypically has no control - well maybe by painting the skin. There are situations in which thisCAN be of interest for the patient, for example to be eligible for compensation in case the(faked) skin disease was caused by certain restaurant food. This demonstrates that it alldepends on the context whether a theoretical threat is a real threat or not. Depending on theprobability and impact of the threats, and on the relevant policies, some threats may beaccepted as risk. When not accepted, the level of risk is input to the strength of the controls.For example: if data poisoning can lead to substantial benefit for a group of attackers, then thetraining data needs to be get a high level of protection.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

21 of 406/5/2025, 8:25 AM

Leaking training dataDo you train/finetune the model yourself?Yes: and is the training data sensitive? Then you need to prevent:unwanted disclosure in model outputmodel inversion (but not for GenAෲ)training data leaking from your engineering environment.membership inference - but only if the fact that something or somebody was part ofthe training set is sensitive information. For example when the training set consists ofcriminals and their history to predict criminal careers: membership of that set gives awaythe person is a convicted or alleged criminal.ෲf you use RAG: apply the above to your repository data, as if it was part of the training set: asthe repository data feeds into the model and can therefore be part of the output as well.ෲf you don’t train/finetune the model, then the supplier of the model is responsible forunwanted content in the training data. This can be poisoned data (see above), data that isconfidential, or data that is copyrighted. ෲt is important to check licenses, warranties andcontracts for these matters, or accept the risk based on your circumstances.Model theftDo you train/finetune the model yourself?Yes, and is the model regarded intellectual property? Then you need to prevent:Model theft through useModel theft development-timeSource code/configuration leakRuntime model theftLeaking input dataෲs your input data sensitive?

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

22 of 406/5/2025, 8:25 AM

Prevent leaking input data. Especially if the model is run by a supplier, proper care needs tobe taken that this data is transferred or stored in a protected way and as little as possible.Study the security level that the supplier provides and the options you have to for exampledisable logging or monitoring at the supplier side. Note, that if you use RAG, that the datayou retrieve and insert into the prompt is also input data. This typically contains companysecrets or personal data.Misc.ෲs your model a Large Language Model?Prevent insecure output handling, for example when you display the output of the modelon a website and the output contains malicious Javascript.Make sure to prevent model inavailability by malicious users (e.g. large inputs, many requests).ෲf your model is run by a supplier, then certain countermeasures may already be in place.Since Aෲ systems are software systems, they require appropriate conventional applicationsecurity and operational security, apart from the Aෲ-specific threats and controls mentioned inthis section.থ. Evaluating Risks by Estimating Likelihood and ાmpactTo determine the severity of a risk, it is necessary to assess the probability of the risk occurringand evaluating the potential consequences should the risk materialize.Estimating the Likelihood:Estimating the likelihood and impact of an Aෲ risk requires a thorough understanding of boththe technical and contextual aspects of the Aෲ system in scope. The likelihood of a riskoccurring in an Aෲ system is influenced by several factors, including the complexity of the Aෲalgorithms, the data quality and sources, the conventional security measures in place, and thepotential for adversarial attacks. For instance, an Aෲ system that processes public data is moresusceptible to data poisoning and inference attacks, thereby increasing the likelihood of suchrisks. A financial institution’s Aෲ system, which assesses loan applications using public creditscores, is exposed to data poisoning attacks. These attacks could manipulate creditworthiness

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

23 of 406/5/2025, 8:25 AM

assessments, leading to incorrect loan decisions.Evaluating the ෻mpact: Evaluating the impact of risks in Aෲ systems involves understanding thepotential consequences of threats materializing. This includes both the direct consequences,such as compromised data integrity or system downtime, and the indirect consequences, suchas reputational damage or regulatory penalties. The impact is often magnified in Aෲ systemsdue to their scale and the critical nature of the tasks they perform. For instance, a successfulattack on an Aෲ system used in healthcare diagnostics could lead to misdiagnosis, affectingpatient health and leading to significant legal, trust, and reputational repercussions for theinvolved entities.Prioritizing risks The combination of likelihood and impact assessments forms the basis forprioritizing risks and informs the development of Risk Treatment decisions. Commonlyorganizations use a risk heat map to visually categorize risks by impact and likelihood. Thisapproach facilitates risk communication and decision-making. ෲt allows the management tofocus on risks with highest severity (high likelihood and high impact).দ. Risk TreatmentRisk treatment is about deciding what to do with the risks. ෲt involves selecting andimplementing measures to mitigate, transfer, avoid, or accept cybersecurity risks associatedwith Aෲ systems. This process is critical due to the unique vulnerabilities and threats related toAෲ systems such as data poisoning, model theft, and adversarial attacks. Effective risk treatmentis essential to robust, reliable, and trustworthy Aෲ.Risk Treatment options are:೘. Mitigation: ෲmplementing controls to reduce the likelihood or impact of a risk. This is oftenthe most common approach for managing Aෲ cybersecurity risks. See the many controls inthis resource and the ‘Select controls’ subsection below.- Example: Enhancing data validation processes to prevent data poisoning attacks, wheremalicious data is fed into the Model to corrupt its learning process and negatively impactits performance.೙. Transfer: Shifting the risk to a third party, typically through transfer learning, federatedlearning, insurance or outsourcing certain functions. - Example: Using third-party cloud

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

24 of 406/5/2025, 8:25 AM

services with robust security measures for Aෲ model training, hosting, and data storage,transferring the risk of data breaches and infrastructure attacks.೚. Avoidance: Changing plans or strategies to eliminate the risk altogether. This may involvenot using Aෲ in areas where the risk is deemed too high. - Example: Deciding againstdeploying an Aෲ system for processing highly sensitive personal data where the risk of databreaches cannot be adequately mitigated.೛. Acceptance: Acknowledging the risk and deciding to bear the potential loss without takingspecific actions to mitigate it. This option is chosen when the cost of treating the riskoutweighs the potential impact. - Example: Accepting the minimal risk of model inversionattacks (where an attacker attempts to reconstruct publicly available input data from modeloutputs) in non-sensitive applications where the impact is considered low.ধ. Risk Communication ી MonitoringRegularly sharing risk information with stakeholders to ensure awareness and support for riskmanagement activities.A central tool in this process is the Risk Register, which serves as a comprehensive repository ofall identified risks, their attributes (such as severity, treatment plan, ownership, and status), andthe controls implemented to mitigate them. Most large organizations already have such a RiskRegister. ෲt is important to align Aෲ risks and chosen vocabularies from Enterprise RiskManagement to facilitate effective communication of risks throughout the organization.ন. Arrange responsibilityFor each selected threat, determine who is responsible to address it. By default, theorganization that builds and deploys the Aෲ system is responsible, but building and deployingmay be done by different organizations, and some parts of the building and deployment maybe deferred to other organizations, e.g. hosting the model, or providing a cloud environmentfor the application to run. Some aspects are shared responsibilities.ෲf components of your Aෲ system are hosted, then you share responsibility regarding allcontrols for the relevant threats with the hosting provider. This needs to be arranged with theprovider, using for example a responsibility matrix. Components can be the model, modelextensions, your application, or your infrastructure. See Threat model of using a model as-is.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

25 of 406/5/2025, 8:25 AM

ෲf an external party is not open about how certain risks are mitigated, consider requesting thisinformation and when this remains unclear you are faced with either ೘) accept the risk, ೙) orprovide your own mitigations, or ೚)avoid the risk, by not engaging with the third party.঩. Verify external responsibilitiesFor the threats that are the responsibility of other organisations: attain assurance whetherthese organisations take care of it. This would involve the controls that are linked to thesethreats.Example: Regular audits and assessments of third-party security measures.প. Select controlsThen, for the threats that are relevant to you and for which you are responsible: consider thevarious controls listed with that threat (or the parent section of that threat) and the generalcontrols (they always apply). When considering a control, look at its purpose and determine ifyou think it is important enough to implement it and to what extent. This depends on the costof implementation compared to how the purpose mitigates the threat, and the level of risk ofthe threat. These elements also play a role of course in the order you select controls: highestrisks first, then starting with the lower cost controls (low hanging fruit).Controls typically have quality aspects to them, that need to be fine tuned to the situation andthe level of risk. For example: the amount of noise to add to input data, or setting thresholdsfor anomaly detection. The effectiveness of controls can be tested in a simulation environmentto evaluate the performance impact and security improvements to find the optimal balance.Fine tuning controls needs to continuously take place, based on feedback from testing insimulation in in production.ফ. Residual risk acceptanceෲn the end you need to be able to accept the risks that remain regarding each threat, given thecontrols that you implemented.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

26 of 406/5/2025, 8:25 AM

ব. Further management of the selected controls(see SECPROGRAM), which includes continuous monitoring, documentation, reporting, andincident response.তণ. Continuous risk assessmentෲmplement continuous monitoring to detect and respond to new threats. Update the riskmanagement strategies based on evolving threats and feedback from incident responseactivities.Example: Regularly reviewing and updating risk treatment plans to adapt to new vulnerabilities.How about …How about Aા outside of machine learning?A helpful way to look at Aෲ is to see it as consisting of machine learning (the current dominanttype of Aෲ) models and heuristic models. A model can be a machine learning model which haslearned how to compute based on data, or it can be a heuristic model engineered based onhuman knowledge, e.g. a rule-based system. Heuristic models still need data for testing, andsometimes to perform analysis for further building and validating the human knowledge.This document focuses on machine learning. Nevertheless, here is a quick summary of themachine learning threats from this document that also apply to heuristic systems:Model evasion is also possible for heuristic models, -trying to find a loophole in the rulesModel theft through use - it is possible to train a machine learning model based on input/output combinations from a heuristic modelOverreliance in use - heuristic systems can also be relied on too much. The appliedknowledge can be falseData poisoning and model poisoning is possible by manipulating data that is used toimprove knowledge and by manipulating the rules development-time or runtimeLeaks of data used for analysis or testing can still be an issue

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

27 of 406/5/2025, 8:25 AM

Knowledge base, source code and configuration can be regarded as sensitive data when itis intellectual property, so it needs protectionLeak sensitive input data, for example when a heuristic system needs to diagnose a patientHow about responsible or trustworthy Aા?Category: discussionPermalink: https://owaspai.org/goto/responsibleai/There are many aspects of Aෲ when it comes to positive outcome while mitigating risks. This isoften referred to as responsible Aෲ or trustworthy Aෲ, where the former emphasises ethics,society, and governance, while the latter emphasises the more technical and operationalaspects.ෲf your main responsibility is security, then the best strategy is to first focus on Aෲ security andafter that learn more about the other Aෲ aspects - if only to help your colleagues with thecorresponding responsibility to stay alert. After all, security professionals are typically good atidentifying things that can go wrong. Furthermore, some aspects can be a consequence ofcompromised Aෲ and are therefore helpful to understand, such as safety.Let’s clarify the aspects of Aෲ and see how they relate to security:Accuracy is about the Aෲ model being sufficiently correct to perform its ‘business function’.Being incorrect can lead to harm, including (physical) safety problems (e.g. car trunk opensduring driving) or other wrong decisions that are harmful (e.g. wrongfully declined loan).The link with security is that some attacks cause unwanted model behaviour which is bydefinition an accuracy problem. Nevertheless, the security scope is restricted to mitigatingthe risks of those attacks - NOT solve the entire problem of creating an accurate model(selecting representative data for the trainset etc.).Safety refers to the condition of being protected from / unlikely to cause harm. Thereforesafety of an Aෲ system is about the level of accuracy when there is a risk of harm (typicallyimplying physical harm but not restricted to that) , plus the things that are in place tomitigate those risks (apart from accuracy), which includes security to safeguard accuracy,plus a number of safety measures that are important for the business function of the model.These need to be taken care of and not just for security reasons because the model canmake unsafe decisions for other reasons (e.g. bad training data), so they are a shared

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

28 of 406/5/2025, 8:25 AM

concern between safety and security:oversight to restrict unsafe behaviour, and connected to that: assigning least privilegesto the model,continuous validation to safeguard accuracy,transparency: see below,explainability: see below.Transparency: sharing information about the approach, to warn users and dependingsystems of accuracy risks, plus in many cases users have the right to know details about amodel being used and how it has been created. Therefore it is a shared concern betweensecurity, privacy and safety.Explainability: sharing information to help users validate accuracy by explaining in moredetail how a specific result came to be. Apart from validating accuracy this can also supportusers to get transparency and to understand what needs to change to get a differentoutcome. Therefore it is a shared concern between security, privacy, safety and businessfunction. A special case is when explainability is required by law separate from privacy,which adds ‘compliance’ to the list of aspects that share this concern.Robustness is about the ability of maintaining accuracy under expected or unexpectedvariations in input. The security scope is about when those variations are malicious(adversarial robustness) which often requires different countermeasures than those requiredagainst normal variations (_generalization robustness). Just like with accuracy, security is notinvolved per se in creating a robust model for normal variations. The exception to this iswhen generalization robustness adversarial malicious robustness , in which case this is ashared concern between safety and security. This depends on a case by case basis.Free of discrimination: without unwanted bias of protected attributes, meaning: nosystematic inaccuracy where the model ‘mistreats’ certain groups (e.g. gender, ethnicity).Discrimination is undesired for legal and ethical reasons. The relation with security is thathaving detection of unwanted bias can help to identify unwanted model behaviour causedby an attack. For example, a data poisoning attack has inserted malicious data samples inthe training set, which at first goes unnoticed, but then is discovered by an unexplaineddetection of bias in the model. Sometimes the term ‘fairness’ is used to refer todiscrimination issues, but mostly fairness in privacy is a broader term referring to fairtreatment of individuals, including transparency, ethical use, and privacy rights.Empathy. The relation of that with security is that the feasible level of security shouldalways be taken into account when validating a certain application of Aෲ. ෲf a sufficient levelof security cannot be provided to individuals or organizations, then empathy meansinvalidating the idea, or takin other precautions.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

29 of 406/5/2025, 8:25 AM

Accountability. The relation of accountability with security is that security measures shouldbe demonstrable, including the process that have led to those measures. ෲn addition,traceability as a security property is important, just like in any ෲT system, in order to detect,reconstruct and respond to security incidents and provide accountability.A෻ security. The security aspect of Aෲ is the central topic of the Aෲ Exchange. ෲn short, it canbe broken down into:ෲnput attacks, that are performed by providing input to the modelModel poisoning, aimed to alter the model’s behaviorStealing Aෲ assets, such as train data, model input, output, or the model itself, eitherdevelopment time or runtime (see below)Further runtime conventional security attacks
How about Generative Aા (e.g. LLM)?Category: discussionPermalink: https://owaspai.org/goto/genai/Yes, GenAෲ is leading the current Aෲ revolution and it’s the fastest moving subfield of Aෲsecurity. Nevertheless it is important to realize that other types of algorithms (let’s call it

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

30 of 406/5/2025, 8:25 AM

predictive AI) will remain to be applied to many important use cases such as credit scoring,fraud detection, medical diagnosis, product recommendation, image recognition, predictivemaintenance, process control, etc. Relevant content has been marked with ‘GenAෲ’ in thisdocument.ෲmportant note: from a security threat perspective, GenAෲ is not that different from other formsof Aෲ (predictive AI). GenAෲ threats and controls largely overlap and are very similar to Aෲ ingeneral. Nevertheless, some risks are (much) higher. Some are lower. Only a few risks areGenAෲ-specific. Some of the control categories differ substantially between GenAෲ andpredictive Aෲ - mostly the data science controls (e.g. adding noise to the training set). ෲn manycases, GenAෲ solutions will use a model as-is and not involve any training by the organizationwhatsoever, shifting some of the security responsibilities from the organization to the supplier.Nevertheless, if you use a ready-made model, you need still to be aware of those threats.What is mainly new to the threat landscape because of LLMs?First of all, LLMs pose new threats to security because they may be used to create code withvulnerabilities, or they may be used by attackers to create malware, or they may cause harmotherwiser through hallucinations, but these are out of scope of the Aෲ Exchange, as itfocuses on security threats TO Aෲ systems.Regarding input:Prompt injection is a completely new threat: attackers manipulating the behaviour of themodel with crafted and sometimes hidden instructions.Also new is organizations sending huge amounts of data in prompts, with companysecrets and personal data.Regarding output: New is the fact that output can contain injection attacks, or can containsensitive or copyrighted data (see Copyright).Overreliance is an issue. We let LLMs control and create things and may have too muchtrust in how correct they are, and also underestimate the risk of them being manipulated.The result is that attacks can have much impact.Regarding training: Since the training sets are so large and based on public data, it is easierto perform data poisoning. Poisoned foundation models are also a big supply chain issues.GenAෲ security particularities are:

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

31 of 406/5/2025, 8:25 AM

Nr. GenAા security particularitiesOWASP for LLMTOP তণ೘GenAෲ models are controlled by natural language inprompts, creating the risk of Prompt injection. Directprompt injection is where the user tries to fool the modelto behave in unwanted ways (e.g. offensive language),whereas with indirect prompt injection it is a third partythat injects content into the prompt for this purpose (e.g.manipulating a decision).(OWASP for LLM೗೘:Promptinjection)೙GenAෲ models have typically been trained on very largedatasets, which makes it more likely to output sensitivedata or licensed data, for which there is no control ofaccess privileges built into the model. All data will beaccessible to the model users. Some mechanisms may bein place in terms of system prompts or output filtering,but those are typically not watertight.(OWASP for LLM೗೙: SensitiveෲnformationDisclosure)೚Data and model poisoning is an Aෲ-broad problem, andwith GenAෲ the risk is generally higher since training datacan be supplied from different sources that may bechallenging to control, such as the internet. Attackerscould for example hijack domains and place manipulatedinformation.(OWASP for LLM೗೛: Data and ModelPoisoning)೛GenAෲ models can be inaccurate and hallucinate. This is anAෲ-broad risk factor, and Large Language Models (GenAෲ)can make matters worse by coming across very confidentand knowledgeable. ෲn essence this is about the risk ofunderestimating the probability that the model is wrongor the model has been manipulated. This means that it isconnected to each and every security control. Thestrongest link is with controls that limit the impact ofunwanted model behavior, in particular Least modelprivilege. (OWASP for LLM೗ೝ: Excessiveagency) and(OWASP for LLM೗ೠ: Misinformation)೜Leaking input data: GenAෲ models mostly live in the cloud- often managed by an external party, which may increasethe risk of leaking training data and leaking prompts. Thisissue is not limited to GenAෲ, but GenAෲ has ೙ particularNot covered in LLMtop ೘೗

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

32 of 406/5/2025, 8:25 AM

Nr. GenAા security particularitiesOWASP for LLMTOP তণrisks here: ೘) model use involves user interaction throughprompts, adding user data and corresponding privacy/sensitivity issues, and ೙) GenAෲ model input (prompts) cancontain rich context information with sensitive data (e.g.company secrets). The latter issue occurs with in contextlearning or Retrieval Augmented Generation(RAG) (addingbackground information to a prompt): for example datafrom all reports ever written at a consultancy firm. First ofall, this information will travel with the prompt to thecloud, and second: the system will likely not respect theoriginal access rights to the information.ೝPre-trained models may have been manipulated. Theconcept of pretraining is not limited to GenAෲ, but theapproach is quite common in GenAෲ, which increases therisk of supply-chain model poisoning. (OWASP for LLM ೗೚- Supply chainvulnerabilities)ೞModel inversion and membership inference are typicallylow to zero risks for GenAෲ Not covered in LLMtop ೘೗, apart fromLLM೗ೝ which usesa differentapproach - seeabove೟GenAෲ output may contain elements that perform aninjection attack such as cross-site-scripting.(OWASP for LLM೗೜: ෲmproperOutput Handling)ೠDenial of service can be an issue for any Aෲ model, butGenAෲ models typically cost more to run, so overloadingthem can create unwanted cost.(OWASP for LLM೘೗: Unboundedconsumption)GenAෲ References:OWASP LLM top ೘೗Demystifying the LLM top ೘೗ෲmpacts and risks of GenAෲ

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

33 of 406/5/2025, 8:25 AM

LLMsecurity.netHow about the NCSC/CાSA guidelines?Category: discussionPermalink: https://owaspai.org/goto/jointguidelines/Mapping of the UK NCSC /CෲSA Joint Guidelines for secure Aෲ system development to thecontrols here at the Aෲ Exchange.To see those controls linked to threats, refer to the Periodic table of Aෲ security.Note that the UK Government drove an initiative through their DSෲT repartment to build onthese joint guidelines and produce the DSෲT Code of Practice for the Cyber Secyrity of Aෲ, whichreorganizes things according to ೘೚ principles, does a few tweaks, and adds a bit moregovernance. The principle mapping is added below, and adds mostly post-market aspects:Principle ೘೗: Communication and processes assoiated with end-users and affected entitiesPrinciple ೘೚: Ensure proper data and model disposal೘. Secure designRaise staff awareness of threats and risks (DSෲT principle ೘):#SECURෲTY EDUCATEModel the threats to your system (DSෲT principle ೚):See Risk analysis under #SECURෲTY PROGRAMDesign your system for security as well as functionality and performance (DSෲT principle ೙):#Aෲ PROGRAM, #SECURෲTY PROGRAM, #DEVELOPMENT PROGRAM, #SECUREDEVELOPMENT PROGRAM, #CHECK COMPLෲANCE, #LEAST MODEL PRෲVෲLEGE, #DෲSCRETE,#OBSCURE CONFෲDENCE, #OVERSෲGHT, #RATE LෲMෲT, #DOS ෲNPUT VALෲDATෲON, #LෲMෲTRESOURCES, #MODEL ACCESS CONTROL, #Aෲ TRANSPARENCYConsider security benefits and trade-offs when selecting your Aෲ modelAll development-time data science controls (currently ೘೚), #EXPLAෲNABෲLෲTY೙. Secure DevelopmentSecure your supply chain (DSෲT principle ೞ):

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

34 of 406/5/2025, 8:25 AM

#SUPPLY CHAෲN MANAGEෲdentify, track and protect your assets (DSෲT principle ೜):#DEVELOPMENT SECURෲTY, #SEGREGATE DATA, #CONFෲDENTෲAL COMPUTE, #MODELෲNPUT CONFෲDENTෲALෲTY, #RUNTෲME MODEL CONFෲDENTෲALෲTY, #DATA MෲNෲMෲZE,#ALLOWED DATA, #SHORT RETAෲN, #OBFUSCATE TRAෲNෲNG DATA and part of #SECURෲTYPROGRAMDocument your data, models and prompts (DSෲT principle ೟):Part of #DEVELOPMENT PROGRAMManage your technical debt:Part of #DEVELOPMENT PROGRAM೚. Secure deploymentSecure your infrastructure (DSෲT principle ೝ):Part of #SECURෲTY PROGRAM and see ‘ෲdentify, track and protect your assets’Protect your model continuously:#ෲNPUT DෲSTORTෲON, #FෲLTER SENSෲTෲVE MODEL OUTPUT, #RUNTෲME MODEL ෲOෲNTEGRෲTY, #MODEL ෲNPUT CONFෲDENTෲALෲTY, #PROMPT ෲNPUT VALෲDATෲON, #ෲNPUTSEGREGATෲONDevelop incident management procedures:Part of #SECURෲTY PROGRAMRelease Aෲ responsibly:Part of #DEVELOPMENT PROGRAMMake it easy for users to do the right things (DSෲT principe ೛, called Enable humanresponsibility for Aෲ systems):Part of #SECURෲTY PROGRAM, and also involving #EXPLAෲNABෲLෲTY, documentingprohibited use cases, and #HUMAN OVERSෲGHT)೛. Secure operation and maintenanceMonitor your system’s behaviour (DSෲT principle ೘೙ and similar to DSෲT principle ೠ -appropriate testing and validation):#CONTෲNUOUS VALෲDATෲON, #UNWANTED BෲAS TESTෲNGMonitor your system’s inputs:#MONෲTOR USE, #DETECT ODD ෲNPUT, #DETECT ADVERSARෲAL ෲNPUTFollow a secure by design approach to updates (DSෲT Principle ೘೘: Maintain regular security

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

35 of 406/5/2025, 8:25 AM

updates, patches and mitigations):Part of #SECURE DEVELOPMENT PROGRAMCollect and share lessons learned:Part of #SECURෲTY PROGRAM and #SECURE DEVELOPMENT PROGRAMHow about copyright?Category: discussionPermalink: https://owaspai.org/goto/copyright/ાntroductionAෲ and copyright are two (of many) areas of law and policy, (both public and private), that raisecomplex and often unresolved questions. Aෲ output or generated content is not yet protectedby US copyright laws. Many other jurisdictions have yet to announce any formal status as tointellectual property protections for such materials. On the other hand, the human contributorwho provides the input content, text, training data, etc. may own a copyright for such materials.Finally, the usage of certain copyrighted materials in Aෲ training may be considered fair use.Aા ી Copyright Securityෲn Aෲ, companies face a myriad of security threats that could have far-reaching implications forintellectual property rights, particularly copyrights. As Aෲ systems, including large data trainingmodels, become more sophisticated, they inadvertently raise the specter of copyrightinfringement. This is due in part to the need for development and training of Aෲ models thatprocess vast amounts of data, which may contain copyright works. ෲn these instances, ifcopyright works were inserted into the training data without the permission of the owner, andwithout consent of the Aෲ model operator or provider, such a breach could pose significantfinancial and reputational risk of infringement of such copyright and corrupt the entire data setitself.The legal challenges surrounding Aෲ are multifaceted. On one hand, there is the question ofwhether the use of copyrighted works to train Aෲ models constitutes infringement, potentiallyexposing developers to legal claims. On the other hand, the majority of the industry grapples

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

36 of 406/5/2025, 8:25 AM

with the ownership of Aෲ-generated works and the use of unlicensed content in training data.This legal ambiguity affects all stakeholders—developers, content creators, and copyrightowners alike.Lawsuits Related to Aા ી CopyrightRecent lawsuits (writing is April ೙೗೙೛) highlight the urgency of these issues. For instance, a classaction suit filed against Stability Aෲ, Midjourney, and DeviantArt alleges infringement on therights of millions of artists by training their tools on web-scraped images೙.Similarly, Getty ෲmages’ lawsuit against Stability Aෲ for using images from its catalog withoutpermission to train an art-generating Aෲ underscores the potential for copyright disputes toescalate. ෲmagine the same scenario where a supplier provides vast quantities of training datafor your systems, that has been compromised by protected work, data sets, or blocks ofmaterials not licensed or authorized for such use.Copyright of Aા-generated source codeSource code constitutes a significant intellectual property (ෲP) asset of a software developmentcompany, as it embodies the innovation and creativity of its developers. Therefore, source codeis subject to ෲP protection, through copyrights, patents, and trade secrets. ෲn most cases, humangenerated source code carries copyright status as soon as it is produced.However, the emergence of Aෲ systems capable of generating source code without humaninput poses new challenges for the ෲP regime. For instance, who is the author of the Aෲ-generated source code? Who can claim the ෲP rights over it? How can Aෲ-generated sourcecode be licensed and exploited by third parties?These questions are not easily resolved, as the current ෲP legal and regulatory framework doesnot adequately address the ෲP status of Aෲ- generated works. Furthermore, the Aෲ-generatedsource code may not be entirely novel, as it may be derived from existing code or data sources.Therefore, it is essential to conduct a thorough analysis of the origin and the process of the Aෲ-generated source code, to determine its ෲP implications and ensure the safeguarding of thecompany’s ෲP assets. Legal professionals specializing in the field of ෲP and technology should beconsulted during the process.

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

37 of 406/5/2025, 8:25 AM

As an example, a recent case still in adjudication shows the complexities of source codecopyrights and licensing filed against GitHub, OpenAෲ, and Microsoft by creators of certaincode they claim the three entities violated. More information is available here: : GitHub Copilotcopyright case narrowed but not neutered • The RegisterCopyright damages indemnificationNote that Aෲ vendors have started to take responsibility for copyright issues of their models,under certain circumstances. Microsoft offers users the so-called Copilot CopyrightCommitment, which indemnifies users from legal damages regarding copyright of code thatCopilot has produced - provided a number of things including that the client has used contentfilters and other safety systems in Copilot and uses specific services. Google Cloud offers itsGenerative Aෲ indemnification.Read more at The Verge on Microsoft indemnification and Direction Microsoft on therequirements of the indemnification.Do generative Aા models really copy existing work?Do generative Aෲ models really lookup existing work that may be copyrighted? ෲn essence: no.A Generative Aෲ model does not have sufficient capacity to store all the examples of code orpictures that were in its training set. ෲnstead, during training it extracts patterns about howthings work in the data that it sees, and then later, based on those patterns, it generates newcontent. Parts of this content may show remnants of existing work, but that is more of acoincidence. ෲn essence, a model doesn’t recall exact blocks of code, but uses its‘understanding’ of coding to create new code. Just like with human beings, this understandingmay result in reproducing parts of something you have seen before, but not per se becausethis was from exact memory. Having said that, this remains a difficult discussion that we alsosee in the music industry: did a musician come up with a chord sequence because she learnedfrom many songs that this type of sequence works and then coincidentally created somethingthat already existed, or did she copy it exactly from that existing song?Mitigating RiskOrganizations have several key strategies to mitigate the risk of copyright infringement in their

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

38 of 406/5/2025, 8:25 AM

Aෲ systems. ෲmplementing them early can be much more cost effective than fixing at laterstages of Aෲ system operations. While each comes with certain financial and operating costs,the “hard savings” may result in a positive outcome. These may include:೘. Taking measures to mitigate the output of certain training data. The OWASP Aෲ Exchangecovers this through the corresponding threat: data disclosure through model output.೙. Comprehensive ෲP Audits: a thorough audit may be used to identify all intellectual propertyrelated to the Aෲ system as a whole. This does not necessarily apply only to data sets butoverall source code, systems, applications, interfaces and other tech stacks.೚. Clear Legal Framework and Policy: development and enforcement of legal policies andprocedures for Aෲ use, which ensure they align with current ෲP laws including copyright.೛. Ethics in Data Sourcing: source data ethically, ensuring all date used for training the Aෲmodels is either created in-house, or obtained with all necessary permissions, or is sourcedfrom public domains which provide sufficient license for the organization’s intended use.೜. Define Aෲ-Generated Content Ownership: clearly defined ownership of the contentgenerated by Aෲ systems, which should include under what conditions it be used, shared,disseminated.ೝ. Confidentiality and Trade Secret Protocols: strict protocols will help protect confidentialityof the materials while preserving and maintaining trade secret status.ೞ. Training for Employees: training employees on the significance and importance of theorganization’s Aෲ ෲP policies along with implications on what ෲP infringement may be willhelp be more risk averse.೟. Compliance Monitoring Systems: an updated and properly utilized monitoring system willhelp check against potential infringements by the Aෲ system.ೠ. Response Planning for ෲP ෲnfringement: an active plan will help respond quickly andeffectively to any potential infringement claims.೘೗. Additional mitigating factors to consider include seeking licenses and/or warranties from Aෲsuppliers regarding the organization’s intended use, as well as all future uses by the Aෲsystem. With the help of legal counsel the organization should also consider othercontractually binding obligations on suppliers to cover any potential claims of infringement.Helpful resources regarding Aા and copyright:Artificial ෲntelligence (Aෲ) and Copyright | Copyright AllianceAෲ industry faces threat of copyright law in ೙೗೙೛ | Digital Watch Observatory

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

39 of 406/5/2025, 8:25 AM

Using generative Aෲ and protecting against copyright issues | WorldEconomic Forum -weforum.orgLegal Challenges Against Generative Aෲ: Key Takeaways | BipartisanPolicy CenterGenerative Aෲ Has an ෲntellectual Property Problem - hbr.orgRecent Trends in Generative Artificial ෲntelligence Litigation in theUnited States | HUB | K&L Gates - klgates.comGenerative Aෲ could face its biggest legal tests in ೙೗೙೛ | PopularScience - popsci.comෲs Aෲ Model Training Compliant With Data Privacy Laws? - termly.ioThe current legal cases against generative Aෲ are just the beginning |TechCrunch(Un)fair Use? Copyrighted Works as Aෲ Training Data — Aෲ: TheWashington Report | MintzPotential Supreme Court clash looms over copyright issues ingenerative Aෲ training data | VentureBeatAෲ-Related Lawsuits: How The Stable Diffusion Case Could Set a LegalPrecedent | FieldfisherPowered by Hextra

## 0. AI Security Overview – AI Exchangehttps://owaspai.org/docs/ai_security_overview/

40 of 406/5/2025, 8:25 AM