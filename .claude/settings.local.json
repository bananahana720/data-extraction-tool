{
  "permissions": {
    "allow": [
      "Bash(pytest:*)",
      "Bash(if [ -d \"venv\" ])",
      "Bash(then source venv/bin/activate)",
      "Bash(elif [ -d \"venv\" ])",
      "Bash([ -f \"venv/Scripts/activate\" ])",
      "Bash(then source venv/Scripts/activate)",
      "Bash(else echo \"Virtual environment not found\")",
      "Bash(fi)",
      "Bash(./venv/Scripts/python.exe -m pytest --version)",
      "Bash(python -m pytest:*)",
      "Bash(./venv/Scripts/python.exe -m pytest:*)",
      "Bash(elif [ -d \".venv\" ])",
      "Bash(then source .venv/bin/activate)",
      "Bash(elif [ -f \"venv/Scripts/activate\" ])"
    ],
    "deny": [],
    "ask": []
  },
  "model": "sonnet",
  "hooks": {
    "SubagentStop": [
      {
        "hooks": [
          {
            "type": "prompt",
            "prompt": "Did the subagent complete its assigned task? Check:\n\n1. Task name in input: [extract from $ARGUMENTS]\n2. Did the subagent execute relevant tools? (yes/no)\n3. Is there output/result data? (yes/no)\n4. Are there uncaught errors? (yes/no = fail)\n\n$ARGUMENTS\n\nBlock if: #2=no OR #3=no OR #4=yes\nOtherwise approve.\n\nRespond: {\"decision\": \"approve|block\", \"reason\": \"[which check failed]\"}"
          }
        ]
      }
    ],
    "PreToolUse": [
      {
        "hooks": [
          {
            "type": "prompt",
            "prompt": "Predict the likely tool chain needed for the current task and validate this tool call fits the optimal path:\n\nCurrent Tool: $ARGUMENTS\nTask Context: [from conversation]\n\nPredict:\n- Expected tool sequence: [Read → Analyze → Edit → Test → Verify]\n- Current position in chain\n- Likely next 2-3 tools\n\nIf current tool breaks the expected pattern, evaluate:\n- Is this a better approach (eg. using grep instead of rg-based tool calls)?\n- Is Claude exploring unnecessarily?\n- Should we enforce the standard pattern?\n\nModify tool input if it can be optimized for the predicted chain."
          }
        ]
      }
    ]
  },
  "outputStyle": "default"
}
